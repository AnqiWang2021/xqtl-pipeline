{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "planned-holder",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Fine-mapping with PolyFun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-aging",
   "metadata": {
    "kernel": "Markdown",
    "tags": []
   },
   "source": [
    "## Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-development",
   "metadata": {
    "kernel": "Markdown",
    "tags": []
   },
   "source": [
    "The purpose of this notebook is to demonstrate a functionally-informed fine-mapping workflow using the PolyFun method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-seeking",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "## Methods Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-macedonia",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "## Input "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-cleaners",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1) GWAS summary statistics including the following variables: \n",
    "\n",
    "- variant_id - variant ID \n",
    "- P - p-value \n",
    "- CHR - chromosome number \n",
    "- BP - base pair position\n",
    "- A1 - The effect allele (i.e., the sign of the effect size is with respect to A1)\n",
    "- A2 - the second allele \n",
    "- MAF - minor allele frequency \n",
    "- BETA - effect size \n",
    "- SE - effect size standard error\n",
    "\n",
    "2) SNP-identifier file or S-LDSC (stratified LD-score regression) LD-score and annotation file\n",
    "\n",
    "   SNP-identifier file should include the following columns: \n",
    "\n",
    "- CHR - chromosome\n",
    "- BP - base pair position (in hg19 coordinates)\n",
    "- A1 - The effect allele \n",
    "- A2 - the second allele\n",
    "\n",
    "3) Ld-score weights file \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-caribbean",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "## Output\n",
    "\n",
    "A `.gz` file containing input summary statistics columns and additionally the following columns:\n",
    "\n",
    "- PIP - posterior causal probability\n",
    "- BETA_MEAN - posterior mean of causal effect size (in standardized genotype scale)\n",
    "- BETA_SD - posterior standard deviation of causal effect size (in standardized genotype scale)\n",
    "- CREDIBLE_SET - the index of the first (typically smallest) credible set that the SNP belongs to (0 means none).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-intensity",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-township",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Step 1: Compute Prior Causal Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-theorem",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 1: Use precomputed prior causal probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-internship",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Use precomputed prior causal probabilities of 19 million imputed UK Biobank SNPs with MAF>0.1%, based on a meta-analysis of 15 UK Biobank traits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-examination",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[prior_causal_prob]\n",
    "parameter: sumstats = AD_sumstats_Jansenetal_2019sept.txt.gz\n",
    "parameter: container = none\n",
    "bash: container = container \n",
    "    mkdir -p /output\n",
    "    python extract_snpvar.py \\\n",
    "        --sumstats sumstats \\\n",
    "        --out /output/snps_with_var.gz \\\n",
    "        --allow-missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-release",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 2: Compute via L2-regularized extension of S-LDSC (preferred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-toilet",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Compute via an L2-regularized extension of stratified LD-score regression (S-LDSC). Procedure for both methods is shown in this workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-sharing",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[munged_sumstats]\n",
    "parameter: sumstats = AD_sumstats_Jansenetal_2019sept.txt.gz\n",
    "parameter: sample_size = int\n",
    "parameter: container = none\n",
    "bash: container = container \n",
    "    mkdir -p /SLDSC_output\n",
    "    python munge_polyfun_sumstats.py \\\n",
    "      --sumstats sumstats \\\n",
    "      --n sample_size \\\n",
    "      --out /SLDSC_output/sumstats_munged.parquet \\\n",
    "      --min-info 0 \\\n",
    "      --min-maf 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-component",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Step 2: Create functional annotations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-radiation",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 1: Use existing function annotation files \n",
    "\n",
    "Use functional annotations for ~19 million UK Biobank imputed SNPs with MAF>0.1%, based on the baseline-LF 2.2.UKB annotations\n",
    "\n",
    "Download (30G): https://data.broadinstitute.org/alkesgroup/LDSCORE/baselineLF_v2.2.UKB.polyfun.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-pulse",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 2: Create annotations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-collective",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "To create your own annotations, for each chromosome, the following files are needed: \n",
    "\n",
    "1) A `.gz` or `.parquet` Annotations file containing the following columns:\n",
    "\n",
    "- CHR - chromosome number\n",
    "- BP base pair position\n",
    "- SNP - dbSNP reference number \n",
    "- A1 - The effect allele \n",
    "- A2 - the second allele\n",
    "- Arbitrary additional columns representing annotations \n",
    "\n",
    "2) A `.l2.M` white-space delimited file containing a single line with the sums of the columns of each annotation\n",
    "\n",
    "3) (Optional) A `l2.M_5_50` file that is the `.l2.M` file but only containing common SNPS (MAF between 5% and 50%) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-chess",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Step 3: Compute LD-scores for annotations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-condition",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 1: Compute with reference panel of sequenced individuals "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-knowing",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Reference panel should have at least 3000 sequenced individuals from target population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-briefing",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ld_score]\n",
    "parameter: container = none\n",
    "parameter: ref_ld = reference.1\n",
    "parameter: annot_file = annotations.1.annot.parquet\n",
    "bash: container = container\n",
    "    mkdir -p\n",
    "    python compute_ldscores.py \\\n",
    "    --bfile ref_ldexample_data/reference.1 \\\n",
    "    --annot annot_file \\\n",
    "    --out output/ldscores1.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-nursery",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 2: Compute with pre-computed UK Biobank LD matrices "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-canada",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Matrices download: https://data.broadinstitute.org/alkesgroup/UKBB_LD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-export",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ld_score_uk]\n",
    "parameter: container = none\n",
    "parameter: annot_file = annotations.1.annot.parquet\n",
    "base: container = container\n",
    "    mkdir -p \n",
    "    python compute_ldscores_from_ld.py \\\n",
    "    --annot annot_file \\\n",
    "    --ukb \\\n",
    "    --out output/ldscores2.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-remainder",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 3: Compute with own pre-computed LD matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-county",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Own pre-computed LD matrices should be in `.bcor` format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-kinase",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ld_score_own]\n",
    "parameter: container = none\n",
    "parameter: annot_file = annotations.1.annot.parquet\n",
    "parameter: sample_size = int\n",
    "base: container = container\n",
    "    mkdir -p \n",
    "    python compute_ldscores_from_ld.py \\\n",
    "    --annot annot_file \\\n",
    "    --out output/ldscores3.parquet \\\n",
    "    --n sample_size\\\n",
    "    bcor_files/*.bcor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-renaissance",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Step 4: Run PolyFun with L2-regularized S-LDSC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-right",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "If prior causal probabilities aren't computed,then use `finemapper.py` instead of `polyfun.py` to perform non-functionally-informed fine-mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-welsh",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[L2_regu_SLDSC]\n",
    "parameter: container = none\n",
    "paramter: ref_ld = /baselineLF2.2.UKB/baselineLF2.2.UKB.\n",
    "parameter: ref_wgt = /weights.UKB.l2.ldscore/weights.UKB.\n",
    "bash: container=container\n",
    "    python polyfun.py \\\n",
    "    --compute-h2-L2 \\\n",
    "    --no-partitions \\\n",
    "    --output-prefix /SLDSC_output/run \\\n",
    "    --sumstats /SLDSC_output/sumstats_munged.parquet \\\n",
    "    --ref-ld-chr ref_ld \\\n",
    "    --w-ld-chr ref_wgt \\\n",
    "    --allow-missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-affect",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Step 5: Functionally informed fine mapping with finemapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-cutting",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Input summary statistics file must have `SNPVAR` column (per-SNP heritability) to perform functionally-informed fine-mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-notion",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fine_mapping]\n",
    "parameter: genotype_file = example_data/chr1\n",
    "parameter: sumstat = example_data/chr1.finemap_sumstats.txt.gz\n",
    "parameter: sample_size = 383290\n",
    "parameter: chr = 1\n",
    "parameter: start = 46000001\n",
    "parameter: end = 49000001\n",
    "parameter: output_path = output/finemap.1.46000001.49000001.gz\n",
    "bash: \n",
    "    mkdir -p LD_cache\n",
    "    mkdir -o output\n",
    "\n",
    "    python finemapper.py \\\n",
    "    --geno genotype_file \\\n",
    "    --sumstats  \\\n",
    "    --n sample_size \\\n",
    "    --chr chr \\\n",
    "    --start start \\\n",
    "    --end end \\\n",
    "    --method susie \\\n",
    "    --max-num-causal 5 \\\n",
    "    --cache-dir LD_cache \\\n",
    "    --out output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-extension",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "## Minimal Working Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-church",
   "metadata": {
    "kernel": "Bash",
    "tags": []
   },
   "outputs": [],
   "source": [
    "module load Singularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "integral-belarus",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[munged_sumstats]\n",
    "parameter: sumstats = example_data/boltlmm_sumstats.gz\n",
    "parameter: sample_size = 327209\n",
    "parameter: output_path = example_data/sumstats_munged.parquet\n",
    "bash: container= none\n",
    "    python munge_polyfun_sumstats.py \\\n",
    "    --sumstats sumstats \\\n",
    "    --n sample_size \\\n",
    "    --out output_path \\\n",
    "    --min-info 0.6 \\\n",
    "    --min-maf 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-liquid",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ld_score]\n",
    "parameter: container = none\n",
    "parameter: ref_ld = reference.1\n",
    "parameter: annot_file = annotations.1.annot.parquet\n",
    "bash: container = container\n",
    "    mkdir -p\n",
    "    python compute_ldscores.py \\\n",
    "    --bfile ref_ld \\\n",
    "    --annot annot_file \\\n",
    "    --out output/ldscores1.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-showcase",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[L2_regu_SLDSC]\n",
    "parameter: output_path = output/testrun\n",
    "paramter: sumstats = example_data/sumstats.parquet\n",
    "paramter: ref_ld = example_data/annotations.\n",
    "parameter: ref_wgt = example_data/weights.\n",
    "bash: container=none\n",
    "    mkdir -p output\n",
    "    python polyfun.py \\\n",
    "    --compute-h2-L2 \\\n",
    "    --no-partitions \\\n",
    "    --output-prefix output_path \\\n",
    "    --sumstats sumstats \\\n",
    "    --ref-ld-chr ref_ld \\\n",
    "    --w-ld-chr ref_wgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-perry",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fine_mapping]\n",
    "parameter: genotype_file = example_data/chr1\n",
    "parameter: sumstats = example_data/chr1.finemap_sumstats.txt.gz\n",
    "parameter: sample_size = 383290\n",
    "parameter: chr = 1\n",
    "parameter: start = 46000001\n",
    "parameter: end = 49000001\n",
    "parameter: output_path = output/finemap.1.46000001.49000001.gz\n",
    "bash: \n",
    "    mkdir -p LD_cache\n",
    "    mkdir -o output\n",
    "\n",
    "    python finemapper.py \\\n",
    "    --geno genotype_file \\\n",
    "    --sumstats sumstats \\\n",
    "    --n sample_size \\\n",
    "    --chr chr \\\n",
    "    --start start \\\n",
    "    --end end \\\n",
    "    --method susie \\\n",
    "    --max-num-causal 5 \\\n",
    "    --cache-dir LD_cache \\\n",
    "    --out output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-desert",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-table",
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('finemap.1.46000001.49000001', sep=\"\\t\")\n",
    "\n",
    "data.head(5)\n",
    "    \n",
    "\n",
    "num_var_cs = np.count_nonzero(data['CREDIBLE_SET'])\n",
    "total_cs = len(data.CREDIBLE_SET.unique())- 1\n",
    "avg_var_cs = float(num_var_cs) / total_cs\n",
    "pip50 = sum(1 for i in data['PIP'] if i >0.5)\n",
    "pip95 = sum(1 for i in data['PIP'] if i >0.95)\n",
    "\n",
    "result = \"Number of variants with PIP > 0.5: \" + str(pip50) + \"\\n\" + \"Number of variants with PIP > 0.95: \" + str(pip95) + \"\\n\" \\\n",
    "    + \"Number of variants that have credible sets: \" + str(num_var_cs) + \"\\n\" \\\n",
    "    + \"Number of unique credible sets: \" + str(total_cs) + \"\\n\" \\\n",
    "    + \"Average number of variants per credible set: \" + str(avg_var_cs) \n",
    "\n",
    "\n",
    "with open('results.txt', 'a') as the_file:\n",
    "    the_file.write(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-hazard",
   "metadata": {
    "kernel": "Python3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import glob\n",
    "\n",
    "# get the location of finemapping result files\n",
    "file_with_annot_location = os.path.join('/mnt', 'mfs', 'statgen','tl3030','AD_2021_output','with_annot', 'finemap.*.gz')print(file_with_annot_location)\n",
    "filenames_with_annot = glob.glob(file_with_annot_location)\n",
    "\n",
    "snp_with_annot = pd.DataFrame()\n",
    "\n",
    "for f in filenames_with_annot:\n",
    "    # read the data\n",
    "    outfile = pd.read_csv(f, delimiter = \"\\t\")\n",
    "    \n",
    "    # filter out SNPs that has PIP >= 0.95\n",
    "    significant = (outfile[outfile['PIP']>=0.95])\n",
    "    snp_with_annot = snp_with_annot.append(significant)\n",
    "    \n",
    "    \n",
    "# remove duplicated SNPs\n",
    "snp_with_annot_uniq = snp_with_annot.drop_duplicates(subset='SNP', keep='first')\n",
    "\n",
    "CS_with_annot = pd.DataFrame()\n",
    "\n",
    "for f in filenames_with_annot:\n",
    "    # read the data\n",
    "    outfile = pd.read_csv(f, delimiter = \"\\t\")\n",
    "    \n",
    "    # filter out SNPs that has CS\n",
    "    significant = (outfile[outfile['CREDIBLE_SET']>0])\n",
    "    CS_with_annot = CS_with_annot.append(significant)\n",
    "    \n",
    "    \n",
    "# remove duplicated SNPs\n",
    "CS_with_annot_uniq = CS_with_annot.drop_duplicates(subset='SNP', keep='first')\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Read in the range file\n",
    "region_range = pd.read_csv(\"/mnt/mfs/statgen/tl3030/range.csv\").dropna()\n",
    "#chr1_160990767_161203192 = pd.read_csv(\"/mnt/mfs/statgen/tl3030/finemapping_result_97gene/finemap.1.160990767.161203192.gz\", delimiter = \"\\t\")\n",
    "#print(chr1_160990767_161203192.head())\n",
    "\n",
    "bpcol = CS_with_annot_uniq[['CHR', 'BP']]\n",
    "#print(bpcol.head())\n",
    "\n",
    "# Assign SNPs to the gene region that it belong to\n",
    "j = 0\n",
    "for i, bp in bpcol.iterrows():\n",
    "    #print(i, bp['CHR'])\n",
    "    for k,row in region_range.iterrows():\n",
    "        if (bp['CHR'] == row['Chr']) and (bp['BP'] > row['start']) and (bp['BP'] < row['end']):\n",
    "            #print(row['Chr'],row['Gene Name'])\n",
    "            #print(i, bp['CHR'], row['Chr'], row['Gene Name'])\n",
    "            CS_with_annot_uniq.iloc[j,15] = row['Gene Name']\n",
    "            #pass\n",
    "            #CS_with_annot_uniq.loc[j,'GENE']= row['Gene Name']\n",
    "    j += 1\n",
    "\n",
    "    \n",
    "CS_with_annot_uniq.to_csv('/mnt/mfs/statgen/tl3030/AD_2021_output/variants_with_CS_2021sumstat_97genes_with_annot.txt', index=False, sep='\\t', mode='w')\n",
    "CS_with_annot_uniq.sort_values(by=['CHR']) # sort the file by chromosome\n",
    "num_of_CS_with_annot = CS_with_annot_uniq.drop_duplicates(subset=['CREDIBLE_SET', 'GENE'], keep = 'last').reset_index(drop = True)\n",
    "num_of_CS_with_annot.sort_values(by=['GENE'])\n",
    "num_of_gene_with_annot = CS_with_annot_uniq.drop_duplicates(subset=['GENE'], keep = 'last').reset_index(drop = True)\n",
    "check_frequency_with_annot = CS_with_annot_uniq.groupby([\"CREDIBLE_SET\", \"GENE\"]).size().reset_index(name=\"Time\")\n",
    "CS_with_1_variant_with_annot = check_frequency_with_annot[check_frequency_with_annot['Time'] == 1]\n",
    "gene_with_annot = CS_with_annot_uniq['GENE']\n",
    "gene_list_with_annot = gene_with_annot.drop_duplicates()\n",
    "\n",
    "\n",
    "print(num_of_CS_with_annot.shape[0])\n",
    "print(num_of_gene_with_annot.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-jimmy",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "###  Summary of Fine-mapping Result Without Functional Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-throw",
   "metadata": {
    "kernel": "Python3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "# get the location of finemapping result files\n",
    "file_without_annot_location = os.path.join('/mnt', 'mfs', 'statgen','tl3030','AD_2021_output','without_annot', 'finemap.*.gz')\n",
    "\n",
    "import glob\n",
    "# get a list of result file name\n",
    "filenames_without_annot = glob.glob(file_without_annot_location)\n",
    "\n",
    "snp_without_annot = pd.DataFrame()\n",
    "\n",
    "for f in filenames_without_annot:\n",
    "    # read the data\n",
    "    outfile = pd.read_csv(f, delimiter = \"\\t\")\n",
    "    \n",
    "    # filter out SNPs that has PIP >= 0.95\n",
    "    significant = (outfile[outfile['PIP']>=0.95])\n",
    "    snp_without_annot = snp_without_annot.append(significant)\n",
    "\n",
    "\n",
    "# remove duplicated SNPs\n",
    "snp_without_annot_uniq = snp_without_annot.drop_duplicates(subset='SNP', keep='first')\n",
    "\n",
    "CS_without_annot = pd.DataFrame()\n",
    "\n",
    "for f in filenames_without_annot:\n",
    "    # read the data\n",
    "    outfile = pd.read_csv(f, delimiter = \"\\t\")\n",
    "    \n",
    "    # filter out SNPs that has CS\n",
    "    significant = (outfile[outfile['CREDIBLE_SET']>0])\n",
    "    CS_without_annot = CS_without_annot.append(significant)\n",
    "\n",
    "# remove duplicated SNPs\n",
    "CS_without_annot_uniq = CS_without_annot.drop_duplicates(subset='SNP', keep='first')\n",
    "\n",
    "\n",
    "# Read in the range file\n",
    "region_range = pd.read_csv(\"/mnt/mfs/statgen/tl3030/range.csv\").dropna()\n",
    "\n",
    "bpcol = CS_without_annot_uniq[['CHR', 'BP']]\n",
    "\n",
    "# Assign SNPs to the gene region that it belong to\n",
    "j = 0\n",
    "for i, bp in bpcol.iterrows():\n",
    "    #print(i, bp['CHR'])\n",
    "    for k,row in region_range.iterrows():\n",
    "        if (bp['CHR'] == row['Chr']) and (bp['BP'] > row['start']) and (bp['BP'] < row['end']):\n",
    "            CS_without_annot_uniq.iloc[j,15] = row['Gene Name']\n",
    "            #CS_without_annot_uniq.loc[j,'GENE']= row['Gene Name']\n",
    "    j += 1\n",
    "\n",
    "\n",
    "num_of_CS_without_annot = CS_without_annot_uniq.drop_duplicates(subset=['CREDIBLE_SET', 'GENE'], keep = 'last').reset_index(drop = True)\n",
    "num_of_CS_without_annot.sort_values(by=['GENE'])\n",
    "num_of_gene_without_annot = CS_without_annot_uniq.drop_duplicates(subset=['GENE'], keep = 'last').reset_index(drop = True)\n",
    "check_frequency_without_annot = CS_without_annot_uniq.groupby([\"CREDIBLE_SET\", \"GENE\"]).size().reset_index(name=\"Time\")\n",
    "CS_with_1_variant_without_annot = check_frequency_without_annot[check_frequency_without_annot['Time'] == 1]\n",
    "gene_without_annot = CS_without_annot_uniq['GENE']\n",
    "gene_list_without_annot = gene_without_annot.drop_duplicates()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "Python3",
     "python3",
     "Python3",
     "#FFD91A",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
