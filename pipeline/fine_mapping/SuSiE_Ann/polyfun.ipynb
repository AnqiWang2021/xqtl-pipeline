{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccfd1f61-720f-4814-a763-3c483b1bfa90",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Fine-mapping with PolyFun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17181047-82d9-408b-b8e5-99816341784f",
   "metadata": {
    "kernel": "Markdown",
    "tags": []
   },
   "source": [
    "## Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf46667-1481-4ccd-8db4-0e682d9e854a",
   "metadata": {
    "kernel": "Markdown",
    "tags": []
   },
   "source": [
    "The purpose of this notebook is to demonstrate a functionally-informed fine-mapping workflow using the PolyFun method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269b5b59-0d48-49e9-90b4-670371f4a88d",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "## Methods Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439b2ed3-6751-4406-a23f-4e35cb3f7404",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "## Input "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78f692d-ed06-473c-ba07-9920165ad88c",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1) GWAS summary statistics including the following variables: \n",
    "\n",
    "- variant_id - variant ID \n",
    "- P - p-value \n",
    "- CHR - chromosome number \n",
    "- BP - base pair position\n",
    "- A1 - The effect allele (i.e., the sign of the effect size is with respect to A1)\n",
    "- A2 - the second allele \n",
    "- MAF - minor allele frequency \n",
    "- BETA - effect size \n",
    "- SE - effect size standard error\n",
    "\n",
    "2) SNP-identifier file or S-LDSC (stratified LD-score regression) LD-score and annotation file\n",
    "\n",
    "   SNP-identifier file should include the following columns: \n",
    "\n",
    "- CHR - chromosome\n",
    "- BP - base pair position (in hg19 coordinates)\n",
    "- A1 - The effect allele \n",
    "- A2 - the second allele\n",
    "\n",
    "3) Ld-score weights file \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df89b45-0910-4989-bc1d-fdb20dc95fce",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "## Output\n",
    "\n",
    "A `.gz` file containing input summary statistics columns and additionally the following columns:\n",
    "\n",
    "- PIP - posterior causal probability\n",
    "- BETA_MEAN - posterior mean of causal effect size (in standardized genotype scale)\n",
    "- BETA_SD - posterior standard deviation of causal effect size (in standardized genotype scale)\n",
    "- CREDIBLE_SET - the index of the first (typically smallest) credible set that the SNP belongs to (0 means none).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c32642-3bb1-49a0-adfa-a12dbe957c4c",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcc256a-bda9-45a0-9aae-9371f06ab2c5",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Step 1: Compute Prior Causal Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ffc404-4d6e-44f9-8b4b-aa6f073eaab0",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 1: Use precomputed prior causal probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd3b503-9144-40b2-8587-cf1e9caa52bb",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Use precomputed prior causal probabilities of 19 million imputed UK Biobank SNPs with MAF>0.1%, based on a meta-analysis of 15 UK Biobank traits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d69ec9-a725-4342-b5c4-1458105de3f1",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[prior_causal_prob]\n",
    "parameter: sumstats = AD_sumstats_Jansenetal_2019sept.txt.gz\n",
    "parameter: container = none\n",
    "bash: container = container \n",
    "    mkdir -p /output\n",
    "    python extract_snpvar.py \\\n",
    "        --sumstats sumstats \\\n",
    "        --out /output/snps_with_var.gz \\\n",
    "        --allow-missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dfabc6-a216-4de7-a8da-95ef574baead",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 2: Compute via L2-regularized extension of S-LDSC (preferred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d6c08a-9b4b-4309-a5da-6adbeca7d762",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Compute via an L2-regularized extension of stratified LD-score regression (S-LDSC). Procedure for both methods is shown in this workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97245757-1817-4236-ab8f-fc86ac047e2d",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[munged_sumstats]\n",
    "parameter: sumstats = AD_sumstats_Jansenetal_2019sept.txt.gz\n",
    "parameter: sample_size = int\n",
    "parameter: container = none\n",
    "bash: container = container \n",
    "    mkdir -p /SLDSC_output\n",
    "    python munge_polyfun_sumstats.py \\\n",
    "      --sumstats sumstats \\\n",
    "      --n sample_size \\\n",
    "      --out /SLDSC_output/sumstats_munged.parquet \\\n",
    "      --min-info 0 \\\n",
    "      --min-maf 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957f6bf2-410a-4c5b-8951-7a7340a5b329",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Step 2: Create functional annotations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec89f80-7eb9-4618-81ef-16ef73fa445d",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 1: Use existing function annotation files \n",
    "\n",
    "Use functional annotations for ~19 million UK Biobank imputed SNPs with MAF>0.1%, based on the baseline-LF 2.2.UKB annotations\n",
    "\n",
    "Download (30G): https://data.broadinstitute.org/alkesgroup/LDSCORE/baselineLF_v2.2.UKB.polyfun.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62380ee-324b-4bb2-83b9-c4856eab2364",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 2: Create annotations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34f515-28ad-4ffd-9416-bf5a98c5ab4c",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "To create your own annotations, for each chromosome, the following files are needed: \n",
    "\n",
    "1) A `.gz` or `.parquet` Annotations file containing the following columns:\n",
    "\n",
    "- CHR - chromosome number\n",
    "- BP base pair position\n",
    "- SNP - dbSNP reference number \n",
    "- A1 - The effect allele \n",
    "- A2 - the second allele\n",
    "- Arbitrary additional columns representing annotations \n",
    "\n",
    "2) A `.l2.M` white-space delimited file containing a single line with the sums of the columns of each annotation\n",
    "\n",
    "3) (Optional) A `l2.M_5_50` file that is the `.l2.M` file but only containing common SNPS (MAF between 5% and 50%) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad4e2b-8e56-4ffb-8f07-5adf4e517073",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Step 3: Compute LD-scores for annotations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba02ea-7614-4725-968a-fa291ca6c21c",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 1: Compute with reference panel of sequenced individuals "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ac711-c423-45a6-8540-de42a5620b21",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Reference panel should have at least 3000 sequenced individuals from target population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e599c4-6520-4a88-9863-613898060535",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ld_score]\n",
    "parameter: container = none\n",
    "parameter: ref_ld = reference.1\n",
    "parameter: annot_file = annotations.1.annot.parquet\n",
    "bash: container = container\n",
    "    mkdir -p\n",
    "    python compute_ldscores.py \\\n",
    "    --bfile ref_ldexample_data/reference.1 \\\n",
    "    --annot annot_file \\\n",
    "    --out output/ldscores1.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871bf7bc-dd52-42a1-8676-e3778ac56a58",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 2: Compute with pre-computed UK Biobank LD matrices "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250da1f9-d8bf-4103-86b8-d78da97e27a8",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Matrices download: https://data.broadinstitute.org/alkesgroup/UKBB_LD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb0444-f80a-45f8-8a04-9926faf2ffd8",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ld_score_uk]\n",
    "parameter: container = none\n",
    "parameter: annot_file = annotations.1.annot.parquet\n",
    "base: container = container\n",
    "    mkdir -p \n",
    "    python compute_ldscores_from_ld.py \\\n",
    "    --annot annot_file \\\n",
    "    --ukb \\\n",
    "    --out output/ldscores2.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a4cef6-6283-4acd-bba0-dcbb7c19a4c7",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 3: Compute with own pre-computed LD matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f9e6d7-fbc2-4ff1-a52a-bdd01889a4e6",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Own pre-computed LD matrices should be in `.bcor` format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847d88f2-50c5-46f4-9942-293f71a1c7a0",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ld_score_own]\n",
    "parameter: container = none\n",
    "parameter: annot_file = annotations.1.annot.parquet\n",
    "parameter: sample_size = int\n",
    "base: container = container\n",
    "    mkdir -p \n",
    "    python compute_ldscores_from_ld.py \\\n",
    "    --annot annot_file \\\n",
    "    --out output/ldscores3.parquet \\\n",
    "    --n sample_size\\\n",
    "    bcor_files/*.bcor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7e9355-2ce3-4760-9a98-e1223f28146a",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Step 4: Run PolyFun with L2-regularized S-LDSC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db90c692-8af7-4775-9b31-91f739e83432",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "If prior causal probabilities aren't computed,then use `finemapper.py` instead of `polyfun.py` to perform non-functionally-informed fine-mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d1b9bc-6b06-40c5-8c3c-1996881c56df",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[L2_regu_SLDSC]\n",
    "parameter: container = none\n",
    "paramter: ref_ld = /baselineLF2.2.UKB/baselineLF2.2.UKB.\n",
    "parameter: ref_wgt = /weights.UKB.l2.ldscore/weights.UKB.\n",
    "bash: container=container\n",
    "    python polyfun.py \\\n",
    "    --compute-h2-L2 \\\n",
    "    --no-partitions \\\n",
    "    --output-prefix /SLDSC_output/run \\\n",
    "    --sumstats /SLDSC_output/sumstats_munged.parquet \\\n",
    "    --ref-ld-chr ref_ld \\\n",
    "    --w-ld-chr ref_wgt \\\n",
    "    --allow-missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ba8311-ff52-44ac-9a28-d77c5b870ddc",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Functionally informed fine mapping with finemapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413d06f7-1ece-4f58-b34e-5a85777162eb",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Input summary statistics file must have `SNPVAR` column (per-SNP heritability) to perform functionally-informed fine-mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b96ce9-4d95-4ee9-a04a-5cc0d7156046",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fine_mapping]\n",
    "parameter: genotype_file = example_data/chr1\n",
    "parameter: sumstat = example_data/chr1.finemap_sumstats.txt.gz\n",
    "parameter: sample_size = 383290\n",
    "parameter: chr = 1\n",
    "parameter: start = 46000001\n",
    "parameter: end = 49000001\n",
    "parameter: output_path = output/finemap.1.46000001.49000001.gz\n",
    "bash: \n",
    "    mkdir -p LD_cache\n",
    "    mkdir -o output\n",
    "\n",
    "    python finemapper.py \\\n",
    "    --geno genotype_file \\\n",
    "    --sumstats  \\\n",
    "    --n sample_size \\\n",
    "    --chr chr \\\n",
    "    --start start \\\n",
    "    --end end \\\n",
    "    --method susie \\\n",
    "    --max-num-causal 5 \\\n",
    "    --cache-dir LD_cache \\\n",
    "    --out output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f3e734-ee0a-409c-8754-46987fbeb407",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "## Minimal Working Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11e2779-3117-4805-a549-c7bcc63446b1",
   "metadata": {
    "kernel": "Bash",
    "tags": []
   },
   "outputs": [],
   "source": [
    "module load Singularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e23a8a-c0b5-4405-9cfa-ed5882c81702",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[munged_sumstats]\n",
    "parameter: sumstats = example_data/boltlmm_sumstats.gz\n",
    "parameter: sample_size = 327209\n",
    "parameter: output_path = example_data/sumstats_munged.parquet\n",
    "bash: container= none\n",
    "    python munge_polyfun_sumstats.py \\\n",
    "    --sumstats sumstats \\\n",
    "    --n sample_size \\\n",
    "    --out output_path \\\n",
    "    --min-info 0.6 \\\n",
    "    --min-maf 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90480f9-2afd-425b-b46e-f22f17fa38ec",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ld_score]\n",
    "parameter: container = none\n",
    "parameter: ref_ld = reference.1\n",
    "parameter: annot_file = annotations.1.annot.parquet\n",
    "bash: container = container\n",
    "    mkdir -p\n",
    "    python compute_ldscores.py \\\n",
    "    --bfile ref_ld \\\n",
    "    --annot annot_file \\\n",
    "    --out output/ldscores1.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735957e8-5780-4c95-ac43-55ed8a6905aa",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[L2_regu_SLDSC]\n",
    "parameter: output_path = output/testrun\n",
    "paramter: sumstats = example_data/sumstats.parquet\n",
    "paramter: ref_ld = example_data/annotations.\n",
    "parameter: ref_wgt = example_data/weights.\n",
    "bash: container=none\n",
    "    mkdir -p output\n",
    "    python polyfun.py \\\n",
    "    --compute-h2-L2 \\\n",
    "    --no-partitions \\\n",
    "    --output-prefix output_path \\\n",
    "    --sumstats sumstats \\\n",
    "    --ref-ld-chr ref_ld \\\n",
    "    --w-ld-chr ref_wgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040bac79-2941-4158-992e-3ba2b887ab57",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fine_mapping]\n",
    "parameter: genotype_file = example_data/chr1\n",
    "parameter: sumstats = example_data/chr1.finemap_sumstats.txt.gz\n",
    "parameter: sample_size = 383290\n",
    "parameter: chr = 1\n",
    "parameter: start = 46000001\n",
    "parameter: end = 49000001\n",
    "parameter: output_path = output/finemap.1.46000001.49000001.gz\n",
    "bash: \n",
    "    mkdir -p LD_cache\n",
    "    mkdir -o output\n",
    "\n",
    "    python finemapper.py \\\n",
    "    --geno genotype_file \\\n",
    "    --sumstats sumstats \\\n",
    "    --n sample_size \\\n",
    "    --chr chr \\\n",
    "    --start start \\\n",
    "    --end end \\\n",
    "    --method susie \\\n",
    "    --max-num-causal 5 \\\n",
    "    --cache-dir LD_cache \\\n",
    "    --out output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f0ca7-d98d-4cd9-bcf8-e45358b814c0",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a07162-b253-430e-a70f-e4110dd7a0cd",
   "metadata": {
    "kernel": "Python3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import glob\n",
    "\n",
    "# get the location of finemapping result files\n",
    "file_with_annot_location = os.path.join('/mnt', 'mfs', 'statgen','tl3030','AD_2021_output','with_annot', 'finemap.*.gz')print(file_with_annot_location)\n",
    "filenames_with_annot = glob.glob(file_with_annot_location)\n",
    "\n",
    "snp_with_annot = pd.DataFrame()\n",
    "\n",
    "for f in filenames_with_annot:\n",
    "    # read the data\n",
    "    outfile = pd.read_csv(f, delimiter = \"\\t\")\n",
    "    \n",
    "    # filter out SNPs that has PIP >= 0.95\n",
    "    significant = (outfile[outfile['PIP']>=0.95])\n",
    "    snp_with_annot = snp_with_annot.append(significant)\n",
    "    \n",
    "    \n",
    "# remove duplicated SNPs\n",
    "snp_with_annot_uniq = snp_with_annot.drop_duplicates(subset='SNP', keep='first')\n",
    "\n",
    "CS_with_annot = pd.DataFrame()\n",
    "\n",
    "for f in filenames_with_annot:\n",
    "    # read the data\n",
    "    outfile = pd.read_csv(f, delimiter = \"\\t\")\n",
    "    \n",
    "    # filter out SNPs that has CS\n",
    "    significant = (outfile[outfile['CREDIBLE_SET']>0])\n",
    "    CS_with_annot = CS_with_annot.append(significant)\n",
    "    \n",
    "    \n",
    "# remove duplicated SNPs\n",
    "CS_with_annot_uniq = CS_with_annot.drop_duplicates(subset='SNP', keep='first')\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Read in the range file\n",
    "region_range = pd.read_csv(\"/mnt/mfs/statgen/tl3030/range.csv\").dropna()\n",
    "#chr1_160990767_161203192 = pd.read_csv(\"/mnt/mfs/statgen/tl3030/finemapping_result_97gene/finemap.1.160990767.161203192.gz\", delimiter = \"\\t\")\n",
    "#print(chr1_160990767_161203192.head())\n",
    "\n",
    "bpcol = CS_with_annot_uniq[['CHR', 'BP']]\n",
    "#print(bpcol.head())\n",
    "\n",
    "# Assign SNPs to the gene region that it belong to\n",
    "j = 0\n",
    "for i, bp in bpcol.iterrows():\n",
    "    #print(i, bp['CHR'])\n",
    "    for k,row in region_range.iterrows():\n",
    "        if (bp['CHR'] == row['Chr']) and (bp['BP'] > row['start']) and (bp['BP'] < row['end']):\n",
    "            #print(row['Chr'],row['Gene Name'])\n",
    "            #print(i, bp['CHR'], row['Chr'], row['Gene Name'])\n",
    "            CS_with_annot_uniq.iloc[j,15] = row['Gene Name']\n",
    "            #pass\n",
    "            #CS_with_annot_uniq.loc[j,'GENE']= row['Gene Name']\n",
    "    j += 1\n",
    "\n",
    "    \n",
    "CS_with_annot_uniq.to_csv('/mnt/mfs/statgen/tl3030/AD_2021_output/variants_with_CS_2021sumstat_97genes_with_annot.txt', index=False, sep='\\t', mode='w')\n",
    "CS_with_annot_uniq.sort_values(by=['CHR']) # sort the file by chromosome\n",
    "num_of_CS_with_annot = CS_with_annot_uniq.drop_duplicates(subset=['CREDIBLE_SET', 'GENE'], keep = 'last').reset_index(drop = True)\n",
    "num_of_CS_with_annot.sort_values(by=['GENE'])\n",
    "num_of_gene_with_annot = CS_with_annot_uniq.drop_duplicates(subset=['GENE'], keep = 'last').reset_index(drop = True)\n",
    "check_frequency_with_annot = CS_with_annot_uniq.groupby([\"CREDIBLE_SET\", \"GENE\"]).size().reset_index(name=\"Time\")\n",
    "CS_with_1_variant_with_annot = check_frequency_with_annot[check_frequency_with_annot['Time'] == 1]\n",
    "gene_with_annot = CS_with_annot_uniq['GENE']\n",
    "gene_list_with_annot = gene_with_annot.drop_duplicates()\n",
    "\n",
    "\n",
    "print(num_of_CS_with_annot.shape[0])\n",
    "print(num_of_gene_with_annot.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fd62e3-636b-40cf-b7e7-e771008771dd",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "###  Summary of Fine-mapping Result Without Functional Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d25d7bc-cee2-45a4-9112-7cb0f021aee0",
   "metadata": {
    "kernel": "Python3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "# get the location of finemapping result files\n",
    "file_without_annot_location = os.path.join('/mnt', 'mfs', 'statgen','tl3030','AD_2021_output','without_annot', 'finemap.*.gz')\n",
    "\n",
    "import glob\n",
    "# get a list of result file name\n",
    "filenames_without_annot = glob.glob(file_without_annot_location)\n",
    "\n",
    "snp_without_annot = pd.DataFrame()\n",
    "\n",
    "for f in filenames_without_annot:\n",
    "    # read the data\n",
    "    outfile = pd.read_csv(f, delimiter = \"\\t\")\n",
    "    \n",
    "    # filter out SNPs that has PIP >= 0.95\n",
    "    significant = (outfile[outfile['PIP']>=0.95])\n",
    "    snp_without_annot = snp_without_annot.append(significant)\n",
    "\n",
    "\n",
    "# remove duplicated SNPs\n",
    "snp_without_annot_uniq = snp_without_annot.drop_duplicates(subset='SNP', keep='first')\n",
    "\n",
    "CS_without_annot = pd.DataFrame()\n",
    "\n",
    "for f in filenames_without_annot:\n",
    "    # read the data\n",
    "    outfile = pd.read_csv(f, delimiter = \"\\t\")\n",
    "    \n",
    "    # filter out SNPs that has CS\n",
    "    significant = (outfile[outfile['CREDIBLE_SET']>0])\n",
    "    CS_without_annot = CS_without_annot.append(significant)\n",
    "\n",
    "# remove duplicated SNPs\n",
    "CS_without_annot_uniq = CS_without_annot.drop_duplicates(subset='SNP', keep='first')\n",
    "\n",
    "\n",
    "# Read in the range file\n",
    "region_range = pd.read_csv(\"/mnt/mfs/statgen/tl3030/range.csv\").dropna()\n",
    "\n",
    "bpcol = CS_without_annot_uniq[['CHR', 'BP']]\n",
    "\n",
    "# Assign SNPs to the gene region that it belong to\n",
    "j = 0\n",
    "for i, bp in bpcol.iterrows():\n",
    "    #print(i, bp['CHR'])\n",
    "    for k,row in region_range.iterrows():\n",
    "        if (bp['CHR'] == row['Chr']) and (bp['BP'] > row['start']) and (bp['BP'] < row['end']):\n",
    "            CS_without_annot_uniq.iloc[j,15] = row['Gene Name']\n",
    "            #CS_without_annot_uniq.loc[j,'GENE']= row['Gene Name']\n",
    "    j += 1\n",
    "\n",
    "\n",
    "num_of_CS_without_annot = CS_without_annot_uniq.drop_duplicates(subset=['CREDIBLE_SET', 'GENE'], keep = 'last').reset_index(drop = True)\n",
    "num_of_CS_without_annot.sort_values(by=['GENE'])\n",
    "num_of_gene_without_annot = CS_without_annot_uniq.drop_duplicates(subset=['GENE'], keep = 'last').reset_index(drop = True)\n",
    "check_frequency_without_annot = CS_without_annot_uniq.groupby([\"CREDIBLE_SET\", \"GENE\"]).size().reset_index(name=\"Time\")\n",
    "CS_with_1_variant_without_annot = check_frequency_without_annot[check_frequency_without_annot['Time'] == 1]\n",
    "gene_without_annot = CS_without_annot_uniq['GENE']\n",
    "gene_list_without_annot = gene_without_annot.drop_duplicates()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Markdown",
     "markdown",
     "markdown",
     "",
     "markdown"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
