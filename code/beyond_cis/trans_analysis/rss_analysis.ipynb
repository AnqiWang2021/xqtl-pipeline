{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "heated-collins",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Fine-mapping with SuSiE RSS model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-thriller",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This notebook take a list of LD reference files and a list of sumstat files from various association studies ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-sullivan",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-digit",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "\n",
    "I. **GWAS Summary Statistics Files**\n",
    "- **Input**: Vector of files for one or more GWAS studies.\n",
    "- **Format**: \n",
    "  - Tab-delimited files.\n",
    "  - First 4 columns: `chrom`, `pos`, `A1`, `A2`\n",
    "  - Additional columns can be loaded using column mapping file see below  \n",
    "- **Column Mapping files (optional)**:\n",
    "  - Optional YAML file for custom column mapping.\n",
    "  - Required columns: `chrom`, `pos`, `A1`, `A2`, either `z` or (`beta` and `se`).\n",
    "  - Optional columns: `n`, `var_y`.\n",
    "\n",
    "II. **GWAS Summary Statistics Meta-File**: this is optional and helpful when there are lots of GWAS data to process via the same command\n",
    "- **Columns**: `study_id`, chromosome number, path to summary statistics file, optional path to column mapping file.\n",
    "- **Note**: Chromosome number `0` indicates a genome-wide file.\n",
    "\n",
    "eg: `gwas_meta.tsv`\n",
    "\n",
    "```\n",
    "study_id    chrom    file_path            column_mapping_file\n",
    "study1      1        gwas1.tsv.gz         column_mapping.yml\n",
    "study1      2        gwas2.tsv.gz         column_mapping.yml\n",
    "study2      0        gwas3.tsv.gz         column_mapping.yml\n",
    "```\n",
    "\n",
    "If both summary stats file (I) and meta data file (II) are specified we will take the union of the two.\n",
    "\n",
    "eg. `column_mapping.yml` left: standard name. Right: original column name.\n",
    "\n",
    "```\n",
    "chrom:chromosome\n",
    "pos:base_pair_location\n",
    "A1:effect_allele\n",
    "A2:other_allele\n",
    "beta:beta\n",
    "se:standard_error\n",
    "pvalue:p_value\n",
    "maf:maf\n",
    "n_cases:n_cases\n",
    "n_controls:n_controls\n",
    "```\n",
    "\n",
    "\n",
    "III. **LD Reference Metadata File**\n",
    "- **Format**: Single TSV file.\n",
    "- **Contents**:\n",
    "  - Columns: `chr`, `start`, `end`, path to the LD matrix, genomic build.\n",
    "  - LD matrix path format: comma-separated, first entry is the LD matrix, second is the bim file.\n",
    "- **Documentation**: Refer to our LD reference preparation document for detailed information (Tosin pending update).\n",
    "\n",
    "IV. For analyzing specific genomic regions, you can specify them using the `--region-names` option in the 'chr:start-end' format, where multiple regions are accepted. Alternatively, you may provide a file containing a list of regions through the `--region-list` option, also adhering to the 'chr:start-end' format. When both `--region-names` and `--region-list` are provided, union of these options will be used to analyze. In cases where neither option is specified, the analysis defaults to encompass all regions specified in the LD reference metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-argument",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-lobby",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1. A RDS file containing SuSiE output object.\n",
    "2. Summary statistics, QC-ed and QC summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-groove",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## MWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-biodiversity",
   "metadata": {
    "kernel": "Bash",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sos run xqtl-pipeline/pipeline/SuSiE_RSS.ipynb SuSiE_RSS \\\n",
    "    --ld-meta-data ADSP_R4_EUR.LD.list \\\n",
    "    --gwas-meta-data AD_sumstat_list.txt \\\n",
    "    --impute \\\n",
    "    --container oras://ghcr.io/cumc/pecotmr_apptainer:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-lecture",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path(\"output/\")\n",
    "parameter: gwas_meta_data = path()\n",
    "parameter: ld_meta_data = path()\n",
    "parameter: gwas_name = []\n",
    "parameter: gwas_data = []\n",
    "parameter: column_mapping = []\n",
    "parameter: region_list = path()\n",
    "parameter: region_name = []\n",
    "parameter: container = ''\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "parameter: job_size = 10\n",
    "parameter: walltime = \"5h\"\n",
    "parameter: mem = \"16G\"\n",
    "parameter: numThreads = 1\n",
    "parameter: impute = True # Whether to impute the sumstat for all the snp in LD but not in sumstat.\n",
    "parameter: QC = True\n",
    "\n",
    "def group_by_region(lst, partition):\n",
    "    # from itertools import accumulate\n",
    "    # partition = [len(x) for x in partition]\n",
    "    # Compute the cumulative sums once\n",
    "    # cumsum_vector = list(accumulate(partition))\n",
    "    # Use slicing based on the cumulative sums\n",
    "    # return [lst[(cumsum_vector[i-1] if i > 0 else 0):cumsum_vector[i]] for i in range(len(partition))]\n",
    "    return partition\n",
    "import os\n",
    "if (not os.path.isfile(region_list)) and len(region_name) == 0:\n",
    "    region_list = ld_meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-limit",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[get_analysis_regions: shared = \"regional_data\"]\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "def file_exists(file_path, relative_path=None):\n",
    "    \"\"\"Check if a file exists at the given path or relative to a specified path.\"\"\"\n",
    "    if os.path.exists(file_path) and os.path.isfile(file_path):\n",
    "        return True\n",
    "    elif relative_path:\n",
    "        relative_file_path = os.path.join(relative_path, file_path)\n",
    "        return os.path.exists(relative_file_path) and os.path.isfile(relative_file_path)\n",
    "    return False\n",
    "\n",
    "def check_required_columns(df, required_columns):\n",
    "    \"\"\"Check if the required columns are present in the dataframe.\"\"\"\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "def parse_region(region):\n",
    "    \"\"\"Parse a region string in 'chr:start-end' format into a list [chr, start, end].\"\"\"\n",
    "    chrom, rest = region.split(':')\n",
    "    start, end = rest.split('-')\n",
    "    return [int(chrom), int(start), int(end)]\n",
    "\n",
    "def extract_regional_data(gwas_meta_data, gwas_name, gwas_data, column_mapping, region_name=None, region_list=None):\n",
    "    \"\"\"\n",
    "    Extracts data from GWAS metadata files and additional GWAS data provided. \n",
    "    Optionally filters data based on specified regions.\n",
    "\n",
    "    Args:\n",
    "    - gwas_meta_data (str): File path to the GWAS metadata file.\n",
    "    - gwas_name (list): Vector of GWAS study names.\n",
    "    - gwas_data (list): Vector of GWAS data.\n",
    "    - column_mapping (list, optional): Vector of column mapping files.\n",
    "    - region_name (list, optional): List of region names in 'chr:start-end' format.\n",
    "    - region_list (str, optional): File path to a file containing regions.\n",
    "\n",
    "    Returns:\n",
    "    - GWAS Dictionary: Maps study IDs to a list containing chromosome number, \n",
    "      GWAS file path, and optional column mapping file path.\n",
    "    - Region Dictionary: Maps region names to lists [chr, start, end].\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If any specified file path does not exist.\n",
    "    - ValueError: If required columns are missing in the input files or vector lengths mismatch.\n",
    "    \"\"\"\n",
    "    # Check vector lengths\n",
    "    if len(gwas_name) != len(gwas_data):\n",
    "        raise ValueError(\"gwas_name and gwas_data must be of equal length\")\n",
    "    \n",
    "    if len(column_mapping) > 0 and len(column_mapping) != len(gwas_name):\n",
    "        raise ValueError(\"If column_mapping is provided, it must be of the same length as gwas_name and gwas_data\")\n",
    "\n",
    "    # Required columns for GWAS file type\n",
    "    required_gwas_columns = ['study_id', 'chrom', 'file_path']\n",
    "\n",
    "    # Base directory of the metadata files\n",
    "    gwas_base_dir = os.path.dirname(gwas_meta_data)\n",
    "    \n",
    "    # Reading the GWAS metadata file\n",
    "    gwas_df = pd.read_csv(gwas_meta_data, sep=\"\\t\")\n",
    "    check_required_columns(gwas_df, required_gwas_columns)\n",
    "    gwas_dict = OrderedDict()\n",
    "\n",
    "    # Process additional GWAS data from vectors\n",
    "    for name, data, mapping in zip(gwas_name, gwas_data, column_mapping or [None]*len(gwas_name)):\n",
    "        gwas_dict[name] = {0: [data, mapping]}\n",
    "\n",
    "    for _, row in gwas_df.iterrows():\n",
    "        file_path = row['file_path']\n",
    "        mapping_file = row.get('column_mapping_file')\n",
    "\n",
    "        # Check if the file and optional mapping file exist\n",
    "        if not file_exists(file_path, gwas_base_dir) or (mapping_file and not file_exists(mapping_file, gwas_base_dir)):\n",
    "            raise FileNotFoundError(f\"File {file_path} not found for {row['study_id']}\")\n",
    "        \n",
    "        # Adjust paths if necessary\n",
    "        file_path = file_path if file_exists(file_path) else os.path.join(gwas_base_dir, file_path)\n",
    "        if mapping_file:\n",
    "            mapping_file = mapping_file if file_exists(mapping_file) else os.path.join(gwas_base_dir, mapping_file)\n",
    "        \n",
    "        # Create or update the entry for the study_id\n",
    "        if row['study_id'] not in gwas_dict:\n",
    "            gwas_dict[row['study_id']] = {}\n",
    "\n",
    "        # Expand chrom 0 to chrom 1-22 or use the specified chrom\n",
    "        chrom_range = range(1, 23) if row['chrom'] == 0 else [row['chrom']]\n",
    "        for chrom in chrom_range:\n",
    "            if chrom in gwas_dict[row['study_id']]:\n",
    "                existing_entry = gwas_dict[row['study_id']][chrom]\n",
    "                raise ValueError(f\"Duplicate chromosome specification for study_id {row['study_id']}, chrom {chrom}. \"\n",
    "                                 f\"Conflicting entries: {existing_entry} and {[file_path, mapping_file]}\")\n",
    "            gwas_dict[row['study_id']][chrom] = [file_path, mapping_file]\n",
    "\n",
    "    # Process region_list and region_name\n",
    "    region_dict = dict()\n",
    "    if region_list and os.path.isfile(region_list):\n",
    "        with open(region_list, 'r') as file:\n",
    "            for line in file:\n",
    "                # Skip empty lines\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 1:\n",
    "                    region = parse_region(parts[0])\n",
    "                elif len(parts) >= 3:\n",
    "                    region = [int(parts[0]), int(parts[1]), int(parts[2])]\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid region format in region_list\")\n",
    "                \n",
    "                region_dict[f\"{region[0]}:{region[1]}-{region[2]}\"] = region\n",
    "                \n",
    "    if region_name:\n",
    "        for region in region_name:\n",
    "            parsed_region = parse_region(region)\n",
    "            region_key = f\"{parsed_region[0]}:{parsed_region[1]}-{parsed_region[2]}\"\n",
    "            if region_key not in region_dict:\n",
    "                region_dict[region_key] = parsed_region\n",
    "\n",
    "    return gwas_dict, region_dict\n",
    "\n",
    "gwas_dict, region_dict = extract_regional_data(gwas_meta_data, gwas_name, gwas_data, column_mapping, region_name, region_list)\n",
    "regional_data = dict([(\"GWAS\", gwas_dict), (\"regions\", region_dict)])\n",
    "print(regional_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-strip",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[SuSiE_RSS_1]\n",
    "parameter: L = 10\n",
    "parameter: max_L = 100\n",
    "# If available the column that indicates sample size within the sumstats\n",
    "parameter: sample_size_col = []\n",
    "# Sample size used to generate the sumstats\n",
    "parameter: sample_size = 0\n",
    "# filtering threshold for raiss imputation\n",
    "parameter: rcond = 0.01\n",
    "parameter: R2_threshold = 0.6\n",
    "depends: sos_variable(\"regional_data\")\n",
    "regions = list(regional_data['regions'].keys())\n",
    "studies = list(regional_data[\"GWAS\"].keys())\n",
    "input: for_each = [\"regions\", \"studies\"]\n",
    "output: f'{cwd:a}/{step_name[:-2]}/{_studies}.{_regions.replace(\":\", \"_\")}.susie_rss.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "    source(\"/home/hs3393/RSS_QC/pecotmr/R/raiss.R\")\n",
    "    library(dplyr)\n",
    "    devtools::load_all(\"/home/hs3393/RSS_QC/previous_version/susieR/\")\n",
    "    library(data.table)\n",
    "    sumstats=fread(\"${regional_data['GWAS'][_studies][regional_data['regions'][_regions][0]][0]}\")\n",
    "  \n",
    "    # rename the columns by yml file -- make the column names consistent\n",
    "    column_file_path = \"${regional_data['GWAS'][_studies][regional_data['regions'][_regions][0]][1]}\"\n",
    "    column_data <- read.table(column_file_path, header = FALSE, sep = \":\", stringsAsFactors = FALSE)\n",
    "    colnames(column_data) = c(\"standard\", \"original\")\n",
    "    count = 1\n",
    "    for (name in colnames(sumstats)){\n",
    "        if(name %in% column_data$original){\n",
    "            index = which(column_data$original == name)\n",
    "            colnames(sumstats)[count] = column_data$standard[index]\n",
    "        }\n",
    "        count = count + 1\n",
    "    }\n",
    "  \n",
    "    ## if the data don't have z scores, derive by beta/se, so that allele flip function can run\n",
    "    if(length(sumstats$z) == 0){\n",
    "          sumstats$z = sumstats$beta / sumstats$se\n",
    "    }\n",
    "  \n",
    "    ## if the data don't have beta, derive it by making beta = z and se =1, so that allele flip function can run\n",
    "    if(length(sumstats$beta) == 0){\n",
    "          sumstats$beta = sumstats$z\n",
    "          sumstats$se = 1\n",
    "    }\n",
    "    \n",
    "    ## load region infomation\n",
    "    region=data.frame(chrom = ${regional_data['regions'][_regions][0]},start = ${regional_data['regions'][_regions][1]},end = ${regional_data['regions'][_regions][2]})\n",
    "    LD_meta_file=read.table(\"${ld_meta_data}\", sep=\" \", header = FALSE, col.names = c(\"chrom\", \"start\", \"end\", \"path\"))\n",
    "    ## Step 1: Load summary stats and LD data for a region, and match them, using the function in pecotmr::LD.R\n",
    "    LD_data = load_LD_matrix(LD_meta_file, region, sumstats)\n",
    "    ## Step 2: basic QC between LD and summary stats --- to correct allele flipping mainly in pecotmr\n",
    "    allele_flip = allele_qc(sumstats, LD_data[[1]]$variants_df, match.min.prop=0.2, remove_dups=FALSE, flip=TRUE, remove=TRUE)\n",
    "    allele_flip = allele_flip %>% mutate(variant_allele_flip = paste(chrom,pos,A1.sumstats,A2.sumstats,sep=\":\"))\n",
    "    LD_extract = LD_data[[1]]$LD[allele_flip$variant_allele_flip,allele_flip$variant_allele_flip]\n",
    "    ## Step 3: Perform SuSiE RSS with QC using Gao's prototype\n",
    "    cols_sample_size=c(${','.join(['\"%s\"' % x for x in sample_size_col if x is not None])})\n",
    "    sample_size = ${sample_size}\n",
    "    L = ${L}\n",
    "    sample_size_col = c(${','.join(['\"%s\"' % x for x in sample_size_col if x is not None])})\n",
    "    ## get sample size: better specified. If not specified, calculate from median \"sample_size_col\". If columns to compute sample size not specified\n",
    "    ## make sample size = 0, so that susie_rss will run without n (not meaning n will = 0)\n",
    "    if(sample_size > 0){\n",
    "      n = sample_size\n",
    "    }else if(length(cols_sample_size) >= 1){\n",
    "      n_col_sum <- allele_flip$${sample_size_col[0]} + allele_flip$${sample_size_col[1]}\n",
    "      n = median(n_col_sum)\n",
    "    }else{\n",
    "      n = 0\n",
    "    }\n",
    "  \n",
    "    # if include QC step, then correct_zR_discrepancy = TRUE\n",
    "    if(${\"TRUE\" if QC else \"FALSE\"}){\n",
    "\n",
    "      if( n > 0){\n",
    "      susie_rss_result = susie_rss(bhat = allele_flip$beta, shat = allele_flip$se,\n",
    "                              R = LD_extract, n = n, L = L,\n",
    "                              correct_zR_discrepancy = TRUE, track_fit = FALSE)\n",
    "      }else{\n",
    "      # run without n\n",
    "      susie_rss_result = susie_rss(bhat = allele_flip$beta, shat = allele_flip$se,\n",
    "                              R = LD_extract, L = L,\n",
    "                              correct_zR_discrepancy = TRUE, track_fit = FALSE)\n",
    "      }\n",
    "\n",
    "      if(${\"TRUE\" if impute else \"FALSE\"}){\n",
    "        outlier = susie_rss_result$zR_outliers\n",
    "        if(length(outlier) == 0){\n",
    "            # no outliers, no need to imputation directly report fit result\n",
    "            result = susie_rss_result\n",
    "        }else{\n",
    "            # with outliers, raiss imputation\n",
    "            ref_panel = allele_flip %>% select(\"chrom\", \"pos\", \"variant_allele_flip\", \"A1.ref\", \"A2.ref\")\n",
    "            colnames(ref_panel) = c(\"chr\", \"pos\", \"variant_id\", \"A0\", \"A1\") \n",
    "            known_zscore =  allele_flip %>% select(\"chrom\", \"pos\", \"variant_allele_flip\", \"A1.ref\", \"A2.ref\", \"z\")\n",
    "            colnames(known_zscore) = c(\"chr\", \"pos\", \"variant_id\", \"A0\", \"A1\", \"Z\")\n",
    "            known_zscores = known_zscore[-outlier, ] %>% arrange(pos)\n",
    "            imputation_result = raiss(ref_panel, known_zscores, LD_extract, rcond = ${rcond}, R2_threshold = ${R2_threshold})\n",
    "            filtered_out_variant = setdiff(allele_flip$variant_allele_flip, imputation_result$variant_id)\n",
    "            filtered_out_id = which(allele_flip$variant_allele_flip %in% filtered_out_variant)\n",
    "            if(length(filtered_out_id) != 0){\n",
    "                LD_extract_filtered = as.matrix(LD_extract)[-filtered_out_id,-filtered_out_id]\n",
    "            }else{\n",
    "                LD_extract_filtered = as.matrix(LD_extract)\n",
    "\n",
    "            }\n",
    "            ## repeat step: get same sample size, if n = 0, run without n parameter\n",
    "            if(n > 0){\n",
    "            impute_rss_fit = susie_rss(z = imputation_result$Z, R = LD_extract_filtered, \n",
    "                               n = n,\n",
    "                               L = L, correct_zR_discrepancy = FALSE,\n",
    "                               track_fit = FALSE)\n",
    "            }else{\n",
    "            impute_rss_fit = susie_rss(z = imputation_result$Z, R = LD_extract_filtered, \n",
    "                               L = L, correct_zR_discrepancy = FALSE,\n",
    "                               track_fit = FALSE)        \n",
    "            }\n",
    "            result = impute_rss_fit\n",
    "            result$z = imputation_result$Z\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "      }else{\n",
    "        ## no imputation\n",
    "             result = susie_rss_result\n",
    "  \n",
    "  \n",
    "          }\n",
    "      }else{\n",
    "        ## no QC\n",
    "        if( n > 0){\n",
    "          result = susie_rss(bhat = allele_flip$beta, shat = allele_flip$se,\n",
    "                                  R = LD_extract, n = n, L = L,\n",
    "                                  correct_zR_discrepancy = FALSE, track_fit = FALSE)\n",
    "          }else{\n",
    "          # run without n\n",
    "          result = susie_rss(bhat = allele_flip$beta, shat = allele_flip$se,\n",
    "                                  R = LD_extract, L = L,\n",
    "                                  correct_zR_discrepancy = FALSE, track_fit = FALSE)\n",
    "          }\n",
    "          \n",
    "      }\n",
    "\n",
    "    saveRDS(result, file = \"${_output}\")\n",
    "    #write.table(allele_flip, \"${_output:n}.sumstats_qced\", sep = \"\\t\", col.names=TRUE, row.names=FALSE, quote=FALSE)\n",
    "\n",
    "\n",
    "    ## Output are 1) RDS file of fine-mapping results and 2) summary stats file for the region after allele flipping QC as well as the SuSiE RSS based QC\n",
    "    ## For fine-mapping results we would like to report both the top variant model (LD  reference free) and the conventional fine-mapping results\n",
    "\n",
    "    ## Ater that we repeat Step 1 and Step 3 with RSS QC (susie_rss as is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-antarctica",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[SuSiE_RSS_2]\n",
    "output: pip_plot = f\"{cwd}/{_input:bn}.png\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '12h', mem = '20G', cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: container=container, expand = \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint = entrypoint\n",
    "    res = readRDS(${_input:r})\n",
    "    png(${_output[0]:r}, width = 14, height=6, unit='in', res=300)\n",
    "    par(mfrow=c(1,2))\n",
    "    susieR::susie_plot(res, y= \"PIP\", pos=list(attr='pos',start=res$pos[1],end=res$pos[length(res$pos)]), add_legend=T, xlab=\"position\")\n",
    "    susieR::susie_plot(res, y= \"z\", pos=list(attr='pos',start=res$pos[1],end=res$pos[length(res$pos)]), add_legend=T, xlab=\"position\", ylab=\"-log10(p)\")\n",
    "    dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-newport",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[SuSiE_RSS_3]\n",
    "input: group_by = 'all'\n",
    "output: analysis_summary = f'{cwd}/{sumstats_path:bnn}.analysis_summary.md', variants_csv = f'{cwd}/{sumstats_path:bnn}.variants.csv'\n",
    "R: container=container, expand = \"${ }\", entrypoint = entrypoint\n",
    "    # Define the theme string\n",
    "    theme <- '---\n",
    "    theme: base-theme\n",
    "    style: |\n",
    "     p {\n",
    "       font-size: 24px;\n",
    "       height: 900px;\n",
    "       margin-top:1cm;\n",
    "      }\n",
    "      img {\n",
    "        height: 70%;\n",
    "        display: block;\n",
    "        margin-left: auto;\n",
    "        margin-right: auto;\n",
    "      }\n",
    "      body {\n",
    "       margin-top: auto;\n",
    "       margin-bottom: auto;\n",
    "       font-family: verdana;\n",
    "      }\n",
    "    ---    \n",
    "    '\n",
    "    text <- \"\"\n",
    "    sep <- '\\n\\n---\\n'\n",
    "\n",
    "    inp <- strsplit(\"${_input:r}\", \" \")[[1]]\n",
    "    inp <- sapply(inp, function(x) paste(head(strsplit(x, \"\\\\.\")[[1]], -1), collapse = \".\"))\n",
    "\n",
    "    r <- unique(strsplit(\"${_input:bn}\", \" \")[[1]])\n",
    "\n",
    "    num_csets <- numeric()\n",
    "    region_info <- character()\n",
    "\n",
    "    variant_info <- list()\n",
    "\n",
    "    for (reg_i in seq_along(unique(inp))) {\n",
    "\n",
    "      rid <- unlist(strsplit(r[reg_i], '\\\\.'))[1]\n",
    "\n",
    "      text_temp <- \"\"\n",
    "      text_temp <- paste0(text_temp, \"#\\n\\n SuSiE RSS \", r[reg_i], \" \\n\")\n",
    "      text_temp <- paste0(text_temp, \"![](\", r[reg_i], \".png)\", sep, \" \\n \\n\")\n",
    "\n",
    "      rd <- readRDS(substr(each, 2, nchar(each)) + \".rds\")\n",
    "\n",
    "      # find the number of cs in the current region\n",
    "      if (is.null(rd$sets$cs)) {\n",
    "        num_csets <- c(num_csets, 0)\n",
    "      } else {\n",
    "        num_csets <- c(num_csets, length(rd$sets$cs))\n",
    "      }\n",
    "      cat(num_csets, \"\\n\")\n",
    "\n",
    "      # this will store the indices of all variants that cross the threshold\n",
    "      ind_p <- which(rd$pip >= ${pip_cutoff})\n",
    "      sumvars <- 0\n",
    "\n",
    "      # if we have at least one cs in the current region\n",
    "      if (num_csets[reg_i] > 0) {\n",
    "        tbl_header <- \"| chr number | pos at highest pip | ref | alt | region id | cs | highest pip |  \\n| --- | --- | --- | --- | --- | --- | --- |  \\n\"\n",
    "\n",
    "        table <- \"\"\n",
    "\n",
    "        sumpips <- 0\n",
    "\n",
    "        for (cset in names(rd$sets$cs)) {\n",
    "          print(cset)\n",
    "\n",
    "          # if we have many variants in the cs\n",
    "          if (length(rd$sets$cs[[cset]]) > 1) {\n",
    "            highestpip <- max(rd$pip[rd$sets$cs[[cset]]])\n",
    "            poswhighestpip <- which.max(rd$pip[rd$sets$cs[[cset]]])\n",
    "\n",
    "            # we make sure that ind_p only stores the variants that aren't in any cs\n",
    "            ind_p <- setdiff(ind_p, rd$sets$cs[[cset]])\n",
    "\n",
    "            # append variant info\n",
    "            i <- poswhighestpip\n",
    "            variant_info[[length(variant_info) + 1]] <- list(rd$chr[i], rd$pos[i], rd$ref[i], rd$alt[i], rid, cset, rd$pip[i])\n",
    "\n",
    "            table <- paste0(table, \"| \", rd$chr[i], \" | \", rd$pos[i], \" | \", rd$ref[i], \" | \", rd$alt[i], \" | \", rid, \" | \", cset, \" | \", sprintf(\"%.2f\", rd$pip[i]), \" |  \\n\")\n",
    "\n",
    "            sumpips <- sumpips + sum(rd$pip[rd$sets$cs[[cset]]])\n",
    "            sumvars <- sumvars + length(rd$sets$cs[[cset]])\n",
    "          } else { # if we have only one variant in the cs\n",
    "            i <- rd$sets$cs[[cset]]\n",
    "\n",
    "            # we make sure that ind_p only stores the variants that aren't in any cs\n",
    "            ind_p <- setdiff(ind_p, i)\n",
    "\n",
    "            # append variant info\n",
    "            variant_info[[length(variant_info) + 1]] <- list(rd$chr[i], rd$pos[i], rd$ref[i], rd$alt[i], rid, cset, rd$pip[i])\n",
    "\n",
    "            table <- paste0(table, \"| \", rd$chr[i], \" | \", rd$pos[i], \" | \", rd$ref[i], \" | \", rd$alt[i], \" | \", rid, \" | \", cset, \" | \", sprintf(\"%.2f\", rd$pip[i]), \" |  \\n\")\n",
    "\n",
    "            sumpips <- sumpips + rd$pip[i]\n",
    "            sumvars <- sumvars + 1\n",
    "          }\n",
    "        }\n",
    "\n",
    "        text_temp <- paste0(text_temp, \"- Total number of variants: \", length(rd$pip), \"\\n\")\n",
    "        text_temp <- paste0(text_temp, \"- Expected number of causal variants: \", sprintf(\"%.2f\", sumpips), \"\\n\")\n",
    "        text_temp <- paste0(text_temp, \"- Number of variants with PIP > \", ${pip_cutoff}, \" and not in any CS: \", length(ind_p), \"\\n\\n\")\n",
    "        text_temp <- paste0(text_temp, tbl_header, table, sep)\n",
    "\n",
    "        if (num_csets[reg_i] > 1) {\n",
    "          text_temp <- paste0(text_temp, \"#### CORR: Correlation between CS | OLAP: Overlap between CS\\n\")\n",
    "\n",
    "          cs <- names(rd$sets$cs)\n",
    "\n",
    "          corrheader <- \"|  |\"\n",
    "          corrbreak <- \"| --- |\"\n",
    "\n",
    "          for (i in cs) {\n",
    "            corrheader <- paste0(corrheader, \" CORR \", i, \" |\")\n",
    "            corrbreak <- paste0(corrbreak, \" --- |\")\n",
    "          }\n",
    "\n",
    "          corrheader <- paste0(corrheader, \"  |\")\n",
    "          corrbreak <- paste0(corrbreak, \" --- |\")\n",
    "\n",
    "          for (i in cs) {\n",
    "            corrheader <- paste0(corrheader, \" OLAP \", i, \" |\")\n",
    "            corrbreak <- paste0(corrbreak, \" --- |\")\n",
    "          }\n",
    "\n",
    "          corrheader <- paste0(corrheader, \"\\n\")\n",
    "          corrbreak <- paste0(corrbreak, \"\\n\")\n",
    "\n",
    "          body <- \"\"\n",
    "\n",
    "          for (en in seq_along(cs)) {\n",
    "            i <- cs[en]\n",
    "            body <- paste0(body, \"| \", i, \" |\")\n",
    "            for (j in rd$cscorr[[en]]) {\n",
    "              body <- paste0(body, \" \", sprintf(\"%.2f\", j), \" |\")\n",
    "            }\n",
    "            body <- paste0(body, \"  |\")\n",
    "            for (j in names(rd$sets$cs)) {\n",
    "              body <- paste0(body, \" \", length(intersect(rd$sets$cs[[i]], rd$sets$cs[[j]])), \" |\")\n",
    "            }\n",
    "            body <- paste0(body, \"\\n\")\n",
    "          }\n",
    "\n",
    "          text_temp <- paste0(text_temp, corrheader, corrbreak, body, sep)\n",
    "        }\n",
    "\n",
    "        region_info <- c(region_info, text_temp)\n",
    "      }\n",
    "    }\n",
    "\n",
    "    f <- file(${_output[\"analysis_summary\"]:r}, \"w\")\n",
    "    writeLines(paste0(theme, text), f)\n",
    "    close(f)\n",
    "\n",
    "    for (i in ind_p) {\n",
    "      # append variant info\n",
    "      variant_info[[length(variant_info) + 1]] <- list(rd$chr[i], rd$pos[i], rd$ref[i], rd$alt[i], rid, \"None\", rd$pip[i])\n",
    "    }\n",
    "\n",
    "    df <- do.call(rbind, variant_info)\n",
    "    colnames(df) <- c(\"chr\", \"pos\", \"ref\", \"alt\", \"rid\", \"cs\", \"pip\")\n",
    "    write.table(df, ${_output[\"variants_csv\"]:r}, sep = \"\\t\", row.names = TRUE, col.names = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-optics",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Generate analysis report: HTML file, and optionally PPTX file\n",
    "[SuSiE_RSS_4]\n",
    "output: f\"{_input['analysis_summary']:n}.html\"\n",
    "sh: container=container_marp, expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint = entrypoint\n",
    "    node /opt/marp/.cli/marp-cli.js ${_input['analysis_summary']} -o ${_output:a} \\\n",
    "        --title '${region_file:bnn} fine mapping analysis' \\\n",
    "        --allow-local-files\n",
    "    node /opt/marp/.cli/marp-cli.js ${_input['analysis_summary']} -o ${_output:an}.pptx \\\n",
    "        --title '${region_file:bnn} fine mapping analysis' \\\n",
    "        --allow-local-files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.24.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
