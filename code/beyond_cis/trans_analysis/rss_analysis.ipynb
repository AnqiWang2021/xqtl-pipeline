{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incident-jason",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Fine-mapping with SuSiE RSS model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-glasgow",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This notebook take a list of LD reference files and a list of sumstat files from various association studies ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-smooth",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-guitar",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1. **FIXME we need to make input as a bed file with chrom, start and end** A tab delimated table describing the path where LD per region stored, can be generated using the ld_per_region_plink step of the genotype processing module.\n",
    "\n",
    "```\n",
    "#id     dir\n",
    "chr17_60570445_65149278 /mnt/vast/hpc/csg/molecular_phenotype_calling/LD/output_npz_2/1300_hg38_EUR_LD_blocks_npz_files/ROSMAP_NIA_WGS.leftnorm.filtered.filtered.chr17_60570445_65149278.flt16.npz\n",
    "```\n",
    "\n",
    "2. A tab delimated table describing path where summary stat per chromosome stored, can be generated using the yml_generator module before the qced sumstat are generated. **FIXME: If the chrom name is zero that means the data is genome-wide**\n",
    "```\n",
    "hs3163@csglogin:/mnt/vast/hpc/csg/xqtl_workflow_testing/susie_rss$ cat /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/qced_sumstat_list.txt\n",
    "#chr    ADGWAS_Bellenguez_2022\n",
    "1       /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/ADGWAS_Bellenguez_2022.1/ADGWAS2022.chr1.sumstat.tsv\n",
    "2       /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/ADGWAS_Bellenguez_2022.2/ADGWAS2022.chr2.sumstat.tsv\n",
    "3       /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/ADGWAS_Bellenguez_2022.3/ADGWAS2022.chr3.sumstat.tsv\n",
    "4       /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/ADGWAS_Bellenguez_2022.4/ADGWAS2022.chr4.sumstat.tsv\n",
    "5       /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/ADGWAS_Bellenguez_2022.5/ADGWAS2022.chr5.sumstat.tsv\n",
    "```\n",
    "\n",
    "3. Regions we want to analyze in the format `chr:start-end`. Can be multiple of these. If not specified we will use the regions in the LD data list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-semester",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-listening",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1. A RDS file containing the output susie object, the name of all variants that went through the analysis, the z score , and the LD used for the analysis.\n",
    "2. A sumstat file with additional column containing the slalom results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-official",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## MWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-performer",
   "metadata": {
    "kernel": "Bash",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sos run pipeline/SuSiE_RSS.ipynb SuSiE_RSS \\\n",
    "    --ld-data test.ld.list \\\n",
    "    --sumstats /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/qced_sumstat_list.txt \\\n",
    "    --container oras://ghcr.io/cumc/pecotmr_apptainer:latest --impute --cwd output_impute_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-delhi",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path(\"output\")\n",
    "# getting the overlapped input\n",
    "parameter: ld_data = path\n",
    "parameter: sumstats = paths\n",
    "import pandas as pd\n",
    "parameter: container = \"\"\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 3\n",
    "\n",
    "parameter: lead_idx_choice = \"pvalue\"\n",
    "parameter: abf_prior_variance = 0.4\n",
    "parameter: nlog10p_dentist_s_threshold = 4\n",
    "parameter: r2_threshold = 0.6\n",
    "parameter: n = 0\n",
    "parameter: max_iter = 1000\n",
    "parameter: impute = True # Whether to impute the sumstat for all the snp in LD but not in sumstat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-beach",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[get_analysis_regions: shared = \"regional_data\"]\n",
    "# This will pair the LD matrix blocks with each of the input summary stats\n",
    "\n",
    "LD_list = pd.read_csv(LD_list,sep=\"\\t\")\n",
    "sumstat_list = pd.read_csv(sumstats,sep=\"\\t\")\n",
    "LD_list[\"#chr\"] = [x[0].replace(\"chr\", \"\") for x in  LD_list[\"#id\"].str.split(\"_\") ]\n",
    "sumstat_list[\"#chr\"] = [str(x).replace(\"chr\", \"\") for x in  sumstat_list[\"#chr\"] ]\n",
    "input_inv = LD_list.merge(sumstat_list)\n",
    "input_list = input_inv.iloc[:,[1,3]].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-caribbean",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[SuSiE_RSS_1]\n",
    "parameter: L = 10\n",
    "parameter: max_L = 1000\n",
    "\n",
    "depends: sos_variable(\"regional_data\")\n",
    "\n",
    "meta_info = regional_data['meta_info']\n",
    "input: regional_data[\"data\"], group_by = 2, group_with = \"meta_info\"\n",
    "# name = f'{_input[0]:b}'.split(\".\")[-3]\n",
    "output: f'{cwd:a}/{_input[1]:bn}.{name}.unisusie_rss.fit.rds',\n",
    "        f'{cwd:a}/{_input[1]:bn}.{name}.unisusie_rss.ss_qced.tsv'    \n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output[0]:nn}.stdout\", stderr = f\"{_output[0]:nn}.stderr\", container = container, entrypoint = entrypoint\n",
    "  \n",
    "    ## Step 1: Load summary stats and LD data for a region, and match them, using the function in pecotmr::LD.R\n",
    "\n",
    "    ## Step 2: basic QC between LD and summary stats --- to correct allele flipping mainly in pecotmr \n",
    "  \n",
    "    ## Step 3: Perform SuSiE RSS with QC using my prototype\n",
    "  \n",
    "    ## Output are 1) RDS file of fine-mapping results and 2) summary stats file for the region after allele flipping QC as well as the SuSiE RSS based QC\n",
    "    ## For fine-mapping results we would like to report both the top variant model (LD  reference free) and the conventional fine-mapping results\n",
    "  \n",
    "    ## Ater that we repeat Step 1 and Step 3 with RSS QC (susie_rss as is). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-flood",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[SuSiE_RSS_2]\n",
    "output: pip_plot = f\"{cwd}/{_input:bn}.png\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '12h', mem = '20G', cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: container=container, expand = \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint = entrypoint\n",
    "    res = readRDS(${_input:r})\n",
    "    png(${_output[0]:r}, width = 14, height=6, unit='in', res=300)\n",
    "    par(mfrow=c(1,2))\n",
    "    susieR::susie_plot(res, y= \"PIP\", pos=list(attr='pos',start=res$pos[1],end=res$pos[length(res$pos)]), add_legend=T, xlab=\"position\")\n",
    "    susieR::susie_plot(res, y= \"z\", pos=list(attr='pos',start=res$pos[1],end=res$pos[length(res$pos)]), add_legend=T, xlab=\"position\", ylab=\"-log10(p)\")\n",
    "    dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-spare",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[SuSiE_RSS_3]\n",
    "input: group_by = 'all'\n",
    "output: analysis_summary = f'{cwd}/{sumstats_path:bnn}.analysis_summary.md', variants_csv = f'{cwd}/{sumstats_path:bnn}.variants.csv'\n",
    "R: container=container, expand = \"${ }\", entrypoint = entrypoint\n",
    "    # Define the theme string\n",
    "    theme <- '---\n",
    "    theme: base-theme\n",
    "    style: |\n",
    "     p {\n",
    "       font-size: 24px;\n",
    "       height: 900px;\n",
    "       margin-top:1cm;\n",
    "      }\n",
    "      img {\n",
    "        height: 70%;\n",
    "        display: block;\n",
    "        margin-left: auto;\n",
    "        margin-right: auto;\n",
    "      }\n",
    "      body {\n",
    "       margin-top: auto;\n",
    "       margin-bottom: auto;\n",
    "       font-family: verdana;\n",
    "      }\n",
    "    ---    \n",
    "    '\n",
    "    text <- \"\"\n",
    "    sep <- '\\n\\n---\\n'\n",
    "\n",
    "    inp <- strsplit(\"${_input:r}\", \" \")[[1]]\n",
    "    inp <- sapply(inp, function(x) paste(head(strsplit(x, \"\\\\.\")[[1]], -1), collapse = \".\"))\n",
    "\n",
    "    r <- unique(strsplit(\"${_input:bn}\", \" \")[[1]])\n",
    "\n",
    "    num_csets <- numeric()\n",
    "    region_info <- character()\n",
    "\n",
    "    variant_info <- list()\n",
    "\n",
    "    for (reg_i in seq_along(unique(inp))) {\n",
    "\n",
    "      rid <- unlist(strsplit(r[reg_i], '\\\\.'))[1]\n",
    "\n",
    "      text_temp <- \"\"\n",
    "      text_temp <- paste0(text_temp, \"#\\n\\n SuSiE RSS \", r[reg_i], \" \\n\")\n",
    "      text_temp <- paste0(text_temp, \"![](\", r[reg_i], \".png)\", sep, \" \\n \\n\")\n",
    "\n",
    "      rd <- readRDS(substr(each, 2, nchar(each)) + \".rds\")\n",
    "\n",
    "      # find the number of cs in the current region\n",
    "      if (is.null(rd$sets$cs)) {\n",
    "        num_csets <- c(num_csets, 0)\n",
    "      } else {\n",
    "        num_csets <- c(num_csets, length(rd$sets$cs))\n",
    "      }\n",
    "      cat(num_csets, \"\\n\")\n",
    "\n",
    "      # this will store the indices of all variants that cross the threshold\n",
    "      ind_p <- which(rd$pip >= ${pip_cutoff})\n",
    "      sumvars <- 0\n",
    "\n",
    "      # if we have at least one cs in the current region\n",
    "      if (num_csets[reg_i] > 0) {\n",
    "        tbl_header <- \"| chr number | pos at highest pip | ref | alt | region id | cs | highest pip |  \\n| --- | --- | --- | --- | --- | --- | --- |  \\n\"\n",
    "\n",
    "        table <- \"\"\n",
    "\n",
    "        sumpips <- 0\n",
    "\n",
    "        for (cset in names(rd$sets$cs)) {\n",
    "          print(cset)\n",
    "\n",
    "          # if we have many variants in the cs\n",
    "          if (length(rd$sets$cs[[cset]]) > 1) {\n",
    "            highestpip <- max(rd$pip[rd$sets$cs[[cset]]])\n",
    "            poswhighestpip <- which.max(rd$pip[rd$sets$cs[[cset]]])\n",
    "\n",
    "            # we make sure that ind_p only stores the variants that aren't in any cs\n",
    "            ind_p <- setdiff(ind_p, rd$sets$cs[[cset]])\n",
    "\n",
    "            # append variant info\n",
    "            i <- poswhighestpip\n",
    "            variant_info[[length(variant_info) + 1]] <- list(rd$chr[i], rd$pos[i], rd$ref[i], rd$alt[i], rid, cset, rd$pip[i])\n",
    "\n",
    "            table <- paste0(table, \"| \", rd$chr[i], \" | \", rd$pos[i], \" | \", rd$ref[i], \" | \", rd$alt[i], \" | \", rid, \" | \", cset, \" | \", sprintf(\"%.2f\", rd$pip[i]), \" |  \\n\")\n",
    "\n",
    "            sumpips <- sumpips + sum(rd$pip[rd$sets$cs[[cset]]])\n",
    "            sumvars <- sumvars + length(rd$sets$cs[[cset]])\n",
    "          } else { # if we have only one variant in the cs\n",
    "            i <- rd$sets$cs[[cset]]\n",
    "\n",
    "            # we make sure that ind_p only stores the variants that aren't in any cs\n",
    "            ind_p <- setdiff(ind_p, i)\n",
    "\n",
    "            # append variant info\n",
    "            variant_info[[length(variant_info) + 1]] <- list(rd$chr[i], rd$pos[i], rd$ref[i], rd$alt[i], rid, cset, rd$pip[i])\n",
    "\n",
    "            table <- paste0(table, \"| \", rd$chr[i], \" | \", rd$pos[i], \" | \", rd$ref[i], \" | \", rd$alt[i], \" | \", rid, \" | \", cset, \" | \", sprintf(\"%.2f\", rd$pip[i]), \" |  \\n\")\n",
    "\n",
    "            sumpips <- sumpips + rd$pip[i]\n",
    "            sumvars <- sumvars + 1\n",
    "          }\n",
    "        }\n",
    "\n",
    "        text_temp <- paste0(text_temp, \"- Total number of variants: \", length(rd$pip), \"\\n\")\n",
    "        text_temp <- paste0(text_temp, \"- Expected number of causal variants: \", sprintf(\"%.2f\", sumpips), \"\\n\")\n",
    "        text_temp <- paste0(text_temp, \"- Number of variants with PIP > \", ${pip_cutoff}, \" and not in any CS: \", length(ind_p), \"\\n\\n\")\n",
    "        text_temp <- paste0(text_temp, tbl_header, table, sep)\n",
    "\n",
    "        if (num_csets[reg_i] > 1) {\n",
    "          text_temp <- paste0(text_temp, \"#### CORR: Correlation between CS | OLAP: Overlap between CS\\n\")\n",
    "\n",
    "          cs <- names(rd$sets$cs)\n",
    "\n",
    "          corrheader <- \"|  |\"\n",
    "          corrbreak <- \"| --- |\"\n",
    "\n",
    "          for (i in cs) {\n",
    "            corrheader <- paste0(corrheader, \" CORR \", i, \" |\")\n",
    "            corrbreak <- paste0(corrbreak, \" --- |\")\n",
    "          }\n",
    "\n",
    "          corrheader <- paste0(corrheader, \"  |\")\n",
    "          corrbreak <- paste0(corrbreak, \" --- |\")\n",
    "\n",
    "          for (i in cs) {\n",
    "            corrheader <- paste0(corrheader, \" OLAP \", i, \" |\")\n",
    "            corrbreak <- paste0(corrbreak, \" --- |\")\n",
    "          }\n",
    "\n",
    "          corrheader <- paste0(corrheader, \"\\n\")\n",
    "          corrbreak <- paste0(corrbreak, \"\\n\")\n",
    "\n",
    "          body <- \"\"\n",
    "\n",
    "          for (en in seq_along(cs)) {\n",
    "            i <- cs[en]\n",
    "            body <- paste0(body, \"| \", i, \" |\")\n",
    "            for (j in rd$cscorr[[en]]) {\n",
    "              body <- paste0(body, \" \", sprintf(\"%.2f\", j), \" |\")\n",
    "            }\n",
    "            body <- paste0(body, \"  |\")\n",
    "            for (j in names(rd$sets$cs)) {\n",
    "              body <- paste0(body, \" \", length(intersect(rd$sets$cs[[i]], rd$sets$cs[[j]])), \" |\")\n",
    "            }\n",
    "            body <- paste0(body, \"\\n\")\n",
    "          }\n",
    "\n",
    "          text_temp <- paste0(text_temp, corrheader, corrbreak, body, sep)\n",
    "        }\n",
    "\n",
    "        region_info <- c(region_info, text_temp)\n",
    "      }\n",
    "    }\n",
    "\n",
    "    f <- file(${_output[\"analysis_summary\"]:r}, \"w\")\n",
    "    writeLines(paste0(theme, text), f)\n",
    "    close(f)\n",
    "\n",
    "    for (i in ind_p) {\n",
    "      # append variant info\n",
    "      variant_info[[length(variant_info) + 1]] <- list(rd$chr[i], rd$pos[i], rd$ref[i], rd$alt[i], rid, \"None\", rd$pip[i])\n",
    "    }\n",
    "\n",
    "    df <- do.call(rbind, variant_info)\n",
    "    colnames(df) <- c(\"chr\", \"pos\", \"ref\", \"alt\", \"rid\", \"cs\", \"pip\")\n",
    "    write.table(df, ${_output[\"variants_csv\"]:r}, sep = \"\\t\", row.names = TRUE, col.names = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-daisy",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Generate analysis report: HTML file, and optionally PPTX file\n",
    "[SuSiE_RSS_4]\n",
    "output: f\"{_input['analysis_summary']:n}.html\"\n",
    "sh: container=container_marp, expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint = entrypoint\n",
    "    node /opt/marp/.cli/marp-cli.js ${_input['analysis_summary']} -o ${_output:a} \\\n",
    "        --title '${region_file:bnn} fine mapping analysis' \\\n",
    "        --allow-local-files\n",
    "    node /opt/marp/.cli/marp-cli.js ${_input['analysis_summary']} -o ${_output:an}.pptx \\\n",
    "        --title '${region_file:bnn} fine mapping analysis' \\\n",
    "        --allow-local-files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
