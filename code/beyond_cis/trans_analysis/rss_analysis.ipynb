{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hairy-cross",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Fine-mapping with SuSiE RSS model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-christmas",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This notebook take a list of LD reference files and a list of sumstat files from various association studies ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-default",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-consensus",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1. **FIXME we need to make input as a bed file with chrom, start and end** A tab delimated table describing the path where LD per region stored, can be generated using the ld_per_region_plink step of the genotype processing module.\n",
    "\n",
    "```\n",
    "#id     dir\n",
    "chr17_60570445_65149278 /mnt/vast/hpc/csg/molecular_phenotype_calling/LD/output_npz_2/1300_hg38_EUR_LD_blocks_npz_files/ROSMAP_NIA_WGS.leftnorm.filtered.filtered.chr17_60570445_65149278.flt16.npz\n",
    "```\n",
    "\n",
    "2. A tab delimated table describing path where summary stat per chromosome stored, can be generated using the yml_generator module before the qced sumstat are generated. **FIXME: If the chrom name is zero that means the data is genome-wide**\n",
    "```\n",
    "hs3163@csglogin:/mnt/vast/hpc/csg/xqtl_workflow_testing/susie_rss$ cat /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/qced_sumstat_list.txt\n",
    "#chr    ADGWAS_Bellenguez_2022\n",
    "1       /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/ADGWAS_Bellenguez_2022.1/ADGWAS2022.chr1.sumstat.tsv\n",
    "2       /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/ADGWAS_Bellenguez_2022.2/ADGWAS2022.chr2.sumstat.tsv\n",
    "3       /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/ADGWAS_Bellenguez_2022.3/ADGWAS2022.chr3.sumstat.tsv\n",
    "4       /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/ADGWAS_Bellenguez_2022.4/ADGWAS2022.chr4.sumstat.tsv\n",
    "5       /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/ADGWAS_Bellenguez_2022.5/ADGWAS2022.chr5.sumstat.tsv\n",
    "```\n",
    "\n",
    "3. Regions we want to analyze in the format `chr:start-end`. Can be multiple of these. If not specified we will use the regions in the LD data list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-delight",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-capture",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1. A RDS file containing the output susie object, the name of all variants that went through the analysis, the z score , and the LD used for the analysis.\n",
    "2. A sumstat file with additional column containing the slalom results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-overhead",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## MWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-central",
   "metadata": {
    "kernel": "Bash",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sos run pipeline/SuSiE_RSS.ipynb SuSiE_RSS \\\n",
    "    --ld-data test.ld.list \\\n",
    "    --sumstats /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/qced_sumstat_list.txt \\\n",
    "    --container containers/stephenslab.sif --impute --cwd output_impute_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-france",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path(\"output\")\n",
    "# getting the overlapped input\n",
    "parameter: ld_data = path\n",
    "parameter: sumstats = paths\n",
    "import pandas as pd\n",
    "parameter: container = \"\"\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 3\n",
    "\n",
    "parameter: lead_idx_choice = \"pvalue\"\n",
    "parameter: abf_prior_variance = 0.4\n",
    "parameter: nlog10p_dentist_s_threshold = 4\n",
    "parameter: r2_threshold = 0.6\n",
    "parameter: n = 0\n",
    "parameter: max_iter = 1000\n",
    "parameter: impute = True # Whether to impute the sumstat for all the snp in LD but not in sumstat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944952a9-0329-45f4-9e07-1aff0076fada",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[get_analysis_regions: shared = \"regional_data\"]\n",
    "# This will pair the LD matrix blocks with each of the input summary stats\n",
    "\n",
    "LD_list = pd.read_csv(LD_list,sep=\"\\t\")\n",
    "sumstat_list = pd.read_csv(sumstats,sep=\"\\t\")\n",
    "LD_list[\"#chr\"] = [x[0].replace(\"chr\", \"\") for x in  LD_list[\"#id\"].str.split(\"_\") ]\n",
    "sumstat_list[\"#chr\"] = [str(x).replace(\"chr\", \"\") for x in  sumstat_list[\"#chr\"] ]\n",
    "input_inv = LD_list.merge(sumstat_list)\n",
    "input_list = input_inv.iloc[:,[1,3]].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-baseball",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[SuSiE_RSS_1]\n",
    "parameter: L = 10\n",
    "parameter: max_L = 1000\n",
    "\n",
    "depends: sos_variable(\"regional_data\")\n",
    "\n",
    "meta_info = regional_data['meta_info']\n",
    "input: regional_data[\"data\"], group_by = 2, group_with = \"meta_info\"\n",
    "# name = f'{_input[0]:b}'.split(\".\")[-3]\n",
    "output: f'{cwd:a}/{_input[1]:bn}.{name}.unisusie_rss.fit.rds',\n",
    "        f'{cwd:a}/{_input[1]:bn}.{name}.unisusie_rss.ss_qced.tsv'    \n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output[0]:nn}.stdout\", stderr = f\"{_output[0]:nn}.stderr\", container = container, entrypoint = entrypoint\n",
    "  \n",
    "    ## Step 1: Load summary stats and LD data for a region, and match them, using the function in pecotmr::LD.R\n",
    "\n",
    "    ## Step 2: basic QC between LD and summary stats --- to correct allele flipping mainly in pecotmr \n",
    "  \n",
    "    ## Step 3: Perform SuSiE RSS with QC using my prototype\n",
    "  \n",
    "    ## Output are 1) RDS file of fine-mapping results and 2) summary stats file for the region after allele flipping QC as well as the SuSiE RSS based QC\n",
    "  \n",
    "    ## Ater that we repeat Step 1 and Step 3 with RSS QC (susie_rss as is). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-banking",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[SuSiE_RSS_2]\n",
    "output: pip_plot = f\"{cwd}/{_input:bn}.png\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '12h', mem = '20G', cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: container=container, expand = \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint = entrypoint\n",
    "    res = readRDS(${_input:r})\n",
    "    png(${_output[0]:r}, width = 14, height=6, unit='in', res=300)\n",
    "    par(mfrow=c(1,2))\n",
    "    susieR::susie_plot(res, y= \"PIP\", pos=list(attr='pos',start=res$pos[1],end=res$pos[length(res$pos)]), add_legend=T, xlab=\"position\")\n",
    "    susieR::susie_plot(res, y= \"z\", pos=list(attr='pos',start=res$pos[1],end=res$pos[length(res$pos)]), add_legend=T, xlab=\"position\", ylab=\"-log10(p)\")\n",
    "    dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-sight",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[SuSiE_RSS_3]\n",
    "sep = \"\" #'\\n\\n---\\n'\n",
    "input: group_by = 'all'\n",
    "output: analysis_summary = f'{cwd}/{sumstats_path:bnn}.analysis_summary.md', variants_csv = f'{cwd}/{sumstats_path:bnn}.variants.csv'\n",
    "python: container=container, expand = \"${ }\", entrypoint = entrypoint\n",
    "\n",
    "    theme = '''---\n",
    "    theme: base-theme\n",
    "    style: |\n",
    "     p {\n",
    "       font-size: 24px;\n",
    "       height: 900px;\n",
    "       margin-top:1cm;\n",
    "      }\n",
    "      img {\n",
    "        height: 70%;\n",
    "        display: block;\n",
    "        margin-left: auto;\n",
    "        margin-right: auto;\n",
    "      }\n",
    "      body {\n",
    "       margin-top: auto;\n",
    "       margin-bottom: auto;\n",
    "       font-family: verdana;\n",
    "      }\n",
    "    ---    \n",
    "    '''\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # will load the rds file outputted in a previous step\n",
    "    def load_rds(filename, types=None):\n",
    "        import os\n",
    "        import pandas as pd, numpy as np\n",
    "        import rpy2.robjects as RO\n",
    "        import rpy2.robjects.vectors as RV\n",
    "        import rpy2.rinterface as RI\n",
    "        from rpy2.robjects import numpy2ri\n",
    "        numpy2ri.activate()\n",
    "        from rpy2.robjects import pandas2ri\n",
    "        pandas2ri.activate()\n",
    "        def load(data, types, rpy2_version=3):\n",
    "            if types is not None and not isinstance(data, types):\n",
    "                return np.array([])\n",
    "            # FIXME: I'm not sure if I should keep two versions here\n",
    "            # rpy2_version 2.9.X is more tedious but it handles BoolVector better\n",
    "            # rpy2 version 3.0.1 converts bool to integer directly without dealing with\n",
    "            # NA properly. It gives something like (0,1,-234235).\n",
    "            # Possibly the best thing to do is to open an issue for it to the developers.\n",
    "            if rpy2_version == 2:\n",
    "                # below works for rpy2 version 2.9.X\n",
    "                if isinstance(data, RI.RNULLType):\n",
    "                    res = None\n",
    "                elif isinstance(data, RV.BoolVector):\n",
    "                    data = RO.r['as.integer'](data)\n",
    "                    res = np.array(data, dtype=int)\n",
    "                    # Handle c(NA, NA) situation\n",
    "                    if np.sum(np.logical_and(res != 0, res != 1)):\n",
    "                        res = res.astype(float)\n",
    "                        res[res < 0] = np.nan\n",
    "                        res[res > 1] = np.nan\n",
    "                elif isinstance(data, RV.FactorVector):\n",
    "                    data = RO.r['as.character'](data)\n",
    "                    res = np.array(data, dtype=str)\n",
    "                elif isinstance(data, RV.IntVector):\n",
    "                    res = np.array(data, dtype=int)\n",
    "                elif isinstance(data, RV.FloatVector):\n",
    "                    res = np.array(data, dtype=float)\n",
    "                elif isinstance(data, RV.StrVector):\n",
    "                    res = np.array(data, dtype=str)\n",
    "                elif isinstance(data, RV.DataFrame):\n",
    "                    res = pd.DataFrame(data)\n",
    "                elif isinstance(data, RV.Matrix):\n",
    "                    res = np.matrix(data)\n",
    "                elif isinstance(data, RV.Array):\n",
    "                    res = np.array(data)\n",
    "                else:\n",
    "                    # I do not know what to do for this\n",
    "                    # But I do not want to throw an error either\n",
    "                    res = str(data)\n",
    "            else:\n",
    "                if isinstance(data, RI.NULLType):\n",
    "                    res = None\n",
    "                else:\n",
    "                    res = data\n",
    "            if isinstance(res, np.ndarray) and res.shape == (1, ):\n",
    "                res = res[0]\n",
    "            return res\n",
    "        def load_dict(res, data, types):\n",
    "            '''load data to res'''\n",
    "            names = data.names if not isinstance(data.names, RI.NULLType) else [\n",
    "                i + 1 for i in range(len(data))\n",
    "            ]\n",
    "            for name, value in zip(names, list(data)):\n",
    "                if isinstance(value, RV.ListVector):\n",
    "                    res[name] = {}\n",
    "                    res[name] = load_dict(res[name], value, types)\n",
    "                else:\n",
    "                    res[name] = load(value, types)\n",
    "            return res\n",
    "        #\n",
    "        if not os.path.isfile(filename):\n",
    "            raise IOError('Cannot find file ``{}``!'.format(filename))\n",
    "        rds = RO.r['readRDS'](filename)\n",
    "        if isinstance(rds, RV.ListVector):\n",
    "            res = load_dict({}, rds, types)\n",
    "        else:\n",
    "            res = load(rds, types)\n",
    "        return res\n",
    "    \n",
    "    def f7(seq):\n",
    "        seen = set()\n",
    "        seen_add = seen.add\n",
    "        return [x for x in seq if not (x in seen or seen_add(x))]\n",
    "\n",
    "\n",
    "\n",
    "    text = \"\"\n",
    "    sep = '\\n\\n---\\n'\n",
    "    \n",
    "    inp = \"${_input:r}\".split(\" \")\n",
    "    for i, each in enumerate(inp):\n",
    "        inp[i] = \".\".join(each.split(\".\")[:-1])\n",
    "\n",
    "    r = f7(\"${_input:bn}\".split(\" \"))\n",
    "    \n",
    "    num_csets = []\n",
    "    region_info = []\n",
    "    \n",
    "    # this will be a 2d array that stores information about each variant of interest in the phenotype\n",
    "    # this includes all the variants in a cs and all the variants past the cutoff\n",
    "    variant_info = []\n",
    "\n",
    "    for reg_i, each in enumerate(f7(inp)):\n",
    "    \n",
    "        rid = r[reg_i].split('.')[0]\n",
    "        \n",
    "        text_temp = \"\"\n",
    "        text_temp += \"#\\n\\n SuSiE RSS {region} \\n\".format(region=r[reg_i])\n",
    "        text_temp += \"![]({region}.png){sep} \\n \\n\".format(region=r[reg_i], sep=sep)\n",
    "\n",
    "        rd = load_rds(each[1:]+\".rds\")\n",
    "        \n",
    "        # find the number of cs in the current region\n",
    "        if rd[\"sets\"][\"cs\"] == None:\n",
    "            num_csets.append(0)\n",
    "        else:\n",
    "            num_csets.append(len(rd[\"sets\"][\"cs\"]))\n",
    "        print(num_csets)\n",
    "        \n",
    "        # this will store the indicies of all variants that cross the threshold\n",
    "        ind_p = []\n",
    "\n",
    "        pval = ${pip_cutoff}\n",
    "\n",
    "        for i, each in enumerate(rd[\"pip\"]):\n",
    "            if each >= pval:\n",
    "                ind_p.append(i)\n",
    "        sumvars = 0\n",
    "        \n",
    "        # if we have at least one cs in the current region\n",
    "        if num_csets[reg_i] > 0:\n",
    "            tbl_header = \"| chr number | pos at highest pip | ref | alt | region id | cs | highest pip |  \\n\"\n",
    "            tbl_header += \"| --- | --- | --- | --- | --- | --- | --- |  \\n\"\n",
    "\n",
    "            table = \"\"\n",
    "            \n",
    "            sumpips = 0\n",
    "            \n",
    "            for cset in rd[\"sets\"][\"cs\"].keys():\n",
    "                print(cset)\n",
    "                \n",
    "                # if we have many variants in the cs\n",
    "                if isinstance(rd[\"sets\"][\"cs\"][cset], np.ndarray):\n",
    "                    highestpip = 0\n",
    "                    poswhighestpip = -1\n",
    "                    for i in rd[\"sets\"][\"cs\"][cset]:\n",
    "                        i = i.item() - 1\n",
    "                        \n",
    "                        # we make sure that ind_p only stores the variants that aren't in any cs\n",
    "                        if i in ind_p: ind_p.remove(i) \n",
    "                        \n",
    "                        # append variant info\n",
    "                        variant_info.append( [rd[\"chr\"][i], rd[\"pos\"][i], rd[\"ref\"][i], rd[\"alt\"][i], rid, cset, rd[\"pip\"][i]] )\n",
    "                        \n",
    "                        if rd[\"pip\"][i] > highestpip:\n",
    "                            highestpip = rd[\"pip\"][i]\n",
    "                            poswhighestpip = i\n",
    "                            \n",
    "                        sumpips += rd[\"pip\"][i]\n",
    "                        sumvars += 1\n",
    "                        \n",
    "                    if poswhighestpip > -1:\n",
    "                        i = poswhighestpip\n",
    "                        table += \"| {chr} | {pos} | {ref} | {alt} | {rid} | {cs} | {pip:.2f} |  \\n\".format(chr=rd[\"chr\"][i], pos=rd[\"pos\"][i], ref=rd[\"ref\"][i], alt=rd[\"alt\"][i], rid=rid, cs=cset, pip=rd[\"pip\"][i])\n",
    "                \n",
    "                else: # if we have only one variant in the cs\n",
    "                    i =  rd[\"sets\"][\"cs\"][cset]\n",
    "                    i = i.item() - 1\n",
    "                    \n",
    "                    # we make sure that ind_p only stores the variants that aren't in any cs\n",
    "                    if i in ind_p: ind_p.remove(i)\n",
    "                    \n",
    "                    # append variant info\n",
    "                    variant_info.append( [rd[\"chr\"][i], rd[\"pos\"][i], rd[\"ref\"][i], rd[\"alt\"][i], rid, cset, rd[\"pip\"][i]] )\n",
    "                    \n",
    "                    table += \"| {chr} | {pos} | {ref} | {alt} | {rid} | {cs} | {pip:.2f} |  \\n\".format(chr=rd[\"chr\"][i], pos=rd[\"pos\"][i], ref=rd[\"ref\"][i], alt=rd[\"alt\"][i], rid=rid, cs=cset, pip=rd[\"pip\"][i])\n",
    "                    \n",
    "                    sumpips += rd[\"pip\"][i]\n",
    "                    sumvars += 1\n",
    "            \n",
    "\n",
    "            text_temp += \"- Total number of variants: {}\\n\".format(len(rd[\"pip\"]))\n",
    "            text_temp += \"- Expected number of causal variants: {:.2f}\\n\".format(sumpips)\n",
    "            text_temp += \"- Number of variants with PIP > {} and not in any CS: {}\\n\\n\".format(pval, len(ind_p))\n",
    "            text_temp += tbl_header + table + sep\n",
    "            \n",
    "            if num_csets[reg_i] > 1:\n",
    "                text_temp += \"#### CORR: Correlation between CS | OLAP: Overlap between CS\\n\"\n",
    "                \n",
    "                cs = list(rd[\"sets\"][\"cs\"].keys())\n",
    "\n",
    "                corrheader = \"|  |\"\n",
    "                corrbreak = \"| --- |\"\n",
    "\n",
    "                for i in cs:\n",
    "                    corrheader += \" CORR {} |\".format(i)\n",
    "                    corrbreak += \" --- |\"\n",
    "                    \n",
    "                corrheader += \"  |\"\n",
    "                corrbreak += \" --- |\"\n",
    "                    \n",
    "                for i in cs:\n",
    "                    corrheader += \" OLAP {} |\".format(i)\n",
    "                    corrbreak += \" --- |\"\n",
    "\n",
    "                corrheader += \"\\n\"\n",
    "                corrbreak += \"\\n\"\n",
    "\n",
    "                body = \"\"\n",
    "\n",
    "                for en, i in enumerate(cs):\n",
    "                    body += \"| {} |\".format(i)\n",
    "                    for j in rd[\"cscorr\"][en]:\n",
    "                        body += \" {:.2f} |\".format(j)\n",
    "                    body += \"  |\"\n",
    "                    for j in rd[\"sets\"][\"cs\"]:\n",
    "                        body += \" {} |\".format(len(np.intersect1d(rd[\"sets\"][\"cs\"][i], rd[\"sets\"][\"cs\"][j])))\n",
    "                    body += \"\\n\"\n",
    "                \n",
    "                text_temp += corrheader + corrbreak + body + sep\n",
    "            \n",
    "        region_info.append(text_temp)\n",
    "            \n",
    "    f = open(${_output[\"analysis_summary\"]:r}, \"w\")\n",
    "    \n",
    "    cset_order = np.argsort(num_csets)\n",
    "    cset_order = cset_order.tolist()\n",
    "    cset_order.reverse()\n",
    "    for c in cset_order:\n",
    "        text += region_info[c]\n",
    "    \n",
    "    f.write(theme + text)\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    for i in ind_p:\n",
    "        # append variant info\n",
    "        variant_info.append( [rd[\"chr\"][i], rd[\"pos\"][i], rd[\"ref\"][i], rd[\"alt\"][i], rid, \"None\", rd[\"pip\"][i]] )\n",
    "        \n",
    "    df = pd.DataFrame(variant_info, columns=[\"chr\", \"pos\", \"ref\", \"alt\", \"rid\", \"cs\", \"pip\"])\n",
    "    df.to_csv(${_output[\"variants_csv\"]:r}, sep = \"\\t\", header = True, index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-ocean",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Generate analysis report: HTML file, and optionally PPTX file\n",
    "[SuSiE_RSS_4]\n",
    "output: f\"{_input['analysis_summary']:n}.html\"\n",
    "sh: container=container_marp, expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint = entrypoint\n",
    "    node /opt/marp/.cli/marp-cli.js ${_input['analysis_summary']} -o ${_output:a} \\\n",
    "        --title '${region_file:bnn} fine mapping analysis' \\\n",
    "        --allow-local-files\n",
    "    node /opt/marp/.cli/marp-cli.js ${_input['analysis_summary']} -o ${_output:an}.pptx \\\n",
    "        --title '${region_file:bnn} fine mapping analysis' \\\n",
    "        --allow-local-files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.20.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
