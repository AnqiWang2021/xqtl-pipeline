{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "heated-collins",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Fine-mapping with SuSiE model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-thriller",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This notebook take a list of regions of interest, one genotype file and a list of phenotype files to perform fine-mapping for individual level data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-sullivan",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-digit",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1. A list of regions to be analyzed. If it is more than 3 columns then the last column of this file will be treated as region name.\n",
    "2. Either a list of per chromosome genotype files, or one file for genotype data of the entire genome. Genotype data has to be in PLINK `bed` format. \n",
    "3. Phenotype file: first column must be sample ID and other columns each for a phenotype ID. It should have a header.\n",
    "4. Covariate file: first column must be sample ID and other columns are covariates. It should have a header. **All covariates in this file will be analyzed as adjustment in linear regression.**\n",
    "\n",
    "### Example genotype data\n",
    "\n",
    "```\n",
    "#chr        path\n",
    "chr21 /mnt/mfs/statgen/xqtl_workflow_testing/protocol_example.genotype.chr21.bed\n",
    "chr22 /mnt/mfs/statgen/xqtl_workflow_testing/protocol_example.genotype.chr22.bed\n",
    "```\n",
    "\n",
    "Alternatively, simply use `protocol_example.genotype.chr21_22.bed` if all chromosomes are in the same file.\n",
    "\n",
    "### Example region list file\n",
    "\n",
    "It should have 3 or 4 columns, with the header a commented out line:\n",
    "\n",
    "```\n",
    "#chr    start    end    region_id\n",
    "chr10   0    6480000    ENSG00000008128\n",
    "chr1    0    6480000    ENSG00000008130\n",
    "chr1    0    6480000    ENSG00000067606\n",
    "chr1    0    7101193    ENSG00000069424\n",
    "chr1    0    7960000    ENSG00000069812\n",
    "chr1    0    6480000    ENSG00000078369\n",
    "chr1    0    6480000    ENSG00000078808\n",
    "```\n",
    "\n",
    "Or, simply\n",
    "\n",
    "```\n",
    "#chr    start    end    \n",
    "chr10   0    6480000    \n",
    "chr1    0    6480000    \n",
    "chr1    0    6480000    \n",
    "chr1    0    7101193    \n",
    "chr1    0    7960000    \n",
    "chr1    0    6480000    \n",
    "chr1    0    6480000    \n",
    "```\n",
    "\n",
    "### About indels\n",
    "\n",
    "Option `--no-indel` will remove indel from analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-argument",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-lobby",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1. A RDS file containing SuSiE output object.\n",
    "2. Some visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-groove",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal Working Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-biodiversity",
   "metadata": {
    "kernel": "Bash",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sos run pipeline/trans_finemapping.ipynb SuSiE  \\\n",
    "    --name dmd_analysis  \\\n",
    "    --genoFile protocol_example.genotype.chr21_22.bed   \\\n",
    "    --phenoFile study_1_phenotypes.tsv \\\n",
    "    --covFile study_1_covariates.tsv \\\n",
    "    --region-list regions_of_interest.tsv \\\n",
    "    --container oras://ghcr.io/cumc/pecotmr_apptainer:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a02605b-a630-4538-b626-9a0432e0816a",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Workflow implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-lecture",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path(\"output\")\n",
    "# A list of file paths for genotype data, or the genotype data itself. \n",
    "parameter: genoFile = path\n",
    "# One or multiple lists of file paths for phenotype data.\n",
    "parameter: phenoFile = path\n",
    "# Covariate file path\n",
    "parameter: covFile = path\n",
    "parameter: region_list = path\n",
    "# Only focus on a subset of samples\n",
    "parameter: keep_samples = path()\n",
    "# It is required to input the name of the analysis\n",
    "parameter: name = str\n",
    "parameter: container = \"\"\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 50\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"10m\"\n",
    "# Memory expected\n",
    "parameter: mem = \"20G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 1\n",
    "\n",
    "def group_by_region(lst, partition):\n",
    "    # from itertools import accumulate\n",
    "    # partition = [len(x) for x in partition]\n",
    "    # Compute the cumulative sums once\n",
    "    # cumsum_vector = list(accumulate(partition))\n",
    "    # Use slicing based on the cumulative sums\n",
    "    # return [lst[(cumsum_vector[i-1] if i > 0 else 0):cumsum_vector[i]] for i in range(len(partition))]\n",
    "    return partition\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def adapt_file_path(file_path, reference_file):\n",
    "    \"\"\"\n",
    "    Adapt a single file path based on its existence and a reference file's path.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The file path to adapt.\n",
    "    - reference_file (str): File path to use as a reference for adaptation.\n",
    "\n",
    "    Returns:\n",
    "    - str: Adapted file path.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If no valid file path is found.\n",
    "    \"\"\"\n",
    "    reference_path = os.path.dirname(reference_file)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.isfile(file_path):\n",
    "        return file_path\n",
    "\n",
    "    # Check file name without path\n",
    "    file_name = os.path.basename(file_path)\n",
    "    if os.path.isfile(file_name):\n",
    "        return file_name\n",
    "\n",
    "    # Check file name in reference file's directory\n",
    "    file_in_ref_dir = os.path.join(reference_path, file_name)\n",
    "    if os.path.isfile(file_in_ref_dir):\n",
    "        return file_in_ref_dir\n",
    "\n",
    "    # Check original file path prefixed with reference file's directory\n",
    "    file_prefixed = os.path.join(reference_path, file_path)\n",
    "    if os.path.isfile(file_prefixed):\n",
    "        return file_prefixed\n",
    "\n",
    "    # If all checks fail, raise an error\n",
    "    raise FileNotFoundError(f\"No valid path found for file: {file_path}\")\n",
    "\n",
    "def adapt_file_path_all(df, column_name, reference_file):\n",
    "    return df[column_name].apply(lambda x: adapt_file_path(x, reference_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-limit",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[get_analysis_regions: shared = \"regional_data\"]\n",
    "# input is genoFile, phenoFile, covFile and optionally region_list. If region_list presents then we only analyze what's contained in the list.\n",
    "# regional_data should be a dictionary like:\n",
    "#{'data': [(\"genotype_1\", \"phenotype_1\", \"covariate_1\"), (\"genotype_1\", \"phenotype_1\", \"covariate_1\"), ... ],\n",
    "# 'meta_info': [(\"chr12:752578-752579\", \"region_id_1\", \"trait_1\", \"trait_2\"), (\"chr13:852580-852581\", \"region_id_2\", \"trait_1\", \"trait_2\") ... ]}\n",
    "\n",
    "def make_meta_data(region_list, region_ids, geno_meta_data, pheno_file, cov_file):\n",
    "    '''\n",
    "    Example output:\n",
    "    #chr    start       end           ID  path     cov_path             cond             geno_path\n",
    "    chr12   652578   852579  chr12_652578_852579  phenotype.tsv  covar.tsv  trait_A,trait_B   protocol_example.genotype.bed\n",
    "    '''\n",
    "    if len(region_ids) != len(region_list):\n",
    "        raise ValueError(\"Length of region_ids does not match the number of rows in region_list.\")\n",
    "\n",
    "    # Read phenotype file\n",
    "    pheno_df = pd.read_csv(pheno_file, sep='\\t', header=0)\n",
    "    # Create a comma-separated string of phenotype columns, excluding the first column\n",
    "    phenotypes = ','.join(pheno_df.columns[1:])\n",
    "\n",
    "    # Create the output DataFrame\n",
    "    output_df = pd.DataFrame({\n",
    "        '#chr': region_list['chr'],\n",
    "        'start': region_list['start'],\n",
    "        'end': region_list['end'],\n",
    "        'ID': region_ids,\n",
    "        'path': pheno_file,\n",
    "        'cov_path': cov_file,\n",
    "        'cond': phenotypes\n",
    "    })\n",
    "\n",
    "    def find_genotype_path(chrom):\n",
    "        match = geno_meta_data[geno_meta_data['#chr'] == chrom]\n",
    "        if not match.empty:\n",
    "            return match.iloc[0]['genotype_file_path']\n",
    "        else:\n",
    "            raise ValueError(f\"Chromosome {chrom} not found in geno_meta_data.\")\n",
    "\n",
    "    # Apply the function to find the genotype path for each row\n",
    "    output_df['geno_path'] = output_df['#chr'].apply(find_genotype_path)\n",
    "    return output_df    \n",
    "\n",
    "# Load phenotype meta data\n",
    "if len(phenoFile) != len(covFile):\n",
    "    raise ValueError(\"Number of input phenotypes files must match that of covariates files\")\n",
    "if len(phenoFile) != len(phenotype_names):\n",
    "    raise ValueError(\"Number of input phenotypes files must match the number of phenotype names\")\n",
    "\n",
    "# Load genotype meta data\n",
    "if f\"{genoFile:x}\" == \".bed\":\n",
    "    geno_meta_data = pd.DataFrame([(\"chr\"+str(x), f\"{genoFile:a}\") for x in range(1,23)] + [(\"chrX\", f\"{genoFile:a}\")], columns=['#chr', 'geno_path'])\n",
    "else:\n",
    "    geno_meta_data = pd.read_csv(f\"{genoFile:a}\", sep = \"\\t\", header=0)\n",
    "    geno_meta_data.iloc[:, 1] = adapt_file_path_all(geno_meta_data, geno_meta_data.columns[1], f\"{genoFile:a}\")\n",
    "    geno_meta_data.columns = ['#chr', 'geno_path']\n",
    "    geno_meta_data['#chr'] = geno_meta_data['#chr'].apply(lambda x: str(x) if str(x).startswith('chr') else f'chr{x}')\n",
    "\n",
    "# Checking the DataFrame\n",
    "valid_chr_values = [f'chr{x}' for x in range(1, 23)] + ['chrX']\n",
    "if not all(value in valid_chr_values for value in geno_meta_data['#chr']):\n",
    "    raise ValueError(\"Invalid chromosome values found. Allowed values are chr1 to chr22 and chrX.\")\n",
    "\n",
    "if len(meta_data.index) == 0:\n",
    "    raise ValueError(\"No region overlap between genotype and any of the phenotypes\")\n",
    "\n",
    "region_ids = []\n",
    "if region_list.is_file():\n",
    "    region_list_df = pd.read_csv(region_list, sep = \"\\t\", header=None, comment = \"#\")\n",
    "    if region_list_df.shape[1] > 3:\n",
    "        # More than 3 columns, extract the last column as ID\n",
    "        region_ids = region_list_df.iloc[:, -1].unique()\n",
    "    elif region_list_df.shape[1] == 3:\n",
    "        # Exactly 3 columns, concatenate the values from the first 3 columns to form the ID\n",
    "        region_ids = region_list_df.astype(str).apply(lambda x: '_'.join(x), axis=1).unique()\n",
    "    else:\n",
    "        raise ValueError(f\"Region file ``{region_list}`` has fewer than 3 columns.\")\n",
    "else:\n",
    "    raise ValueError(f\"Region file ``{region_list}`` is not found!\")\n",
    "    \n",
    "print(region_list)\n",
    "print(region_id)\n",
    "\n",
    "# Create the final dictionary\n",
    "regional_data = {\n",
    "    'data': [(row['geno_path'], *row['path'].split(','), *row['cov_path'].split(',')) for _, row in meta_data.iterrows()],\n",
    "    'meta_info': [(f\"{row['#chr']}:{row['start']}-{row['end']}\", # this is the phenotype region\n",
    "                   f\"{row['#chr']}:{row['start_cis']}-{row['end_cis']}\", # this is the cis-window region\n",
    "                   row['ID'], *row['cond'].split(',')) for _, row in meta_data.iterrows()]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-strip",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[SuSiE_RSS_1]\n",
    "parameter: L = 10\n",
    "parameter: max_L = 100\n",
    "# If available the column that indicates sample size within the sumstats\n",
    "parameter: sample_size_col = []\n",
    "# Sample size used to generate the sumstats\n",
    "parameter: sample_size = 0\n",
    "# filtering threshold for raiss imputation\n",
    "parameter: rcond = 0.01\n",
    "parameter: R2_threshold = 0.6\n",
    "depends: sos_variable(\"regional_data\")\n",
    "regions = list(regional_data['regions'].keys())\n",
    "studies = list(regional_data[\"GWAS\"].keys())\n",
    "input: for_each = [\"regions\", \"studies\"]\n",
    "output: f'{cwd:a}/{step_name[:-2]}/{_studies}.{_regions.replace(\":\", \"_\")}.susie_rss.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(pecotmr)\n",
    "    library(dplyr)\n",
    "    library(susieR)\n",
    "    library(data.table)\n",
    "    sumstats=fread(\"${regional_data['GWAS'][_studies][regional_data['regions'][_regions][0]][0]}\")\n",
    "  \n",
    "    # rename the columns by yml file -- make the column names consistent\n",
    "    column_file_path = \"${regional_data['GWAS'][_studies][regional_data['regions'][_regions][0]][1]}\"\n",
    "    column_data <- read.table(column_file_path, header = FALSE, sep = \":\", stringsAsFactors = FALSE)\n",
    "    colnames(column_data) = c(\"standard\", \"original\")\n",
    "    count = 1\n",
    "    for (name in colnames(sumstats)){\n",
    "        if(name %in% column_data$original){\n",
    "            index = which(column_data$original == name)\n",
    "            colnames(sumstats)[count] = column_data$standard[index]\n",
    "        }\n",
    "        count = count + 1\n",
    "    }\n",
    "  \n",
    "    ## if the data don't have z scores, derive by beta/se, so that allele flip function can run\n",
    "    if(length(sumstats$z) == 0){\n",
    "          sumstats$z = sumstats$beta / sumstats$se\n",
    "    }\n",
    "  \n",
    "    ## if the data don't have beta, derive it by making beta = z and se =1, so that allele flip function can run\n",
    "    if(length(sumstats$beta) == 0){\n",
    "          sumstats$beta = sumstats$z\n",
    "          sumstats$se = 1\n",
    "    }\n",
    "    \n",
    "    ## load region infomation\n",
    "    region=data.frame(chrom = ${regional_data['regions'][_regions][0]},start = ${regional_data['regions'][_regions][1]},end = ${regional_data['regions'][_regions][2]})\n",
    "    LD_meta_file=read.table(\"${ld_meta_data}\", sep=\" \", header = FALSE, col.names = c(\"chrom\", \"start\", \"end\", \"path\"))\n",
    "    ## Step 1: Load summary stats and LD data for a region, and match them, using the function in pecotmr::LD.R\n",
    "    LD_data = load_LD_matrix(LD_meta_file, region, sumstats)\n",
    "    ## Step 2: basic QC between LD and summary stats --- to correct allele flipping mainly in pecotmr\n",
    "    allele_flip = allele_qc(sumstats, LD_data[[1]]$variants_df, match.min.prop=0.2, remove_dups=FALSE, flip=TRUE, remove=TRUE)\n",
    "    allele_flip = allele_flip %>% mutate(variant_allele_flip = paste(chrom,pos,A1.sumstats,A2.sumstats,sep=\":\"))\n",
    "    LD_extract = LD_data[[1]]$LD[allele_flip$variant_allele_flip,allele_flip$variant_allele_flip]\n",
    "    ## Step 3: Perform SuSiE RSS with QC using Gao's prototype\n",
    "    cols_sample_size=c(${','.join(['\"%s\"' % x for x in sample_size_col if x is not None])})\n",
    "    sample_size = ${sample_size}\n",
    "    L = ${L}\n",
    "    sample_size_col = c(${','.join(['\"%s\"' % x for x in sample_size_col if x is not None])})\n",
    "    ## get sample size: better specified. If not specified, calculate from median \"sample_size_col\". If columns to compute sample size not specified\n",
    "    ## make sample size = 0, so that susie_rss will run without n (not meaning n will = 0)\n",
    "    if(sample_size > 0){\n",
    "      n = sample_size\n",
    "    }else if(length(cols_sample_size) >= 1){\n",
    "      n_col_sum <- allele_flip$${sample_size_col[0]} + allele_flip$${sample_size_col[1]}\n",
    "      n = median(n_col_sum)\n",
    "    }else{\n",
    "      n = 0\n",
    "    }\n",
    "  \n",
    "    # if include QC step, then correct_zR_discrepancy = TRUE\n",
    "    if(${\"TRUE\" if QC else \"FALSE\"}){\n",
    "\n",
    "      if( n > 0){\n",
    "      susie_rss_result = susie_rss(bhat = allele_flip$beta, shat = allele_flip$se,\n",
    "                              R = LD_extract, n = n, L = L,\n",
    "                              correct_zR_discrepancy = TRUE, track_fit = FALSE)\n",
    "      }else{\n",
    "      # run without n\n",
    "      susie_rss_result = susie_rss(bhat = allele_flip$beta, shat = allele_flip$se,\n",
    "                              R = LD_extract, L = L,\n",
    "                              correct_zR_discrepancy = TRUE, track_fit = FALSE)\n",
    "      }\n",
    "\n",
    "      if(${\"TRUE\" if impute else \"FALSE\"}){\n",
    "        outlier = susie_rss_result$zR_outliers\n",
    "        if(length(outlier) == 0){\n",
    "            # no outliers, no need to imputation directly report fit result\n",
    "            result = susie_rss_result\n",
    "        }else{\n",
    "            # with outliers, raiss imputation\n",
    "            ref_panel = allele_flip %>% select(\"chrom\", \"pos\", \"variant_allele_flip\", \"A1.ref\", \"A2.ref\")\n",
    "            colnames(ref_panel) = c(\"chr\", \"pos\", \"variant_id\", \"A0\", \"A1\") \n",
    "            known_zscore =  allele_flip %>% select(\"chrom\", \"pos\", \"variant_allele_flip\", \"A1.ref\", \"A2.ref\", \"z\")\n",
    "            colnames(known_zscore) = c(\"chr\", \"pos\", \"variant_id\", \"A0\", \"A1\", \"Z\")\n",
    "            known_zscores = known_zscore[-outlier, ] %>% arrange(pos)\n",
    "            imputation_result = raiss(ref_panel, known_zscores, LD_extract, rcond = ${rcond}, R2_threshold = ${R2_threshold})\n",
    "            filtered_out_variant = setdiff(allele_flip$variant_allele_flip, imputation_result$variant_id)\n",
    "            filtered_out_id = which(allele_flip$variant_allele_flip %in% filtered_out_variant)\n",
    "            if(length(filtered_out_id) != 0){\n",
    "                LD_extract_filtered = as.matrix(LD_extract)[-filtered_out_id,-filtered_out_id]\n",
    "            }else{\n",
    "                LD_extract_filtered = as.matrix(LD_extract)\n",
    "\n",
    "            }\n",
    "            ## repeat step: get same sample size, if n = 0, run without n parameter\n",
    "            if(n > 0){\n",
    "            impute_rss_fit = susie_rss(z = imputation_result$Z, R = LD_extract_filtered, \n",
    "                               n = n,\n",
    "                               L = L, correct_zR_discrepancy = FALSE,\n",
    "                               track_fit = FALSE)\n",
    "            }else{\n",
    "            impute_rss_fit = susie_rss(z = imputation_result$Z, R = LD_extract_filtered, \n",
    "                               L = L, correct_zR_discrepancy = FALSE,\n",
    "                               track_fit = FALSE)        \n",
    "            }\n",
    "            result = impute_rss_fit\n",
    "            result$z = imputation_result$Z\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "      }else{\n",
    "        ## no imputation\n",
    "             result = susie_rss_result\n",
    "  \n",
    "  \n",
    "          }\n",
    "      }else{\n",
    "        ## no QC\n",
    "        if( n > 0){\n",
    "          result = susie_rss(bhat = allele_flip$beta, shat = allele_flip$se,\n",
    "                                  R = LD_extract, n = n, L = L,\n",
    "                                  correct_zR_discrepancy = FALSE, track_fit = FALSE)\n",
    "          }else{\n",
    "          # run without n\n",
    "          result = susie_rss(bhat = allele_flip$beta, shat = allele_flip$se,\n",
    "                                  R = LD_extract, L = L,\n",
    "                                  correct_zR_discrepancy = FALSE, track_fit = FALSE)\n",
    "          }\n",
    "          \n",
    "      }\n",
    "\n",
    "    saveRDS(result, file = \"${_output}\")\n",
    "    #write.table(allele_flip, \"${_output:n}.sumstats_qced\", sep = \"\\t\", col.names=TRUE, row.names=FALSE, quote=FALSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.24.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
