{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "heated-collins",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Fine-mapping with SuSiE model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-thriller",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This notebook take a list of regions of interest, one genotype file and a list of phenotype files to perform fine-mapping for individual level data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-sullivan",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-digit",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1. A list of regions to be analyzed. If it is more than 3 columns then the last column of this file will be treated as region name.\n",
    "2. Either a list of per chromosome genotype files, or one file for genotype data of the entire genome. Genotype data has to be in PLINK `bed` format. \n",
    "3. Phenotype file: first column must be sample ID and other columns each for a phenotype ID. It should have a header.\n",
    "4. Covariate file: first column must be sample ID and other columns are covariates. It should have a header. **All covariates in this file will be analyzed as adjustment in linear regression.**\n",
    "\n",
    "### Example genotype data\n",
    "\n",
    "```\n",
    "#chr        path\n",
    "chr21 /mnt/mfs/statgen/xqtl_workflow_testing/protocol_example.genotype.chr21.bed\n",
    "chr22 /mnt/mfs/statgen/xqtl_workflow_testing/protocol_example.genotype.chr22.bed\n",
    "```\n",
    "\n",
    "Alternatively, simply use `protocol_example.genotype.chr21_22.bed` if all chromosomes are in the same file.\n",
    "\n",
    "### Example region list file\n",
    "\n",
    "It should have 3 or 4 columns, with the header a commented out line:\n",
    "\n",
    "```\n",
    "#chr    start    end    region_id\n",
    "chr10   0    6480000    ENSG00000008128\n",
    "chr1    0    6480000    ENSG00000008130\n",
    "chr1    0    6480000    ENSG00000067606\n",
    "chr1    0    7101193    ENSG00000069424\n",
    "chr1    0    7960000    ENSG00000069812\n",
    "chr1    0    6480000    ENSG00000078369\n",
    "chr1    0    6480000    ENSG00000078808\n",
    "```\n",
    "\n",
    "Or, simply\n",
    "\n",
    "```\n",
    "#chr    start    end    \n",
    "chr10   0    6480000    \n",
    "chr1    0    6480000    \n",
    "chr1    0    7101193    \n",
    "chr1    0    7960000    \n",
    "```\n",
    "\n",
    "in either case we need to make sure that each row is unique.\n",
    "\n",
    "### About indels\n",
    "\n",
    "Option `--no-indel` will remove indel from analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-argument",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-lobby",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1. A RDS file containing SuSiE output object.\n",
    "2. Some visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-groove",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal Working Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-biodiversity",
   "metadata": {
    "kernel": "Bash",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sos run pipeline/trans_finemapping.ipynb SuSiE  \\\n",
    "    --name dmd_analysis  \\\n",
    "    --genoFile protocol_example.genotype.chr21_22.bed   \\\n",
    "    --phenoFile study_1_phenotypes.tsv \\\n",
    "    --covFile study_1_covariates.tsv \\\n",
    "    --region-list regions_of_interest.tsv \\\n",
    "    --container oras://ghcr.io/cumc/pecotmr_apptainer:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a02605b-a630-4538-b626-9a0432e0816a",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Workflow implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-lecture",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path(\"output\")\n",
    "# A list of file paths for genotype data, or the genotype data itself. \n",
    "parameter: genoFile = path\n",
    "# One or multiple lists of file paths for phenotype data.\n",
    "parameter: phenoFile = path\n",
    "# Covariate file path\n",
    "parameter: covFile = path\n",
    "parameter: region_list = path\n",
    "# Only focus on a subset of samples\n",
    "parameter: keep_samples = path()\n",
    "# It is required to input the name of the analysis\n",
    "parameter: name = str\n",
    "parameter: container = \"\"\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 50\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"10m\"\n",
    "# Memory expected\n",
    "parameter: mem = \"20G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 1\n",
    "\n",
    "def group_by_region(lst, partition):\n",
    "    # from itertools import accumulate\n",
    "    # partition = [len(x) for x in partition]\n",
    "    # Compute the cumulative sums once\n",
    "    # cumsum_vector = list(accumulate(partition))\n",
    "    # Use slicing based on the cumulative sums\n",
    "    # return [lst[(cumsum_vector[i-1] if i > 0 else 0):cumsum_vector[i]] for i in range(len(partition))]\n",
    "    return partition\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def adapt_file_path(file_path, reference_file):\n",
    "    \"\"\"\n",
    "    Adapt a single file path based on its existence and a reference file's path.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The file path to adapt.\n",
    "    - reference_file (str): File path to use as a reference for adaptation.\n",
    "\n",
    "    Returns:\n",
    "    - str: Adapted file path.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If no valid file path is found.\n",
    "    \"\"\"\n",
    "    reference_path = os.path.dirname(reference_file)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.isfile(file_path):\n",
    "        return file_path\n",
    "\n",
    "    # Check file name without path\n",
    "    file_name = os.path.basename(file_path)\n",
    "    if os.path.isfile(file_name):\n",
    "        return file_name\n",
    "\n",
    "    # Check file name in reference file's directory\n",
    "    file_in_ref_dir = os.path.join(reference_path, file_name)\n",
    "    if os.path.isfile(file_in_ref_dir):\n",
    "        return file_in_ref_dir\n",
    "\n",
    "    # Check original file path prefixed with reference file's directory\n",
    "    file_prefixed = os.path.join(reference_path, file_path)\n",
    "    if os.path.isfile(file_prefixed):\n",
    "        return file_prefixed\n",
    "\n",
    "    # If all checks fail, raise an error\n",
    "    raise FileNotFoundError(f\"No valid path found for file: {file_path}\")\n",
    "\n",
    "def adapt_file_path_all(df, column_name, reference_file):\n",
    "    return df[column_name].apply(lambda x: adapt_file_path(x, reference_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-limit",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[get_analysis_regions: shared = \"regional_data\"]\n",
    "# input is genoFile, phenoFile, covFile and optionally region_list. If region_list presents then we only analyze what's contained in the list.\n",
    "# regional_data should be a dictionary like:\n",
    "#{'data': [(\"genotype_1\", \"phenotype_1\", \"covariate_1\"), (\"genotype_1\", \"phenotype_1\", \"covariate_1\"), ... ],\n",
    "# 'meta_info': [(\"chr12:752578-752579\", \"region_id_1\", \"trait_1\", \"trait_2\"), (\"chr13:852580-852581\", \"region_id_2\", \"trait_1\", \"trait_2\") ... ]}\n",
    "\n",
    "def make_meta_data(region_list, region_ids, geno_meta_data, pheno_file, cov_file):\n",
    "    '''\n",
    "    Example output:\n",
    "    #chr    start       end           ID  path     cov_path             cond             geno_path\n",
    "    chr12   652578   852579  chr12_652578_852579  phenotype.tsv  covar.tsv  trait_A,trait_B   protocol_example.genotype.bed\n",
    "    '''\n",
    "    if len(region_ids) != len(region_list):\n",
    "        raise ValueError(\"Length of region_ids does not match the number of rows in region_list.\")\n",
    "    if not pheno_file.is_file():\n",
    "        raise ValueError(f\"Phenotype file ``{pheno_file}`` does not exist\")\n",
    "    pheno_file = str(pheno_file)\n",
    "    if not cov_file.is_file():\n",
    "        raise ValueError(f\"Covariate file ``{cov_file}`` does not exist\")\n",
    "    cov_file = str(cov_file)\n",
    "    # Read phenotype file\n",
    "    pheno_df = pd.read_csv(pheno_file, delim_whitespace=True, header=0)\n",
    "    # Create a comma-separated string of phenotype columns, excluding the first column\n",
    "    # FIXME HERE!\n",
    "    phenotypes = ','.join(pheno_df.columns[1:])\n",
    "\n",
    "    # Create the output DataFrame\n",
    "    output_df = pd.DataFrame({\n",
    "        '#chr': region_list.iloc[:, 0],\n",
    "        'start': region_list.iloc[:, 1],\n",
    "        'end': region_list.iloc[:, 2],\n",
    "        'ID': region_ids,\n",
    "        'path': pheno_file,\n",
    "        'cov_path': cov_file,\n",
    "        'cond': phenotypes\n",
    "    })\n",
    "\n",
    "    def find_genotype_path(chrom):\n",
    "        match = geno_meta_data[geno_meta_data['#chr'] == chrom]\n",
    "        if not match.empty:\n",
    "            return match.iloc[0]['geno_path']\n",
    "        else:\n",
    "            raise ValueError(f\"Chromosome {chrom} not found in geno_meta_data.\")\n",
    "\n",
    "    # Apply the function to find the genotype path for each row\n",
    "    output_df['geno_path'] = output_df['#chr'].apply(find_genotype_path)\n",
    "    return output_df    \n",
    "\n",
    "# Load genotype meta data\n",
    "if f\"{genoFile:x}\" == \".bed\":\n",
    "    geno_meta_data = pd.DataFrame([(\"chr\"+str(x), f\"{genoFile:a}\") for x in range(1,23)] + [(\"chrX\", f\"{genoFile:a}\")], columns=['#chr', 'geno_path'])\n",
    "else:\n",
    "    geno_meta_data = pd.read_csv(f\"{genoFile:a}\", sep = \"\\t\", header=0)\n",
    "    geno_meta_data.iloc[:, 1] = adapt_file_path_all(geno_meta_data, geno_meta_data.columns[1], f\"{genoFile:a}\")\n",
    "    geno_meta_data.columns = ['#chr', 'geno_path']\n",
    "    geno_meta_data['#chr'] = geno_meta_data['#chr'].apply(lambda x: str(x) if str(x).startswith('chr') else f'chr{x}')\n",
    "\n",
    "# Checking the DataFrame\n",
    "valid_chr_values = [f'chr{x}' for x in range(1, 23)] + ['chrX']\n",
    "if not all(value in valid_chr_values for value in geno_meta_data['#chr']):\n",
    "    raise ValueError(\"Invalid chromosome values found. Allowed values are chr1 to chr22 and chrX.\")\n",
    "\n",
    "region_ids = []\n",
    "if region_list.is_file():\n",
    "    region_list_df = pd.read_csv(region_list, delim_whitespace=True, header=None, comment = \"#\")\n",
    "    if region_list_df.shape[1] > 3:\n",
    "        # More than 3 columns, extract the last column as ID\n",
    "        region_ids = region_list_df.iloc[:, -1].unique()\n",
    "    elif region_list_df.shape[1] == 3:\n",
    "        # Exactly 3 columns, concatenate the values from the first 3 columns to form the ID\n",
    "        region_ids = region_list_df.astype(str).apply(lambda x: '_'.join(x), axis=1).unique()\n",
    "    else:\n",
    "        raise ValueError(f\"Region file ``{region_list}`` has fewer than 3 columns.\")\n",
    "else:\n",
    "    raise ValueError(f\"Region file ``{region_list}`` is not found!\")\n",
    "    \n",
    "meta_data = make_meta_data(region_list_df, region_ids, geno_meta_data, phenoFile, covFile)\n",
    "\n",
    "# Create the final dictionary\n",
    "regional_data = {\n",
    "    'data': [(row['geno_path'], *row['path'].split(','), *row['cov_path'].split(',')) for _, row in meta_data.iterrows()],\n",
    "    'meta_info': [(f\"{row['#chr']}:{row['start']}-{row['end']}\", # this is the phenotype region\n",
    "                   row['ID'], *row['cond'].split(',')) for _, row in meta_data.iterrows()]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-strip",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[SuSiE_1]\n",
    "# initial number of single effects for SuSiE\n",
    "parameter: init_L = 8\n",
    "# maximum number of single effects to use for SuSiE\n",
    "parameter: max_L = 30\n",
    "# remove a variant if it has more than imiss missing individual level data\n",
    "parameter: imiss = 1.0\n",
    "# MAF cutoff\n",
    "parameter: maf = 0.005\n",
    "# MAC cutoff, on top of MAF cutoff\n",
    "parameter: mac = 5\n",
    "# Remove indels if indel = False\n",
    "parameter: indel = True\n",
    "parameter: pip_cutoff = 0.1\n",
    "parameter: coverage = [0.95, 0.7, 0.5]\n",
    "depends: sos_variable(\"regional_data\")\n",
    "\n",
    "meta_info = regional_data['meta_info']\n",
    "input: regional_data[\"data\"], group_by = lambda x: group_by_region(x, regional_data[\"data\"]), group_with = \"meta_info\"\n",
    "output: f'{cwd:a}/{step_name[:-2]}/{name}.{_meta_info[1]}.susie.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "    # extract subset of samples\n",
    "    keep_samples = NULL\n",
    "    if (${\"TRUE\" if keep_samples.is_file() else \"FALSE\"}) {\n",
    "      keep_samples = unlist(strsplit(readLines(${keep_samples:ar}), \"\\\\s+\"))\n",
    "      message(paste(length(keep_samples), \"samples are selected to be loaded for analysis\"))\n",
    "    }\n",
    "    print(${_input[0]:anr}) # this is genotype\n",
    "    print(${_input[1]:ar}) # phenotype file\n",
    "    print(${_input[2]:ar}) # covariate\n",
    "    print(\"${_meta_info[0]}\") # region coordinate\n",
    "    print(\"${_meta_info[1]}\") # region ID\n",
    "    print (c(${\",\".join(['\"%s\"' % x for x in _meta_info[2:]])})) # phenotypes to analyze\n",
    "    stop(\"stop here for now\")\n",
    "    \n",
    "    library(pecotmr)\n",
    "    # Load genotype data\n",
    "    tryCatch({\n",
    "    fdat = load_regional_univariate_data(genotype = ${_input[0]:anr},\n",
    "                                          phenotype = ${_input[1]:ar},\n",
    "                                          covariate = ${_input[2]:ar},\n",
    "                                          region = NULL,\n",
    "                                          cis_window = \"${_meta_info[0]}\",\n",
    "                                          conditions = c(${\",\".join(['\"%s\"' % x for x in _meta_info[2:]])}),\n",
    "                                          maf_cutoff = ${maf},\n",
    "                                          mac_cutoff = ${mac},\n",
    "                                          imiss_cutoff = ${imiss},\n",
    "                                          keep_indel = ${\"TRUE\" if indel else \"FALSE\"},\n",
    "                                          keep_samples = keep_samples,\n",
    "                                          scale_residuals = FALSE)\n",
    "    }, NoSNPsError = function(e) {\n",
    "        message(\"Error: \", e$message)\n",
    "        #saveRDS(NULL, ${_output:ar})\n",
    "        saveRDS(list(\"${_meta_info[1]}\" = e$message), ${_output:ar}, compress='xz')\n",
    "        quit(save=\"no\")\n",
    "    })\n",
    "    # Univeriate analysis suite\n",
    "    fitted = list()\n",
    "    for (r in 1:length(fdat$residual_Y)) {\n",
    "      st = proc.time()\n",
    "      fitted[[r]] = susie_wrapper(fdat$residual_X[[r]], fdat$residual_Y[[r]], ${init_L}, ${max_L}, ${coverage[0]})\n",
    "      fitted[[r]] = susie_post_processor(fitted[[r]], fdat$residual_X[[r]], fdat$residual_Y[[r]], fdat$residual_X_scalar[[r]], fdat$residual_Y_scalar[[r]], \n",
    "                                       fdat$maf[[r]], secondary_coverage = c(${\",\".join([str(x) for x in coverage[1:]])}), signal_cutoff = ${pip_cutoff}, \n",
    "                                       other_quantities = list(dropped_samples = fdat$dropped_sample[[r]]))\n",
    "      fitted[[r]]$total_time_elapsed = proc.time() - st\n",
    "    }\n",
    "    names(fitted) <- names(fdat$residual_Y)\n",
    "    saveRDS(list(\"${_meta_info[1]}\" = fitted), ${_output:ar}, compress='xz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.20.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
