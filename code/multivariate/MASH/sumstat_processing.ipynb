{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "numerous-invasion",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Summary statistics formatting\n",
    "This notebook takes in more than one collections of sumstat text file,  to produce a collections of merged.rds per gene files that can served as the input of both MASH and MVSuSiE analysis.\n",
    "\n",
    "## Input\n",
    "1. a sumstat list with columns: \"#chr\", theme1, theme2, theme3, each cells not under #chr represent the path to 1 sumstat file(generated by yml generator)\n",
    "2. region_list:a table with columns: chr, start, end, gene_ID for partition\n",
    "## Output\n",
    "1. 23 merged sumstat file in txt format, 1 for each chrom\n",
    "2. merged sumstat file in rds format, 1 for each gene\n",
    "3. 2 file documenting 1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1ca5b8-d94e-4099-9ebc-73348b7f07cc",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c095f9b-afd6-4dfd-a631-4647617cf1be",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run ~/codes/xqtl-pipeline/pipeline/sumstat_processing.ipynb processing \\\n",
    "    --name protocol_example_protein \\\n",
    "    --asso_files /mnt/vast/hpc/csg/rf2872/Work/test/mash_test/test_pQTL_asso_list \\\n",
    "               /mnt/vast/hpc/csg/rf2872/Work/test/mash_test/test_pQTL_asso_list \\\n",
    "    --region_file test.region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-uncle",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "import glob\n",
    "parameter: name = str\n",
    "parameter: asso_files = paths\n",
    "parameter: region_file = path\n",
    "# Path to work directory where output locates\n",
    "parameter: cwd = path(\"./output\")\n",
    "# Containers that contains the necessary packages\n",
    "parameter: container = ''\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 8\n",
    "parameter: per_chunk =100\n",
    "# Columns: \"#chr\", sumstat(merged.vcf.gz)\n",
    "parameter: table_name = \"\"\n",
    "parameter: bhat = \"bhat\"\n",
    "parameter: sbhat = \"sbhat\"\n",
    "parameter: expected_ncondition = 0\n",
    "##  conditions can be excluded if needs arise. If nothing to exclude keep the default 0\n",
    "parameter: exclude_condition = []\n",
    "parameter: datadir = \"\"\n",
    "parameter: seed = 999\n",
    "parameter: n_random = 4\n",
    "parameter: n_null = 4\n",
    "parameter: z_only = False\n",
    "parameter: na_remove = \"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6538a2f4-5138-4eb8-9d4f-eb0b87a02704",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Get the random and null effects per analysis unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643320b6-1800-4ae8-a68e-8956c0dc9497",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[processing_1]\n",
    "import re\n",
    "import pandas as pd\n",
    "def find_matching_files_for_region(chr_id):\n",
    "    chr_number = chr_id[3:]  # subset 1 from chr1\n",
    "    pattern_str = r\"\\.{chr_number}\\.\"\n",
    "    pattern = re.compile(pattern_str.format(chr_number=chr_number))\n",
    "    paths = []\n",
    "    for asso_file in asso_files:\n",
    "        with open(asso_file, 'r') as af:\n",
    "            for aline in af:\n",
    "                if pattern.search(aline):\n",
    "                    paths.append(aline.strip())\n",
    "    return \",\".join(paths)\n",
    "\n",
    "updated_regions = []\n",
    "with open(region_file, 'r') as regions:\n",
    "    header = regions.readline().strip()\n",
    "    updated_regions.append(header + \"\\tpath\\tregion\")\n",
    "    for line in regions:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        chr_id, start, end, gene_id = parts\n",
    "        paths = find_matching_files_for_region(chr_id)\n",
    "        updated_regions.append(f\"{chr_id}\\t{start}\\t{end}\\t{gene_id}\\t{paths}\\t{chr_id}:{start}-{end}\")\n",
    "\n",
    "meta_df = pd.DataFrame([line.split(\"\\t\") for line in updated_regions[1:]], columns=updated_regions[0].split(\"\\t\"))\n",
    "meta = meta_df[['gene_id', 'path', 'region']].to_dict(orient='records')\n",
    "\n",
    "input: for_each='meta'\n",
    "output: f'{cwd:a}/{name}_cache/{name}.{_meta[\"gene_id\"]}.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'  \n",
    "R: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    # Extract and preprocess data from phenotype_path\n",
    "    extract_data <- function(path, region) {\n",
    "        tabix_region(path, region) %>%\n",
    "            mutate(variant = paste(`#CHROM`, POS, REF, ALT, sep = \":\")) %>%\n",
    "            select(-c(3, 6:9)) %>%\n",
    "            distinct(variant, .keep_all = TRUE) %>%\n",
    "            as.matrix\n",
    "    }\n",
    "    # Extract bhat and sbhat\n",
    "    extract_component <- function(df, component_index) {\n",
    "        df %>%\n",
    "            select(6:ncol(df)) %>%\n",
    "            mutate(across(everything(), ~as.numeric(strsplit(as.character(.), \":\")[[1]][component_index]))) %>%\n",
    "            as.matrix\n",
    "    }\n",
    "\n",
    "    load_combined_matrix_data <- function(phenotype_path, region) {\n",
    "        library(dplyr)   \n",
    "        Y <- lapply(phenotype_path, extract_data, region)\n",
    "\n",
    "        # Combine matrices\n",
    "        combined_matrix <- Reduce(function(x, y) merge(x, y, by = c(\"variant\", \"#CHROM\", \"POS\", \"REF\", \"ALT\")), Y) %>%\n",
    "            distinct(variant, .keep_all = TRUE)\n",
    "\n",
    "        dat <- list(\n",
    "            bhat = extract_component(combined_matrix, 1),\n",
    "            sbhat = extract_component(combined_matrix, 2)\n",
    "        )\n",
    "\n",
    "        rownames(dat$bhat) <- rownames(dat$sbhat) <- combined_matrix$variant\n",
    "\n",
    "        return(dat)\n",
    "    }\n",
    "  \n",
    "    tabix_region <- function(file, region){\n",
    "        data.table::fread(cmd = paste0(\"tabix -h \", file, \" \", region))%>%as_tibble() \n",
    "    }\n",
    "  \n",
    "    region <- \"${_meta['region']}\"\n",
    "    phenotype_path <- unlist(strsplit(\"${_meta['path']}\", \",\"))\n",
    "\n",
    "    dat <- tryCatch(\n",
    "      {\n",
    "        # Try to run the function\n",
    "         load_combined_matrix_data(phenotype_path = phenotype_path, region = region)\n",
    "      },\n",
    "      error = function(e) {\n",
    "        message(\"gsub chr in region id...\")\n",
    "        # If an error occurs, modify the region and try again\n",
    "         load_combined_matrix_data(phenotype_path = phenotype_path, region =  gsub(\"chr\", \"\", region))\n",
    "      }\n",
    "    )\n",
    "      saveRDS(dat, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c4c4d-f41c-436d-b6b2-10bb20138049",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# extract data for MASH from summary stats\n",
    "[processing_2]\n",
    "input:  group_by = per_chunk\n",
    "output: f\"{cwd}/{name}_cache/{name}_batch{_index+1}.rds\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\",stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container\n",
    "    library(dplyr)\n",
    "    library(stringr)\n",
    "    set.seed(${seed})\n",
    "    #library(huiiy)\n",
    "    matxMax <- function(mtx) {\n",
    "      return(arrayInd(which.max(mtx), dim(mtx)))\n",
    "    }\n",
    "    remove_rownames = function(x) {\n",
    "        for (name in names(x)) rownames(x[[name]]) = NULL\n",
    "        return(x)\n",
    "    }\n",
    "    handle_nan_etc = function(x) {\n",
    "      x$bhat[which(is.nan(x$bhat))] = 0\n",
    "      x$sbhat[which(is.nan(x$sbhat) | is.infinite(x$sbhat))] = 1E3\n",
    "      return(x)\n",
    "    }\n",
    "    extract_one_data = function(dat, n_random, n_null, infile, na_remove = TRUE) {\n",
    "        if (is.null(dat)) return(NULL)\n",
    "        if(na_remove == TRUE){\n",
    "          na.info = list()\n",
    "          na.info$n_bhat_ori = nrow(dat$${bhat})\n",
    "          dat$bhat = na.omit(dat$${bhat})\n",
    "          na.info$n_bhat = nrow(dat$${bhat})\n",
    "          na.info$n_sbhat_ori = nrow(dat$${sbhat})\n",
    "          dat$sbhat = na.omit(dat$${sbhat})\n",
    "          na.info$n_sbhat_ori = nrow(dat$${sbhat})\n",
    "          msg = paste(c(\"Out of \",na.info$n_bhat_ori,\" SNP, \",na.info$n_bhat,\" was retained for analysis\"), collapse = \"\")\n",
    "          message(msg)\n",
    "          if (na.info$n_bhat == 0){\n",
    "            stop(\"None of the SNP was retained for analysis, skipping genes\") }\n",
    "        }\n",
    "        z = abs(dat$${bhat}/dat$${sbhat})\n",
    "        # random samples can include the real signals \n",
    "        sample_idx = 1:nrow(z)\n",
    "        \n",
    "        random_idx = sample(sample_idx, min(n_random, length(sample_idx)), replace = F)\n",
    "        random = list(bhat = dat$${bhat}[random_idx,,drop=F], sbhat = dat$${sbhat}[random_idx,,drop=F])\n",
    "        # null samples defined as |z| < 2\n",
    "        null.id = which(apply(abs(z), 1, max) < 2)\n",
    "        if (length(null.id) == 0) {\n",
    "          warning(paste(\"Null data is empty for input file\", infile))\n",
    "          null = list()\n",
    "        } else {\n",
    "          null_idx = sample(null.id, min(n_null, length(null.id)), replace = F)\n",
    "          null = list(bhat = dat$${bhat}[null_idx,,drop=F], sbhat = dat$${sbhat}[null_idx,,drop=F])\n",
    "        }\n",
    "        dat = (list(random = remove_rownames(random), null = remove_rownames(null)))\n",
    "        dat$random = handle_nan_etc(dat$random)\n",
    "        dat$null = handle_nan_etc(dat$null)\n",
    "        return(dat)\n",
    "    }\n",
    "    reformat_data = function(dat, z_only = TRUE) {\n",
    "        # make output consistent in format with \n",
    "        # https://github.com/stephenslab/gtexresults/blob/master/workflows/mashr_flashr_workflow.ipynb      \n",
    "        res = list(random.z = dat$random$bhat/dat$random$sbhat, \n",
    "                  null.z = dat$null$bhat/dat$null$sbhat)\n",
    "        if (!z_only) {\n",
    "          res = c(res, list(random.b = dat$random$bhat,\n",
    "           null.b = dat$null$bhat,\n",
    "           null.s = dat$null$sbhat,\n",
    "           random.s = dat$random$sbhat))\n",
    "      }\n",
    "      return(res)\n",
    "    }\n",
    "    merge_data = function(res, one_data) {\n",
    "      if (length(res) == 0) {\n",
    "          return(one_data)\n",
    "      } else if (is.null(is.null(res$random.b)|is.null(res$null.b))) {\n",
    "          return(one_data)\n",
    "      } else if (is.null(one_data)) {\n",
    "          return(res)\n",
    "      } else {\n",
    "          for (d in names(one_data)) {\n",
    "            if (is.null(one_data[[d]])) {\n",
    "              next\n",
    "            } else {\n",
    "                res[[d]] = as.matrix(rbind(res[[d]],as.data.frame(one_data[[d]])))\n",
    "            }\n",
    "          }\n",
    "          return(res)\n",
    "      }\n",
    "    }\n",
    "    res = list()\n",
    "    signals.df<-NULL\n",
    " \n",
    "    \n",
    "    for (f in c(${_input:r,})) {\n",
    "      # If cannot read the input for some reason then we just skip it, assuming we have other enough data-sets to use.\n",
    "      dat = tryCatch(readRDS(f), error = function(e) return(NULL))${(\"$\"+table_name) if table_name != \"\" else \"\"}\n",
    "      if (is.null(dat)) {\n",
    "          message(paste(\"Skip loading file\", f, \"due to load failure.\"))\n",
    "          next\n",
    "      }\n",
    "      if (${expected_ncondition} > 0 && (ncol(dat$${bhat}) != ${expected_ncondition} || ncol(dat$${sbhat}) != ${expected_ncondition})) {\n",
    "          message(paste(\"Skip loading file\", f, \"because it has\", ncol(dat$${bhat}), \"columns different from required\", ${expected_ncondition}))\n",
    "          next\n",
    "      }\n",
    "      if(length(c(${\",\".join([repr(x) for x in exclude_condition])})) > 0 ){\n",
    "      message(paste(\"Excluding condition ${exclude_condition} from the analysis\"))\n",
    "      dat$bhat = dat$bhat[,-c(${\",\".join(exclude_condition)})]\n",
    "      dat$sbhat = dat$sbhat[,-c(${\",\".join(exclude_condition)})]\n",
    "      dat$Z = dat$Z[,-c(${\",\".join(exclude_condition)})]\n",
    "      }\n",
    "\n",
    "      dat<-tryCatch(extract_one_data(dat, ${n_random}, ${n_null}, f, ${na_remove}), error = function(e) return(NULL))\n",
    "      res = tryCatch(merge_data(res, reformat_data(dat , ${\"TRUE\" if z_only else \"FALSE\"})), error = function(e) message(\"Skipping gene due to lack of SNPs\"))\n",
    "      \n",
    "    saveRDS(res, ${_output:r})}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce79fe99-6dfe-42c2-bf47-f22992403990",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[processing_3]\n",
    "input: group_by = \"all\"\n",
    "output: f\"{cwd}/{name}.rds\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '16G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", container = container,stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', volumes = [f'{cwd:ad}:{cwd:ad}']\n",
    "    merge_data = function(res, one_data) {\n",
    "      if (length(res) == 0) {\n",
    "          return(one_data)\n",
    "      } else {\n",
    "          for (d in names(one_data)) {\n",
    "            res[[d]] = rbind(res[[d]], one_data[[d]])\n",
    "          }\n",
    "          return(res)\n",
    "      }\n",
    "    }\n",
    "    dat = list()\n",
    "    for (f in c(${_input:r,})) {\n",
    "      dat = merge_data(dat, readRDS(f))\n",
    "    }\n",
    "    saveRDS(dat, ${_output:r})\n",
    " \n",
    "bash: expand = \"${ }\", container = container,stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', volumes = [f'{cwd:ad}:{cwd:ad}']\n",
    "    rm -rf ${cwd}/${name}_cache/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "R",
     "ir",
     "R",
     "#DCDCDA",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.24.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
