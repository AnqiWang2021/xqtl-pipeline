{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path(\"output\")\n",
    "# A list of file paths for genotype data, or the genotype data itself. \n",
    "parameter: genoFile = path\n",
    "# One or multiple lists of file paths for phenotype data.\n",
    "parameter: phenoFile = paths\n",
    "# Covariate file path\n",
    "parameter: covFile = paths\n",
    "# Optional: if a region list is provide the analysis will be focused on provided region. \n",
    "# The LAST column of this list will contain the ID of regions to focus on\n",
    "# Otherwise, all regions with both genotype and phenotype files will be analyzed\n",
    "parameter: region_list = path()\n",
    "# Optional: if a region name is provided \n",
    "# the analysis would be focused on the union of provides region list and region names\n",
    "parameter: region_name = []\n",
    "# Only focus on a subset of samples\n",
    "parameter: keep_samples = path()\n",
    "# An optional list documenting the custom cis window for each region to analyze, with four column, chr, start, end, region ID (eg gene ID).\n",
    "# If this list is not provided, the default `window` parameter (see below) will be used.\n",
    "parameter: customized_cis_windows = path()\n",
    "# Specify the cis window for the up and downstream radius to analyze around the region of interest in units of bp\n",
    "# When this is zero, we will rely on customized_cis_windows\n",
    "parameter: window = 0\n",
    "# It is required to input the name of the analysis\n",
    "parameter: name = str\n",
    "# save data object or not\n",
    "parameter: save_data = False\n",
    "parameter: container = \"\"\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 200\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"1h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"20G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 1\n",
    "# Name of phenotypes\n",
    "parameter: phenotype_names = [f'{x:bn}' for x in phenoFile]\n",
    "parameter: seed = 999\n",
    "\n",
    "def group_by_region(lst, partition):\n",
    "    # from itertools import accumulate\n",
    "    # partition = [len(x) for x in partition]\n",
    "    # Compute the cumulative sums once\n",
    "    # cumsum_vector = list(accumulate(partition))\n",
    "    # Use slicing based on the cumulative sums\n",
    "    # return [lst[(cumsum_vector[i-1] if i > 0 else 0):cumsum_vector[i]] for i in range(len(partition))]\n",
    "    return partition\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def adapt_file_path(file_path, reference_file):\n",
    "    \"\"\"\n",
    "    Adapt a single file path based on its existence and a reference file's path.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The file path to adapt.\n",
    "    - reference_file (str): File path to use as a reference for adaptation.\n",
    "\n",
    "    Returns:\n",
    "    - str: Adapted file path.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If no valid file path is found.\n",
    "    \"\"\"\n",
    "    reference_path = os.path.dirname(reference_file)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.isfile(file_path):\n",
    "        return file_path\n",
    "\n",
    "    # Check file name without path\n",
    "    file_name = os.path.basename(file_path)\n",
    "    if os.path.isfile(file_name):\n",
    "        return file_name\n",
    "\n",
    "    # Check file name in reference file's directory\n",
    "    file_in_ref_dir = os.path.join(reference_path, file_name)\n",
    "    if os.path.isfile(file_in_ref_dir):\n",
    "        return file_in_ref_dir\n",
    "\n",
    "    # Check original file path prefixed with reference file's directory\n",
    "    file_prefixed = os.path.join(reference_path, file_path)\n",
    "    if os.path.isfile(file_prefixed):\n",
    "        return file_prefixed\n",
    "\n",
    "    # If all checks fail, raise an error\n",
    "    raise FileNotFoundError(f\"No valid path found for file: {file_path}\")\n",
    "\n",
    "def adapt_file_path_all(df, column_name, reference_file):\n",
    "    return df[column_name].apply(lambda x: adapt_file_path(x, reference_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[get_analysis_regions: shared = \"regional_data\"]\n",
    "# input is genoFile, phenoFile, covFile and optionally region_list. If region_list presents then we only analyze what's contained in the list.\n",
    "# regional_data should be a dictionary like:\n",
    "#{'data': [(\"genotype_1.bed\", \"phenotype_1.bed.gz\", \"covariate_1.gz\"), (\"genotype_2.bed\", \"phenotype_1.bed.gz\", \"phenotype_2.bed.gz\", \"covariate_1.gz\", \"covariate_2.gz\") ... ],\n",
    "# 'meta_info': [(\"chr12:752578-752579\",\"chr12:752577-752580\", \"gene_1\", \"trait_1\"), (\"chr13:852580-852581\",\"chr13:852579-852580\", \"gene_2\", \"trait_1\", \"trait_2\") ... ]}\n",
    "\n",
    "def process_pheno_files(pheno_files, cov_files, phenotype_names):\n",
    "    '''\n",
    "    Example output:\n",
    "    #chr    start      end    start_cis       end_cis           ID  path     cov_path             cond             coordinate     geno_path\n",
    "    0  chr12   752578   752579  652578   852579  ENSG00000060237_Q9H4A3  protocol_example.protein_1.bed.gz,protocol_example.protein_2.bed.gz  covar_1.gz,covar_2.gz  trait_A,trait_B    chr12:752578-752579  protocol_example.genotype.chr21_22.bed\n",
    "    '''\n",
    "    # Initialize an empty DataFrame for accumulation\n",
    "    accumulated_pheno_df = pd.DataFrame()\n",
    "\n",
    "    merge_keys = ['#chr', 'start', 'end', 'ID']\n",
    "\n",
    "    for pheno_path, cov_path, phenotype_name in zip(pheno_files, cov_files, phenotype_names):\n",
    "        if not os.path.isfile(cov_path):\n",
    "            raise FileNotFoundError(f\"No valid path found for file: {cov_path}\")\n",
    "\n",
    "        # Read and process each phenotype file\n",
    "        pheno_df = pd.read_csv(pheno_path, sep=\"\\t\", header=0)\n",
    "        \n",
    "        # Adapt pheno file paths and add additional information\n",
    "        pheno_df.iloc[:, 4] = adapt_file_path_all(pheno_df, pheno_df.columns[4], f\"{pheno_path:a}\")\n",
    "        pheno_df = pheno_df.assign(\n",
    "            cov_path=str(cov_path), \n",
    "            cond=phenotype_name)\n",
    "\n",
    "        # Merge with the accumulated DataFrame\n",
    "        if accumulated_pheno_df.empty:\n",
    "            accumulated_pheno_df = pheno_df\n",
    "        else:\n",
    "            # Merge on specified keys with default suffixes\n",
    "            merged_df = pd.merge(accumulated_pheno_df, pheno_df, on=merge_keys, how='outer', suffixes=('_x', '_y'))\n",
    "\n",
    "            # Determine non-key columns\n",
    "            non_key_columns = [col for col in pheno_df.columns if col not in merge_keys]\n",
    "\n",
    "            # Concatenate non-key columns for matching keys\n",
    "            for col in non_key_columns:\n",
    "                col_x = f'{col}_x'\n",
    "                col_y = f'{col}_y'\n",
    "\n",
    "                # Handling concatenation for matching keys\n",
    "                merged_df[col] = merged_df.apply(\n",
    "                    lambda row: row[col_x] if pd.isna(row[col_y]) else \n",
    "                                (row[col_y] if pd.isna(row[col_x]) else f'{row[col_x]},{row[col_y]}'), axis=1)\n",
    "\n",
    "                # Drop the temporary columns\n",
    "                merged_df.drop([col_x, col_y], axis=1, inplace=True)\n",
    "\n",
    "            accumulated_pheno_df = merged_df\n",
    "    return accumulated_pheno_df\n",
    "\n",
    "# Load phenotype meta data\n",
    "if len(phenoFile) != len(covFile):\n",
    "    raise ValueError(\"Number of input phenotypes files must match that of covariates files\")\n",
    "if len(phenoFile) != len(phenotype_names):\n",
    "    raise ValueError(\"Number of input phenotypes files must match the number of phenotype names\")\n",
    "meta_data = process_pheno_files(phenoFile, covFile, phenotype_names)\n",
    "\n",
    "# Load genotype meta data\n",
    "if f\"{genoFile:x}\" == \".bed\":\n",
    "    geno_meta_data = pd.DataFrame([(\"chr\"+str(x), f\"{genoFile:a}\") for x in range(1,23)] + [(\"chrX\", f\"{genoFile:a}\")], columns=['#chr', 'geno_path'])\n",
    "else:\n",
    "    geno_meta_data = pd.read_csv(f\"{genoFile:a}\", sep = \"\\t\", header=0)\n",
    "    geno_meta_data.iloc[:, 1] = adapt_file_path_all(geno_meta_data, geno_meta_data.columns[1], f\"{genoFile:a}\")\n",
    "    geno_meta_data.columns = ['#chr', 'geno_path']\n",
    "    geno_meta_data['#chr'] = geno_meta_data['#chr'].apply(lambda x: str(x) if str(x).startswith('chr') else f'chr{x}')\n",
    "\n",
    "# Checking the DataFrame\n",
    "valid_chr_values = [f'chr{x}' for x in range(1, 23)] + ['chrX']\n",
    "if not all(value in valid_chr_values for value in geno_meta_data['#chr']):\n",
    "    raise ValueError(\"Invalid chromosome values found. Allowed values are chr1 to chr22 and chrX.\")\n",
    "\n",
    "meta_data = meta_data.merge(geno_meta_data, on='#chr', how='inner')\n",
    "\n",
    "if len(meta_data.index) == 0:\n",
    "    raise ValueError(\"No region overlap between genotype and any of the phenotypes\")\n",
    "\n",
    "region_ids = []\n",
    "# If region_list is provided, read the file and extract IDs\n",
    "if region_list.is_file():\n",
    "    region_list_df = pd.read_csv(region_list, delim_whitespace=True, header=None, comment = \"#\")\n",
    "    region_ids = region_list_df.iloc[:, -1].unique()  # Extracting the last column for IDs\n",
    "\n",
    "# If region_name is provided, include those IDs as well\n",
    "# --region-name A B C will result in a list of [\"A\", \"B\", \"C\"] here\n",
    "if len(region_name) > 0:\n",
    "    region_ids = list(set(region_ids).union(set(region_name)))\n",
    "\n",
    "# If either region_list or region_name is provided, filter the meta_data\n",
    "if len(region_ids) > 0:\n",
    "    meta_data = meta_data[meta_data['ID'].isin(region_ids)]\n",
    "\n",
    "# Adjust cis-window\n",
    "if os.path.isfile(customized_cis_windows):\n",
    "    print(f\"Loading customized cis-window data from {customized_cis_windows}\")\n",
    "    cis_list = pd.read_csv(customized_cis_windows, comment=\"#\", header=None, names=[\"#chr\",\"start\",\"end\",\"ID\"], sep=\"\\t\")\n",
    "    meta_data = pd.merge(meta_data, cis_list, on=['#chr', 'ID'], how='left', suffixes=('', '_cis')) \n",
    "    mismatches = meta_data[meta_data['start_cis'].isna()]\n",
    "    if not mismatches.empty:\n",
    "        print(\"First 5 mismatches:\")\n",
    "        print(mismatches[['ID']].head())\n",
    "        raise ValueError(f\"{len(mismatches)} regions to analyze cannot be found in ``{customized_cis_windows}``. Please check your ``{customized_cis_windows}`` database to make sure it contains all cis-window definitions. \")\n",
    "else:\n",
    "    if window <=0 :\n",
    "        raise ValueError(\"Please either input valid path to cis-window file via ``--customized-cis-windows``, or set ``--window`` to a positive integer\")\n",
    "    meta_data['start_cis'] = meta_data['start'].apply(lambda x: max(x - window, 0))\n",
    "    meta_data['end_cis'] = meta_data['end'] + window\n",
    "# Create the final dictionary\n",
    "regional_data = {\n",
    "    'data': [(row['geno_path'], *row['path'].split(','), *row['cov_path'].split(',')) for _, row in meta_data.iterrows()],\n",
    "    'meta_info': [(f\"{row['#chr']}:{row['start']}-{row['end']}\", # this is the phenotype region\n",
    "                   f\"{row['#chr']}:{row['start_cis']}-{row['end_cis']}\", # this is the cis-window region\n",
    "                   row['ID'], *row['cond'].split(',')) for _, row in meta_data.iterrows()]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[susie_twas_1]\n",
    "# initial number of single effects for SuSiE\n",
    "parameter: init_L = 8\n",
    "# maximum number of single effects to use for SuSiE\n",
    "parameter: max_L = 30\n",
    "# remove a variant if it has more than imiss missing individual level data\n",
    "parameter: imiss = 1.0\n",
    "# MAF cutoff\n",
    "parameter: maf = 0.0025\n",
    "# MAC cutoff, on top of MAF cutoff\n",
    "parameter: mac = 5\n",
    "# Remove indels if indel = False\n",
    "parameter: indel = True\n",
    "parameter: pip_cutoff = 0.025\n",
    "parameter: coverage = [0.95, 0.7, 0.5]\n",
    "# Compute TWAS weights as well\n",
    "parameter: twas_weights = True\n",
    "# Perform K folds valiation CV for TWAS\n",
    "# Set it to zero if this is to be skipped\n",
    "parameter: twas_cv_folds = 5\n",
    "parameter: twas_cv_threads = twas_cv_folds\n",
    "# maximum number of variants to consider for CV\n",
    "# We will randomly pick a subset of it for CV purpose\n",
    "parameter: max_cv_variants = 5000\n",
    "# Further limit CV to only using common variants\n",
    "parameter: min_cv_maf = 0.05\n",
    "parameter: ld_reference_meta_file = path()\n",
    "depends: sos_variable(\"regional_data\")\n",
    "# Check if both 'data' and 'meta_info' are empty lists\n",
    "stop_if(len(regional_data['data']) == 0, f'Either genotype or phenotype data are not available for region {\", \".join(region_name)}.')\n",
    "\n",
    "meta_info = regional_data['meta_info']\n",
    "input: regional_data[\"data\"], group_by = lambda x: group_by_region(x, regional_data[\"data\"]), group_with = \"meta_info\"\n",
    "output: f'{cwd:a}/{step_name[:-2]}/{name}.{_meta_info[2]}.susie{\"_weights_db\" if twas_weights else \"\"}.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "    options(warn=1)\n",
    "    library(pecotmr)\n",
    "    # extract subset of samples\n",
    "    keep_samples = NULL\n",
    "    if (${\"TRUE\" if keep_samples.is_file() else \"FALSE\"}) {\n",
    "      keep_samples = unlist(strsplit(readLines(${keep_samples:ar}), \"\\\\s+\"))\n",
    "      message(paste(length(keep_samples), \"samples are selected to be loaded for analysis\"))\n",
    "    }\n",
    "    # Load regional association data\n",
    "    tryCatch({\n",
    "    fdat = load_regional_univariate_data(genotype = ${_input[0]:anr},\n",
    "                                          phenotype = c(${\",\".join(['\"%s\"' % x.absolute() for x in _input[1:len(_input)//2+1]])}),\n",
    "                                          covariate = c(${\",\".join(['\"%s\"' % x.absolute() for x in _input[len(_input)//2+1:]])}),\n",
    "                                          region = \"${_meta_info[0]}\",\n",
    "                                          cis_window = \"${_meta_info[1]}\",\n",
    "                                          conditions = c(${\",\".join(['\"%s\"' % x for x in _meta_info[3:]])}),\n",
    "                                          maf_cutoff = ${maf},\n",
    "                                          mac_cutoff = ${mac},\n",
    "                                          imiss_cutoff = ${imiss},\n",
    "                                          keep_indel = ${\"TRUE\" if indel else \"FALSE\"},\n",
    "                                          keep_samples = keep_samples,\n",
    "                                          extract_region_name = \"${_meta_info[2]}\",\n",
    "                                          phenotype_header = 4,\n",
    "                                          region_name_col = 4,\n",
    "                                          scale_residuals = FALSE)\n",
    "    }, NoSNPsError = function(e) {\n",
    "        message(\"Error: \", paste(e$message, \"${_meta_info[2] + '@' + _meta_info[1]}\"))\n",
    "        #saveRDS(NULL, ${_output:ar})\n",
    "        saveRDS(list(${_meta_info[2]} = e$message), ${_output:ar}, compress='xz')\n",
    "        quit(save=\"no\")\n",
    "    })\n",
    "  \n",
    "    if (${\"TRUE\" if save_data else \"FALSE\"}) {\n",
    "      # save data object for debug purpose\n",
    "      saveRDS(list(${_meta_info[2]} = fdat), \"${_output:ann}.dataset.rds\", compress='xz')\n",
    "    }\n",
    "    # Univeriate analysis suite\n",
    "    fitted = setNames(replicate(length(fdat$residual_Y), list(), simplify = FALSE), names(fdat$residual_Y))\n",
    "    existing_file = gsub(\"_DeJager\", \"\", ${_output:ar})\n",
    "    existing_data = readRDS(existing_file)[[1]]\n",
    "  \n",
    "    for (r in 1:length(fitted)) {\n",
    "      st = proc.time()\n",
    "      init_L=${init_L}\n",
    "      max_L=${max_L}\n",
    "      # codes to rerun\n",
    "      fitted[[r]] = susie_wrapper(fdat$residual_X[[r]], fdat$residual_Y[[r]], init_L=init_L, max_L=max_L, refine=TRUE, coverage = ${coverage[0]})\n",
    "      fitted[[r]] = susie_post_processor(fitted[[r]], fdat$residual_X[[r]], fdat$residual_Y[[r]], fdat$residual_X_scalar[[r]], fdat$residual_Y_scalar[[r]], \n",
    "                                       fdat$maf[[r]], secondary_coverage = c(${\",\".join([str(x) for x in coverage[1:]])}), signal_cutoff = ${pip_cutoff},\n",
    "                                       other_quantities = list(dropped_samples = list(X=fdat$dropped_sample$dropped_samples_X[[r]], y=fdat$dropped_sample$dropped_samples_Y[[r]], \n",
    "                                                                                      covar=fdat$dropped_sample$dropped_samples_covar[[r]])))\n",
    "      if (!is.null(fitted[[r]]$susie_result_trimmed)) {\n",
    "          L = length(fitted[[r]]$susie_result_trimmed$V)\n",
    "          init_L = max(1, L-2)\n",
    "          max_L = L+3\n",
    "      } else {\n",
    "          init_L = 2\n",
    "          max_L = 2\n",
    "      }\n",
    "  \n",
    "      variants_kept = filter_variants_by_ld_reference(colnames(fdat$residual_X[[r]]), ${ld_reference_meta_file:r})\n",
    "      X = fdat$residual_X[[r]][,variants_kept$data,drop=F]\n",
    "      fitted[[r]]$preset_variants_result = susie_wrapper(X, fdat$residual_Y[[r]], init_L=init_L, max_L=max_L, refine=TRUE, coverage = ${coverage[0]})\n",
    "      fitted[[r]]$preset_variants_result = susie_post_processor(fitted[[r]]$preset_variants_result, X, fdat$residual_Y[[r]], \n",
    "                               if (fdat$residual_X_scalar[[r]]==1) 1 else fdat$residual_X_scalar[[r]][variants_kept$idx], \n",
    "                               fdat$residual_Y_scalar[[r]], fdat$maf[[r]][variants_kept$idx], \n",
    "                               secondary_coverage = c(${\",\".join([str(x) for x in coverage[1:]])}), signal_cutoff = ${pip_cutoff})\n",
    "      fitted[[r]]$preset_variants_result$analysis_script=NULL\n",
    "      fitted[[r]]$preset_variants_result$sumstats=NULL\n",
    "      # codes to load\n",
    "      existing_key = gsub(\"_DeJager_eQTL\", \"\", names(fitted)[[r]])\n",
    "      tmp = existing_data[[existing_key]]\n",
    "      fitted[[r]]$twas_weights = tmp$twas_weights\n",
    "      fitted[[r]]$twas_predictions = tmp$twas_predictions\n",
    "      fitted[[r]]$twas_cv_result = tmp$twas_cv_result\n",
    "      fitted[[r]]$total_time_elapsed = tmp$total_time_elapsed\n",
    "      fitted[[r]]$region_info = list(region_coord=parse_region(\"${_meta_info[0]}\"), grange=parse_region(\"${_meta_info[1]}\"), region_name=\"${_meta_info[2]}\")\n",
    "      # original data no longer relevant, set to NA to release memory\n",
    "      fdat$residual_X[[r]] <- NA\n",
    "      fdat$residual_Y[[r]] <- NA\n",
    "    }\n",
    "    saveRDS(list(${_meta_info[2]} = fitted), ${_output:ar}, compress='xz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "calysto_bash",
     "Bash",
     "#E6EEFF",
     "shell"
    ],
    [
     "Markdown",
     "markdown",
     "markdown",
     "",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.24.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
