{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c95d8992-e05e-482a-8b33-5c6725650d43",
   "metadata": {},
   "source": [
    "# Fine-mapping with SuSiE RSS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f125aa-1492-4b3e-aa71-4097225a2465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29b654fe-c703-479f-a1af-4c68f0a862e3",
   "metadata": {},
   "source": [
    "This notebook take a list of LD file and a list of sumstat file and do salmon QC and susie RSS for each overlap LD block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c31441-f4f8-41cd-9b5e-cdc65d22e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "[global]\n",
    "\n",
    "parameter: container = \"\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 20\n",
    "\n",
    "# getting the overlapped input\n",
    "parameter: LD_list = path\n",
    "parameter: sumstat_list = path\n",
    "import pandas as pd\n",
    "LD_list = pd.read_csv(LD_list,\"\\t\")\n",
    "sumstat_list = pd.read_csv(sumstat_list,\"\\t\")\n",
    "LD_list[\"#chr\"] = [x[0].replace(\"chr\", \"\") for x in  LD_list[\"#id\"].str.split(\"_\") ]\n",
    "sumstat_list[\"#chr\"] = [str(x).replace(\"chr\", \"\") for x in  sumstat_list[\"#chr\"] ]\n",
    "input_inv = LD_list.merge(sumstat_list)\n",
    "input_list = input_inv.iloc[:,[1,3]].values.tolist()\n",
    "parameter: lead_idx_choice = \"pvalue\"\n",
    "parameter: abf_prior_variance = 0.4\n",
    "parameter: nlog10p_dentist_s_threshold = 4\n",
    "parameter: r2_threshold = 0.6\n",
    "parameter: n = 0\n",
    "\n",
    "\n",
    "[SuSiE_RSS_1]\n",
    "input: input_list, group_by = 2\n",
    "output: f'{cwd:a}/{_input[1]:bn}.{_input[0].split(\".\")[-3]}.unisusie_rss.fit.rds',f'{cwd:a}/{_input[1]:bn}.{_input[0].split(\".\")[-3]}.unisusie_rss.ss_qced.tsv'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "python: expand = '${ }', stdout = f\"{_output:nn}.stdout\", stderr = f\"{_output:nn}.stderr\", container = container\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import rpy2\n",
    "    from backports import zoneinfo\n",
    "    import rpy2.robjects.numpy2ri as numpy2ri\n",
    "    from rpy2.robjects import pandas2ri\n",
    "    from rpy2.robjects.packages import SignatureTranslatedAnonymousPackage\n",
    "    import rpy2.robjects as ro\n",
    "    numpy2ri.activate()\n",
    "    pandas2ri.activate()\n",
    "    def load_npz_ld(path):\n",
    "        np_ld_loaded = np.load(path,allow_pickle=True)\n",
    "        # sort by start position\n",
    "        snp_id = [x.replace(\":\",\"_\") for x in np_ld_loaded.get(\"arr_1\")]\n",
    "        np_ld_loaded = np_ld_loaded.get(\"arr_0\")\n",
    "        new = np_ld_loaded + np_ld_loaded.T\n",
    "        np.fill_diagonal(new, np.diag(new)/2)\n",
    "        return new,snp_id\n",
    "\n",
    "    def get_bcor_meta(bcor_obj):\n",
    "        df_ld_snps = bcor_obj.getMeta()\n",
    "        df_ld_snps.rename(columns={'rsid':'SNP', 'position':'BP', 'chromosome':'CHR', 'allele1':'A1', 'allele2':'A2'}, inplace=True, errors='raise')\n",
    "        ###df_ld_snps['CHR'] = df_ld_snps['CHR'].astype(np.int64)\n",
    "        df_ld_snps['BP'] = df_ld_snps['BP'].astype(np.int64)\n",
    "        return df_ld_snps\n",
    "\n",
    "    def load_ld_bcor(ld_prefix):\n",
    "        bcor_file = ld_prefix+'.bcor'\n",
    "        import os\n",
    "        import time\n",
    "        from ldstore.bcor import bcor\n",
    "        if not os.path.exists(bcor_file):\n",
    "            raise IOError('%s not found'%(bcor_file))\n",
    "        t0 = time.time()\n",
    "        bcor_obj = bcor(bcor_file)\n",
    "        df_ld_snps = get_bcor_meta(bcor_obj)\n",
    "        ld_arr = bcor_obj.readCorr([])\n",
    "        assert np.all(~np.isnan(ld_arr))\n",
    "        return ld_arr, df_ld_snps\n",
    "\n",
    "    def abf(beta, se, W=0.04):\n",
    "        from scipy import special \n",
    "        z = beta / se\n",
    "        V = se ** 2\n",
    "        r = W / (W + V)\n",
    "        lbf = 0.5 * (np.log(1 - r) + (r * z ** 2))\n",
    "        denom = special.logsumexp(lbf)\n",
    "        prob = np.exp(lbf - denom)\n",
    "        return lbf, prob\n",
    "    \n",
    "    def get_cs(variant, prob, coverage=0.95):\n",
    "        ordering = np.argsort(prob)[::-1]\n",
    "        idx = np.where(np.cumsum(prob[ordering]) > coverage)[0][0]\n",
    "        cs = variant[ordering][: (idx + 1)]\n",
    "        return cs\n",
    "    def slalom(df,LD,abf_prior_variance = 0.4 ,nlog10p_dentist_s_threshold = 4, r2_threshold = 0.6  ):\n",
    "        from scipy import stats\n",
    "        lbf, prob = abf(df.beta, df.se, W=abf_prior_variance)\n",
    "        cs = get_cs(df.variant, prob, coverage=0.95)\n",
    "        cs_99 = get_cs(df.variant, prob, coverage=0.99)\n",
    "        df[\"lbf\"] = lbf\n",
    "        df[\"prob\"] = prob\n",
    "        df[\"cs\"] = df.variant.isin(cs)\n",
    "        df[\"cs_99\"] = df.variant.isin(cs_99)\n",
    "        \n",
    "        if ${lead_idx_choice} == \"pvalue\":\n",
    "            lead_idx_snp = df.pvalue.idxmin()\n",
    "        else:\n",
    "            lead_idx_snp = df.prob.idxmax()\n",
    "            \n",
    "        lead_variant = df.variant[lead_idx_snp]\n",
    "        df[\"lead_variant\"] = False\n",
    "        df[\"lead_variant\"].iloc[lead_idx_snp] = True\n",
    "        # annotate LD     \n",
    "        ## This is to identify the R for each snp vs the lead snp\n",
    "        df[\"r\"] = [LD[np.where(np.in1d(df.variant,lead_variant))][:,np.where(np.in1d(df.variant,x))][0][0][0] for x in df.variant]\n",
    "        lead_z = (df.beta / df.se).iloc[lead_idx_snp]\n",
    "        df[\"t_dentist_s\"] = ((df.beta / df.se) - df.r * lead_z) ** 2 / (1 - df.r ** 2)\n",
    "        df[\"t_dentist_s\"] = np.where(df[\"t_dentist_s\"] < 0, np.inf, df[\"t_dentist_s\"])\n",
    "        df[\"t_dentist_s\"].iloc[lead_idx_snp] = np.nan\n",
    "        df[\"nlog10p_dentist_s\"] = stats.chi2.logsf(df[\"t_dentist_s\"], df=1) / -np.log(10)\n",
    "        df[\"r2\"] = df.r ** 2\n",
    "        df[\"outliers\"] = (df.r2 > r2_threshold) & (df.nlog10p_dentist_s > nlog10p_dentist_s_threshold)\n",
    "        df_output = df\n",
    "        n_r2 = np.sum(df.r2 > r2_threshold)\n",
    "        n_dentist_s_outlier = np.sum(\n",
    "            (df.r2 > r2_threshold) & (df.nlog10p_dentist_s > nlog10p_dentist_s_threshold)\n",
    "        )\n",
    "        max_pip_idx = df.prob.idxmax()\n",
    "        df_summary = pd.DataFrame(\n",
    "            {\n",
    "                \"lead_pip_variant\": [df.variant.iloc[max_pip_idx]],\n",
    "                \"n_total\": [len(df.index)],\n",
    "                \"n_r2\": [n_r2],\n",
    "                \"n_dentist_s_outlier\": [n_dentist_s_outlier],\n",
    "                \"fraction\": [n_dentist_s_outlier / n_r2 if n_r2 > 0 else 0],\n",
    "                \"max_pip\": [np.max(df.prob)]\n",
    "            }\n",
    "            )\n",
    "        return df, df_summary\n",
    "    \n",
    "    ## Load LD\n",
    "    if \"${_input[0]}\".endswith(\"npz\"):\n",
    "        LD,snp_id = load_npz_ld(${_input[0]:r}) \n",
    "    if \"${_input[0]}\".endswith(\"bcor\"):\n",
    "        LD,snp_id = load_ld_bcor(${_input[0]:nr}) \n",
    "        \n",
    "    sumstat = pd.read_csv(${_input[1]:r}, \"\\t\")\n",
    "    ## Get only intersect snp\n",
    "    intersct = np.intersect1d(sumstat.variant.to_numpy(),snp_id)\n",
    "    sumstat = sumstat.query(\"variant in @intersct\").reset_index()\n",
    "    indice = np.where(np.in1d(snp_id, intersct))\n",
    "    LD = LD[np.ix_(indice[0].tolist(), indice[0].tolist())]    \n",
    "    ## slalom\n",
    "    ss_qc,ss_qc_sum = slalom(sumstat,LD,${abf_prior_variance},${nlog10p_dentist_s_threshold},${r2_threshold})\n",
    "    print(ss_qc_sum)\n",
    "    ss_qc.to_csv(${_output[1]:r})\n",
    "    ## Filter out outlier\n",
    "    LD = LD[np.ix_(ss_qc[~ss_qc.outliers].index,ss_qc[~ss_qc.outliers].index)]  \n",
    "    ss_qc = ss_qc[~ss_qc.outliers]\n",
    "    ss_qc = ss_qc.assign(z = ss_qc.beta/ss_qc.se)\n",
    "    ## SuSiERSS\n",
    "    string=\"\"\"\n",
    "    susie_rss_analysis=function(ss_df, R, n, var_y, z_ld_weight = 0, estimate_residual_variance = FALSE, \n",
    "    prior_variance = 50, check_prior = TRUE, output_path ){\n",
    "    res = susieR:::susie_rss(as.matrix(ss_df$z),as.matrix(R),n,var_y, z_ld_weight = 0, estimate_residual_variance = FALSE, \n",
    "    prior_variance = 50, check_prior = TRUE)\n",
    "    res$variants = ss_df$variant\n",
    "    res$z = ss_df$z\n",
    "    res$LD = R\n",
    "    res$corr = susieR:::get_cs_correlation(res, X = NULL, Xcorr = R, max = FALSE)\n",
    "    rownames(res$corr) <- names(res$cs)\n",
    "    colnames(res$corr) <- names(res$cs)\n",
    "    if (length(res$sets$cs) > 1) {\n",
    "        index_combos = expand.grid(1:length(res$sets$cs),1:length(res$sets$cs))\n",
    "        in_common = apply(index_combos, 1, function(x) intersect(res$sets$cs[[x[1]]], res$sets$cs[[x[2]]]))\n",
    "        counts = unlist(lapply(in_common, length))\n",
    "        ovlp_mat = matrix(counts, ncol = length(res$sets$cs), byrow = T)\n",
    "        ovlp_mat[lower.tri(ovlp_mat)] = NA\n",
    "        rownames(ovlp_mat) = names(res$sets$cs)\n",
    "        colnames(ovlp_mat) = names(res$sets$cs)\n",
    "        print(ovlp_mat)\n",
    "        res$sets[[\"ovlp_mat\"]] = ovlp_mat\n",
    "    }\n",
    "    saveRDS(res,output_path)\n",
    "    return(res)\n",
    "    }\n",
    "    \"\"\"\n",
    "    susie_analysis = SignatureTranslatedAnonymousPackage(string, \"susie_analysis\")\n",
    "    analysis_result = susie_analysis.susie_rss_analysis(ss_df = ss_qc,R = LD,output_path=${_output[0]:r} ${f', n ={n}' if n > 2 else \"\"})\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Singularity Susie_rpy2",
   "language": "python",
   "name": "rpy2_susie"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
