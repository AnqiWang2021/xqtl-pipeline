{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e1e633c",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Fine-mapping with SuSiE RSS model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c57bcd",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This notebook take a list of LD file and a list of sumstat file and do salmon QC and susie RSS for each overlap LD block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cbf18f",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3351974b",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1. A tab delimated table describing the path where LD per region stored, can be generated using the ld_per_region_plink step of the genotype processing module.\n",
    "\n",
    "```\n",
    "#id     dir\n",
    "chr17_60570445_65149278 /mnt/vast/hpc/csg/molecular_phenotype_calling/LD/output_npz_2/1300_hg38_EUR_LD_blocks_npz_files/ROSMAP_NIA_WGS.leftnorm.filtered.filtered.chr17_60570445_65149278.flt16.npz\n",
    "```\n",
    "\n",
    "2. A tab delimated table describing  path where summary stat per chromosome stored, can be generated using the yml_generator module before the qced sumstat are generated.\n",
    "```\n",
    "hs3163@csglogin:/mnt/vast/hpc/csg/xqtl_workflow_testing/susie_rss$ cat /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/qced_sumstat_list.txt\n",
    "#chr    ADGWAS_Bellenguez_2022\n",
    "1       /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/ADGWAS_Bellenguez_2022.1/ADGWAS2022.chr1.sumstat.tsv\n",
    "2       /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/ADGWAS_Bellenguez_2022.2/ADGWAS2022.chr2.sumstat.tsv\n",
    "3       /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/ADGWAS_Bellenguez_2022.3/ADGWAS2022.chr3.sumstat.tsv\n",
    "4       /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/ADGWAS_Bellenguez_2022.4/ADGWAS2022.chr4.sumstat.tsv\n",
    "5       /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/ADGWAS_Bellenguez_2022.5/ADGWAS2022.chr5.sumstat.tsv\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48ae441",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e496bda2",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1. A RDS file containing the output susie object, the name of all variants that went through the analysis, the z score , and the LD used for the analysis.\n",
    "2. A sumstat file with additional column containing the slalom results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b710c514",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## MWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b16c49d4",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-158593d50ce7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-158593d50ce7>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    sos run pipeline/SuSiE_RSS.ipynb SuSiE_RSS \\\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sos run pipeline/SuSiE_RSS.ipynb SuSiE_RSS \\\n",
    "    --LD_list test.ld.list \\\n",
    "    --sumstat_list /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/qced_sumstat_list.txt \\\n",
    "    --container containers/stephenslab.sif --impute --cwd output_impute_2 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6edb76",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: container = \"\"\n",
    "parameter: entrypoint={('micromamba run -n' + ' ' + container.split('/')[-1][:-4]) if container.endswith('.sif') else f''}\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"140h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"230G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 1\n",
    "parameter: cwd = path(\"output\")\n",
    "# getting the overlapped input\n",
    "parameter: LD_list = path\n",
    "parameter: sumstat_list = path\n",
    "import pandas as pd\n",
    "LD_list = pd.read_csv(LD_list,sep=\"\\t\")\n",
    "sumstat_list = pd.read_csv(sumstat_list,sep=\"\\t\")\n",
    "LD_list[\"#chr\"] = [x[0].replace(\"chr\", \"\") for x in  LD_list[\"#id\"].str.split(\"_\") ]\n",
    "sumstat_list[\"#chr\"] = [str(x).replace(\"chr\", \"\") for x in  sumstat_list[\"#chr\"] ]\n",
    "input_inv = LD_list.merge(sumstat_list)\n",
    "input_list = input_inv.iloc[:,[1,3]].values.tolist()\n",
    "parameter: lead_idx_choice = \"pvalue\"\n",
    "parameter: abf_prior_variance = 0.4\n",
    "parameter: nlog10p_dentist_s_threshold = 4\n",
    "parameter: r2_threshold = 0.6\n",
    "parameter: n = 0\n",
    "parameter: max_iter = 1000\n",
    "parameter: impute = True # Whether to impute the sumstat for all the snp in LD but not in sumstat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7232236",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[SuSiE_RSS]\n",
    "parameter: L = 10\n",
    "parameter: max_L = 1000\n",
    "input: input_list, group_by = 2\n",
    "name = f'{_input[0]:b}'.split(\".\")[-3]\n",
    "output: f'{cwd:a}/{_input[1]:bn}.{name}.unisusie_rss.fit.rds',\n",
    "        f'{cwd:a}/{_input[1]:bn}.{name}.unisusie_rss.ss_qced.tsv'\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "python: expand = '${ }', stdout = f\"{_output[0]:nn}.stdout\", stderr = f\"{_output[0]:nn}.stderr\", container = container, entrypoint = entrypoint\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import rpy2\n",
    "    from backports import zoneinfo\n",
    "    import rpy2.robjects.numpy2ri as numpy2ri\n",
    "    from rpy2.robjects import pandas2ri\n",
    "    from rpy2.robjects.packages import SignatureTranslatedAnonymousPackage\n",
    "    import rpy2.robjects as ro\n",
    "    numpy2ri.activate()\n",
    "    pandas2ri.activate()\n",
    "    def load_npz_ld(path):\n",
    "        np_ld_loaded = np.load(path,allow_pickle=True)\n",
    "        # sort by start position\n",
    "        snp_id = [x.replace(\":\",\"_\") for x in np_ld_loaded.get(\"arr_1\")]\n",
    "        np_ld_loaded = np_ld_loaded.get(\"arr_0\")\n",
    "        new = np_ld_loaded + np_ld_loaded.T\n",
    "        np.fill_diagonal(new, np.diag(new)/2)\n",
    "        return new,snp_id\n",
    "\n",
    "    def get_bcor_meta(bcor_obj):\n",
    "        df_ld_snps = bcor_obj.getMeta()\n",
    "        df_ld_snps.rename(columns={'rsid':'SNP', 'position':'BP', 'chromosome':'CHR', 'allele1':'A1', 'allele2':'A2'}, inplace=True, errors='raise')\n",
    "        ###df_ld_snps['CHR'] = df_ld_snps['CHR'].astype(np.int64)\n",
    "        df_ld_snps['BP'] = df_ld_snps['BP'].astype(np.int64)\n",
    "        return df_ld_snps\n",
    "\n",
    "    def load_ld_bcor(ld_prefix):\n",
    "        bcor_file = ld_prefix+'.bcor'\n",
    "        import os\n",
    "        import time\n",
    "        from ldstore.bcor import bcor\n",
    "        if not os.path.exists(bcor_file):\n",
    "            raise IOError('%s not found'%(bcor_file))\n",
    "        t0 = time.time()\n",
    "        bcor_obj = bcor(bcor_file)\n",
    "        df_ld_snps = get_bcor_meta(bcor_obj)\n",
    "        ld_arr = bcor_obj.readCorr([])\n",
    "        assert np.all(~np.isnan(ld_arr))\n",
    "        return ld_arr, df_ld_snps\n",
    "\n",
    "    def abf(z, se, W=0.04):\n",
    "        from scipy import special \n",
    "        V = se ** 2\n",
    "        r = W / (W + V)\n",
    "        lbf = 0.5 * (np.log(1 - r) + (r * z ** 2))\n",
    "        denom = special.logsumexp(lbf)\n",
    "        prob = np.exp(lbf - denom)\n",
    "        return lbf, prob\n",
    "    \n",
    "    def get_cs(variant, prob, coverage=0.95):\n",
    "        ordering = np.argsort(prob)[::-1]\n",
    "        idx = np.where(np.cumsum(prob[ordering]) > coverage)[0][0]\n",
    "        cs = variant[ordering][: (idx + 1)]\n",
    "        return cs\n",
    "    def slalom(df,LD,abf_prior_variance = 0.4 ,nlog10p_dentist_s_threshold = 4, r2_threshold = 0.6  ):\n",
    "        from scipy import stats\n",
    "        lbf, prob = abf(df.z, df.se if \"se\" in df.columns else 1, W=abf_prior_variance)\n",
    "        cs = get_cs(df.variant, prob, coverage=0.95)\n",
    "        cs_99 = get_cs(df.variant, prob, coverage=0.99)\n",
    "        df[\"lbf\"] = lbf\n",
    "        df[\"prob\"] = prob\n",
    "        df[\"cs\"] = df.variant.isin(cs)\n",
    "        df[\"cs_99\"] = df.variant.isin(cs_99)\n",
    "        \n",
    "        if \"${lead_idx_choice}\" == \"pvalue\":\n",
    "            lead_idx_snp = df.pvalue.idxmin()\n",
    "        else:\n",
    "            lead_idx_snp = df.prob.idxmax()\n",
    "            \n",
    "        lead_variant = df.variant[lead_idx_snp]\n",
    "        df[\"lead_variant\"] = False\n",
    "        df[\"lead_variant\"].iloc[lead_idx_snp] = True\n",
    "        # annotate LD     \n",
    "        ## This is to identify the R for each snp vs the lead snp\n",
    "        df[\"r\"] = [LD[np.where(np.in1d(df.variant,lead_variant))][:,np.where(np.in1d(df.variant,x))][0][0][0] for x in df.variant]\n",
    "        lead_z = df.z.iloc[lead_idx_snp]\n",
    "        df[\"t_dentist_s\"] = (df.z - df.r * lead_z) ** 2 / (1 - df.r ** 2)\n",
    "        df[\"t_dentist_s\"] = np.where(df[\"t_dentist_s\"] < 0, np.inf, df[\"t_dentist_s\"])\n",
    "        df[\"t_dentist_s\"].iloc[lead_idx_snp] = np.nan\n",
    "        df[\"nlog10p_dentist_s\"] = stats.chi2.logsf(df[\"t_dentist_s\"], df=1) / -np.log(10)\n",
    "        df[\"r2\"] = df.r ** 2\n",
    "        df[\"outliers\"] = (df.r2 > r2_threshold) & (df.nlog10p_dentist_s > nlog10p_dentist_s_threshold)\n",
    "        df_output = df\n",
    "        n_r2 = np.sum(df.r2 > r2_threshold)\n",
    "        n_dentist_s_outlier = np.sum(\n",
    "            (df.r2 > r2_threshold) & (df.nlog10p_dentist_s > nlog10p_dentist_s_threshold)\n",
    "        )\n",
    "        max_pip_idx = df.prob.idxmax()\n",
    "        df_summary = pd.DataFrame(\n",
    "            {\n",
    "                \"lead_pip_variant\": [df.variant.iloc[max_pip_idx]],\n",
    "                \"n_total\": [len(df.index)],\n",
    "                \"n_r2\": [n_r2],\n",
    "                \"n_dentist_s_outlier\": [n_dentist_s_outlier],\n",
    "                \"fraction\": [n_dentist_s_outlier / n_r2 if n_r2 > 0 else 0],\n",
    "                \"max_pip\": [np.max(df.prob)]\n",
    "            }\n",
    "            )\n",
    "        return df, df_summary\n",
    "    \n",
    "    ## Load LD\n",
    "    if \"${_input[0]}\".endswith(\"npz\"):\n",
    "        LD,snp_id = load_npz_ld(${_input[0]:r}) \n",
    "    if \"${_input[0]}\".endswith(\"bcor\"):\n",
    "        LD,snp_id = load_ld_bcor(${_input[0]:nr}) \n",
    "        \n",
    "    sumstat = pd.read_csv(${_input[1]:r}, sep=\"\\t\")\n",
    "    # check if z column already exists. If not then create it\n",
    "    if \"z\" not in sumstat.columns:\n",
    "        sumstat[\"z\"] = sumstat.beta / sumstat.se\n",
    "    \n",
    "    ## Remove nan in LD\n",
    "    where = np.where(np.isnan(LD))\n",
    "    na_snp = np.array(snp_id)[[x for x in where[0]] + [x for x in where[1]]]\n",
    "    ### Remove NA from LD\n",
    "    LD = LD[np.ix_(~np.in1d(snp_id, na_snp),~np.in1d(snp_id, na_snp))]\n",
    "    ### Remove NA from snp_list\n",
    "    snp_id = np.array(snp_id)[~np.in1d(snp_id, na_snp)]\n",
    "    \n",
    "    ## Get only intersect snp\n",
    "    intersct = np.intersect1d(sumstat.variant.to_numpy(),snp_id)\n",
    "    sumstat = sumstat.query(\"variant in @intersct\").reset_index()\n",
    "    indice = np.where(np.in1d(snp_id, intersct))\n",
    "    ## slalom\n",
    "    ss_qc,ss_qc_sum = slalom(sumstat,LD[np.ix_(indice[0].tolist(), indice[0].tolist())],${abf_prior_variance},${nlog10p_dentist_s_threshold},${r2_threshold})\n",
    "    print(ss_qc_sum)\n",
    "    ss_qc.to_csv(${_output[1]:r},\"\\t\",index = 0 )\n",
    "    ## Filter out outlier\n",
    "    ### Get outliers\n",
    "    outliers = ss_qc[ss_qc.outliers].variant\n",
    "    ### Remove outliers from LD\n",
    "    LD = LD[np.ix_(~np.in1d(snp_id, outliers),~np.in1d(snp_id, outliers))]\n",
    "    ### Remove outliers from snp_list\n",
    "    snp_id = np.array(snp_id)[~np.in1d(snp_id, outliers)]\n",
    "    ss_qc = ss_qc[~ss_qc.outliers]\n",
    "\n",
    "    if ${impute}:\n",
    "        cur_LD = LD\n",
    "        cur_miss = ~np.in1d(snp_id, ss_qc.variant)\n",
    "        print(f'{np.mean(cur_miss)} of snps in the LD panel does not have sumstat. Their sumstat are imputed')\n",
    "        ## Sumstat imputation based on fusion code\n",
    "        cur_wgt = cur_LD[np.ix_(cur_miss, np.logical_not(cur_miss))].dot(np.linalg.inv(cur_LD[np.logical_not(cur_miss), np.logical_not(cur_miss)] + 0.1 * np.eye(np.sum(np.logical_not(cur_miss)))))\n",
    "        cur_impz  = cur_wgt @ ss_qc.z\n",
    "        cur_r2pred = np.diag(cur_wgt.dot(cur_LD[np.ix_(np.logical_not(cur_miss),np.logical_not(cur_miss))]).dot(cur_wgt.T))\n",
    "        imputed_z = cur_impz/np.sqrt(cur_r2pred)\n",
    "        working_ss = pd.concat([ss_qc[[\"variant\",\"z\"]], pd.DataFrame({\"z\" : imputed_z, \"variant\": np.array(snp_id)[cur_miss]})]).set_index(\"variant\").reindex(snp_id).reset_index()\n",
    "        import scipy.stats as st\n",
    "        working_ss[\"beta\"] = working_ss.z\n",
    "        working_ss[\"se\"] = 1\n",
    "        working_ss[\"pvalue\"] = (1 - st.norm.cdf(working_ss.z))*2\n",
    "        \n",
    "        \n",
    "        working_ss_qc, working_ss_qc_sum = slalom(working_ss,cur_LD,0.4,4,0.6)\n",
    "        print(working_ss_qc_sum)\n",
    "        working_ss_qc.to_csv('${_output[1]:nn}.imputed_ss_qced.tsv',\"\\t\",index = 0 )\n",
    "        imputed_outliers = working_ss_qc[working_ss_qc.outliers].variant\n",
    "        ### Remove outliers from LD\n",
    "        cur_LD = LD[np.ix_(~np.in1d(snp_id, imputed_outliers),~np.in1d(snp_id, imputed_outliers))]\n",
    "        ### Remove outliers from snp_list\n",
    "        snp_id = np.array(snp_id)[~np.in1d(snp_id, imputed_outliers)]\n",
    "        working_ss_qc = working_ss_qc[~working_ss_qc.outliers]\n",
    "        ss_qc = working_ss_qc\n",
    "        LD = cur_LD \n",
    "    ### For non-imputed LD, retained only the overlap snps. For imputed LD, this should do nothing as snp_id, ss_qc.variant .\n",
    "    indice = np.where(np.in1d(snp_id, ss_qc.variant))\n",
    "    LD = LD[np.ix_(indice[0].tolist(), indice[0].tolist())]\n",
    "    ## SuSiERSS\n",
    "    string=\"\"\"\n",
    "    susie_rss_analysis=function(ss_df, R, n, var_y, z_ld_weight = 0, estimate_residual_variance = FALSE, \n",
    "    prior_variance = 50, check_prior = TRUE, output_path,L = ${L} , max_iter  = ${max_iter} ){\n",
    "    res = susieR:::susie_rss(as.matrix(ss_df$z),as.matrix(R), n = ${n},var_y, z_ld_weight = 0, estimate_residual_variance = FALSE, \n",
    "    prior_variance = 50, check_prior = TRUE, max_iter = ${max_iter},verbose = TRUE )\n",
    "    res$variants = ss_df$variant\n",
    "    res$z = ss_df$z\n",
    "    res$corr = susieR:::get_cs_correlation(res, X = NULL, Xcorr = R, max = FALSE)\n",
    "    rownames(res$corr) <- names(res$cs)\n",
    "    colnames(res$corr) <- names(res$cs)\n",
    "    if (length(res$sets$cs) > 1) {\n",
    "        index_combos = expand.grid(1:length(res$sets$cs),1:length(res$sets$cs))\n",
    "        in_common = apply(index_combos, 1, function(x) intersect(res$sets$cs[[x[1]]], res$sets$cs[[x[2]]]))\n",
    "        counts = unlist(lapply(in_common, length))\n",
    "        ovlp_mat = matrix(counts, ncol = length(res$sets$cs), byrow = T)\n",
    "        ovlp_mat[lower.tri(ovlp_mat)] = NA\n",
    "        rownames(ovlp_mat) = names(res$sets$cs)\n",
    "        colnames(ovlp_mat) = names(res$sets$cs)\n",
    "        print(ovlp_mat)\n",
    "        res$sets[[\"ovlp_mat\"]] = ovlp_mat\n",
    "    }\n",
    "    saveRDS(res,output_path)\n",
    "    return(res)\n",
    "    }\n",
    "    \"\"\"\n",
    "    susie_analysis = SignatureTranslatedAnonymousPackage(string, \"susie_analysis\")\n",
    "    analysis_result = susie_analysis.susie_rss_analysis(ss_df = ss_qc,R = LD,output_path=${_output[0]:r} ${f', n ={n}' if n > 2 else \"\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "version": "0.24.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
