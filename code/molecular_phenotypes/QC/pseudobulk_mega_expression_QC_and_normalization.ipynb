{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Pseudobulk mega phenotype data QC and normalization\n",
    "It is based on Nick's code and normalization method from [this page](https://bigomics.ch/blog/why-how-normalize-rna-seq-data/). Should be optimized for general use. \n",
    "\n",
    "The main idea is to merge the 3 dataset of snRNA pseudobulk from `phils' 1st version(masasshi)`, `kelli` and `phils' 2nd version` so as to get the mega data with more samples(~800) to be more powerful to download QTL analysis. First do the normalization within dataset, then `removeBatchEffect()` function from `limma` package was used to remove the batch effect from these 3 data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input\n",
    "The input is pseudo bulk eqtl phenotype data of raw count matrix. In this notebook, we use the following files as input:\n",
    "phenotype original file:\n",
    "1) De Jager batch:  \n",
    "`/mnt/vast/hpc/homes/al4225/pseudobulk_phil_old/cell_expr_sampleid/snuc_pseudo_bulk.{tissue}.count_matrix`\n",
    "2) Kellis batch:   \n",
    "`/mnt/vast/hpc/homes/al4225/project/fungen-xqtl-analysis/analysis/Wang_Columbia/ROSMAP/pseudo_bulk_eqtl_kelli/cell_expr_sampleid/snuc_pseudo_bulk.{tissue}.count_matrix`\n",
    "3) De Jager new batch:   \n",
    "`/mnt/vast/hpc/homes/al4225/pseudobulk_phil_new/cell_expr/snuc_pseudo_bulk.{tissue}.count_matrix`\n",
    "\n",
    "For Ast, Inh, Oli, OPC, Mic, Exc, the input is separate raw count matrix(1st col is gene name, 2nd col to the end are sampleid with raw counts expression value), for each specific celltype. So we list the celltype name as 1st col, phil's 1st data path as the 2nd col, kelli's data as the 3rd col, phil's 2nd data as the 4th col in a txt file as the input. e.g.     \n",
    "`Ast\t/mnt/vast/hpc/homes/al4225/pseudobulk_phil_old/cell_expr_sampleid/snuc_pseudo_bulk.Ast.count_matrix\t/mnt/vast/hpc/homes/al4225/project/fungen-xqtl-analysis/analysis/Wang_Columbia/ROSMAP/pseudo_bulk_eqtl_kelli/cell_expr_sampleid/snuc_pseudo_bulk.Ast.count_matrix\t/mnt/vast/hpc/homes/al4225/pseudobulk_phil_new/cell_expr/snuc_pseudo_bulk.Ast.count_matrix`\n",
    "\n",
    "\n",
    "## Steps:\n",
    "-- `Count Cells by Sample`: Calculate the number of cells for each sample using metadata. This helps in filtering samples based on cell count.    \n",
    "-- `Filter Samples`: Exclude samples with fewer than 10 cells to ensure sufficient data quality and representativeness.\n",
    "-- `Select samples`: Include all samples from phil's 1st data, then keep new samples from kelli's data based on phil's 1st data and get the merged12 data, and them keep new samples from phil's 2nd data based on the merged12 data.\n",
    "-- `Gene Filtering`: Use `filterByExpr()` to retain genes with sufficient expression across samples, improving the reliability of statistical tests.    \n",
    "-- `Normalization`: Apply TMM normalization to adjust for composition effects, making counts between samples comparable.    \n",
    "-- `Voom Transformation`: Transform count data to log2-counts per million (logCPM), stabilizing variance across genes.   \n",
    "-- `Filter by Expression`: Remove genes with mean log2CPM < 2.0 to focus on genes with significant expression levels.    \n",
    "-- `Remove batch effect:` `removeBatchEffect()` function from `limma` package was used to remove the batch effect from these 3 data.     \n",
    "-- `Quantile normalization`: Apply quantile normalization to ensure that the distribution of expression values is consistent across samples.    \n",
    "\n",
    "## Output\n",
    "\n",
    "The output is a normalized.log2cpm.tsv file, with 1st column  `id` as gene name, then the sampleids as following columns.      \n",
    "**1) Normalized.log2cpm.tsv file**: `/home/al4225/pseudobulk_merge/mega_quantnorm_SM/snuc_pseudo_bulk.{tissue}.mega.normalized.log2cpm.tsv`    \n",
    "**2) Raw count matrix**(keep all samples of phil's old version, then keep new kelli's sample, then keep new samples of phil's new version): `/home/al4225/pseudobulk_merge/cellcounts_mega/snuc_pseudo_bulk.{tissue}.mega.count_matrix`    \n",
    "**3) Cell counts of each sample**: `/home/al4225/pseudobulk_merge/ncells_mega/snuc_pseudo_bulk.{tissue}.mega.nCells`    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Global parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sos"
    }
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# It is required to input the name of the analysis\n",
    "parameter: name = str\n",
    "parameter: cwd = path(\"output\")\n",
    "parameter: container = \"\"\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 5\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"20h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Phenotype merging and QC\n",
    "Before merging, should make sure the sample names from different dataset are using the same format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sos run pipeline/pseudobulk_mega_expression_QC_and_normalization.ipynb mergedata \\\n",
    "    --name snuc_pseudo_bulk \\\n",
    "    --file_paths /mnt/vast/hpc/homes/al4225/pseudobulk_merge/celltypes_3.txt \\\n",
    "    --cwd /mnt/vast/hpc/homes/al4225/pseudobulk_merge/mega_quantnorm_SM/ \\\n",
    "    --container /home/al4225/project/fungen-xqtl-analysis/analysis/Wang_Columbia/ROSMAP/pseudo_bulk_eqtl_kelli/container/seurat.sif \\\n",
    "    --mem 80G -J 50 -c /mnt/vast/hpc/csg/molecular_phenotype_calling/csg.yml -q csg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "[mergedata]\n",
    "# data: 1st: phil's old, 2nd: kelli, 3rd:phils new\n",
    "import pandas as pd\n",
    "# load the input txt file with celltype name and paths\n",
    "parameter: file_paths = path()\n",
    "\n",
    "#for each tissue.\n",
    "input_file_paths = pd.read_csv(file_paths, sep = \"\\t\", header=None)\n",
    "print(input_file_paths)\n",
    "input_inv = input_file_paths.values.tolist()\n",
    "tissue_id_inv = [x[0] for x in input_inv]\n",
    "input_files = [x[1:] for x in input_inv]\n",
    "\n",
    "print(\"\\ntissue ID List:\")\n",
    "print(tissue_id_inv)\n",
    "print(\"\\nFile List:\")\n",
    "print(input_files)\n",
    "print(\"Length of tissue_id_inv:\", len(tissue_id_inv))\n",
    "print(\"Length of group by ts:\", len(input_files[0]))\n",
    "input: input_files, group_by =len(input_files[0]), group_with = \"tissue_id_inv\" \n",
    "output: normalized_log2cpm = f'{cwd:a}/{name}.{_tissue_id_inv}.mega.normalized.log2cpm.tsv'\n",
    "#output: gene_expression_matrix = f'{cwd:a}/{name}.{_tissue_id_inv}.mega.count_matrix' # raw count matrix output\n",
    "#output: cell_counts = f'{cwd:a}/{name}.{_tissue_id_inv}.mega.nCells' # cell counts of each sample output\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:0}.stdout\", stderr = f\"{_output:0}.stderr\", container = container, entrypoint = entrypoint\n",
    "library(Seurat)\n",
    "library(edgeR)\n",
    "library(limma)\n",
    "library(dplyr)\n",
    "\n",
    "# phils' version1\n",
    "expr1 <- read.table(${_input[0]:r}, header = TRUE, row.names = 1, check.names=F)\n",
    "cat(\"load phils version1:\\n\")\n",
    "head(expr1)[,1:10]\n",
    "\n",
    "# Kelli's data\n",
    "expr2 <- read.table(${_input[1]:r}, header = TRUE, row.names = 1, check.names=F)\n",
    "cat(\"load kelli data:\\n\")\n",
    "head(expr2)[,1:10]\n",
    "\n",
    "# phil's new data\n",
    "expr3 = read.table(${_input[2]:r}, sep=\"\\t\", header=T, check.names=F)\n",
    "cat(\"load phils new data version2:\\n\")\n",
    "head(expr3)[,1:10]\n",
    "\n",
    "genes1 = rownames(expr1)\n",
    "genes2 = rownames(expr2)\n",
    "genes3 = rownames(expr3)\n",
    "\n",
    "# Find common genes among all three sets\n",
    "common_genes = Reduce(intersect, list(genes1, genes2, genes3))\n",
    "\n",
    "# Filter the expression matrices to keep only common genes\n",
    "expr1 = expr1[common_genes, ]\n",
    "expr2 = expr2[common_genes, ]\n",
    "expr3 = expr3[common_genes, ]\n",
    "\n",
    "sample1 = colnames(expr1)\n",
    "sample2 = colnames(expr2)\n",
    "sample3 = colnames(expr3)\n",
    "common_samples = intersect(sample1, sample2)\n",
    "cat(\"common samplesof expr1 and expr2:\\n\")\n",
    "print(common_samples)\n",
    "# remove the common samples in expr2\n",
    "expr2 <- expr2[, !(colnames(expr2) %in% common_samples)]\n",
    "expr12 <- cbind(expr1,expr2)\n",
    "common_samples12_3 = intersect(colnames(expr12), sample3)\n",
    "# remove the common samples in expr3\n",
    "expr3 <- expr3[, !(colnames(expr3) %in% common_samples12_3)]\n",
    "expr_raw <- cbind(expr12,expr3)\n",
    "#write.table(expr_raw, file = \"${_output['gene_expression_matrix']}\", sep = \"\\t\", row.names = TRUE, quote = FALSE, col.names = TRUE)\n",
    "\n",
    "# get the ncell df\n",
    "sample_ids <- colnames(expr_raw)\n",
    "cell_counts <- colSums(expr_raw)\n",
    "cellcounts <- data.frame(sampleid = sample_ids, ncell = cell_counts)\n",
    "head(cellcounts)\n",
    "#write.table(cellcounts, file = \"${_output['cell_counts']}\", sep = \"\\t\", row.names = FALSE, col.names = TRUE, quote = FALSE)\n",
    "\n",
    "#filtering out samples with fewer than 10 cells in a celltype\n",
    "col_sums <- colSums(expr1, na.rm = TRUE)\n",
    "print(paste(\"Original data frame dimensions of expr1:\", dim(expr1)))\n",
    "keep_cols <- names(col_sums)[col_sums > 9]\n",
    "expr1 <- expr1[keep_cols]\n",
    "\n",
    "col_sums <- colSums(expr2, na.rm = TRUE)\n",
    "keep_cols <- names(col_sums)[col_sums > 9]\n",
    "expr2 <- expr2[keep_cols]\n",
    "\n",
    "col_sums <- colSums(expr3, na.rm = TRUE)\n",
    "keep_cols <- names(col_sums)[col_sums > 9]\n",
    "expr3 <- expr3[keep_cols]\n",
    "\n",
    "y1 <- DGEList(counts = expr1)\n",
    "keep <- filterByExpr(y1)\n",
    "y1 <- y1[keep,,keep.lib.sizes=F]\n",
    "#counts per million\n",
    "y1 <- calcNormFactors(y1, method = \"TMM\")\n",
    "v1 <- voom(y1, plot=F)\n",
    "logcpm1 <- v1$E\n",
    "\n",
    "y2 <- DGEList(counts = expr2)\n",
    "keep <- filterByExpr(y2)\n",
    "y2 <- y2[keep,,keep.lib.sizes=F]\n",
    "#counts per million\n",
    "y2 <- calcNormFactors(y2, method = \"TMM\")\n",
    "v2 <- voom(y2, plot=F)\n",
    "logcpm2 <- v2$E\n",
    "\n",
    "y3 <- DGEList(counts = expr3)\n",
    "keep <- filterByExpr(y3)\n",
    "y3 <- y3[keep,,keep.lib.sizes=F]\n",
    "#counts per million\n",
    "y3 <- calcNormFactors(y3, method = \"TMM\")\n",
    "v3 <- voom(y3, plot=F)\n",
    "logcpm3 <- v3$E\n",
    "\n",
    "genes1 = rownames(logcpm1)\n",
    "genes2 = rownames(logcpm2)\n",
    "genes3 = rownames(logcpm3)\n",
    "\n",
    "# Find common genes among all three sets\n",
    "common_genes = Reduce(intersect, list(genes1, genes2, genes3))\n",
    "\n",
    "# Filter the expression matrices to keep only common genes\n",
    "logcpm1 = logcpm1[common_genes, ]\n",
    "logcpm2 = logcpm2[common_genes, ]\n",
    "logcpm3 = logcpm3[common_genes, ]\n",
    "\n",
    "batch <- c(rep(\"Batch_phil1\", ncol(logcpm1)), rep(\"Batch_kelli\", ncol(logcpm2)), rep(\"Batch_phil2\", ncol(logcpm3)))\n",
    "\n",
    "logcpm <- cbind(logcpm1, logcpm2,logcpm3)\n",
    "# remove genes if mean log2CPM < 2.0\n",
    "mean_logcpm <- apply(logcpm, 1, mean)\n",
    "logcpm <- logcpm[mean_logcpm > 2.0,]\n",
    "\n",
    "# remove batch effect\n",
    "logcpm <- removeBatchEffect(logcpm, batch=batch)\n",
    "logcpm <- as.data.frame(logcpm)\n",
    "logcpm$id <- rownames(logcpm)\n",
    "rownames(logcpm) <- NULL\n",
    "logcpm <- logcpm[, c(\"id\", setdiff(names(logcpm), \"id\"))]\n",
    "\n",
    "# convert log2CPM to matrix\n",
    "logcpm_id <- logcpm$id\n",
    "logcpm <- as.matrix(logcpm[, colnames(logcpm) != \"id\"])\n",
    "rownames(logcpm) <- logcpm_id\n",
    "\n",
    "# quantile normalizarion\n",
    "logcpm <- t(apply(logcpm, 1, rank, ties.method = \"average\"))\n",
    "logcpm <- qnorm(logcpm / (ncol(logcpm) + 1))\n",
    "\n",
    "# export\n",
    "df <- data.frame(id = rownames(logcpm), logcpm, check.names = F)\n",
    "write.table(df, file=\"${_output['normalized_log2cpm']}\", sep=\"\\t\", quote = F, row.names = F)\n",
    "\n",
    "cat(\"the mega normalized pseudo_bulk_eqtl tsv are saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (my_kernel)",
   "language": "python",
   "name": "my_python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.24.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
