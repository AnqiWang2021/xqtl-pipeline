{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Stratified LD Score Regression \n",
    "This notebook implements the pipepline of [S-LDSC](https://github.com/bulik/ldsc/wiki) for LD score and functional enrichment analysis. It is written by Anmol Singh (singh.anmol@columbia.edu), with input from Dr. Gao Wang.\n",
    "\n",
    "**FIXME: the initial draft is complete but pending Gao's review and documentation with minimal working example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "The pipeline is developed to integrate GWAS summary statistics data, annotation data, and LD reference panel data to compute functional enrichment for each of the epigenomic annotations that the user provides using the S-LDSC model. We will first start off with an introduction, instructions to set up, and the minimal working examples. Then the workflow code that can be run using SoS on any data will be at the end. \n",
    "\n",
    "## A brief review on Stratified LD score regression\n",
    "\n",
    "Here I briefly review LD Score Regression and what it is used for. For more in depth information on LD Score Regression please read the following three papers:\n",
    "\n",
    "1. \"LD Score regression distinguishes confounding from polygenicity in genome-wide association studies\" by Sullivan et al (2015)\n",
    "\n",
    "2. \"Partitioning heritability by functional annotation using genome-wide association summary statistics\" by Finucane et al (2015)\n",
    "\n",
    "3. \"Linkage disequilibrium–dependent architecture of human complex traits shows action of negative selection\" by Gazal et al (2017)\n",
    "\n",
    "As stated in Sullivan et al 2015, confounding factors and polygenic effects can cause inflated test statistics and other methods cannot distinguish between inflation from confounding bias and a true signal. LD Score Regression (LDSC) is a technique that aims to identify the impact of confounding factors and polygenic effects using information from GWAS summary statistics. \n",
    "\n",
    "This approach involves using regression to mesaure the relationship between Linkage Disequilibrium (LD) scores and test statistics of SNPs from the GWAS summary statistics. Variants in LD with a \"causal\" variant show an elevation in test statistics in association analysis proportional to their LD (measured by $r^2$) with the causal variant within a certain window size (could be 1 cM, 1kB, etc.). In contrast, inflation from confounders such as population stratification that occur purely from genetic drift will not correlate with LD. For a polygenic trait, SNPs with a high LD score will have more significant χ2 statistics on average than SNPs with a low LD score. Thus, if we regress the $\\chi^2$ statistics from GWAS against LD Score, the intercept minus one is an estimator of the mean contribution of confounding bias to the inflation in the test statistics. The regression model is known as LD Score regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### LDSC model\n",
    "\n",
    "Under a polygenic assumption, in which effect sizes for variants are drawn independently from distributions with variance proportional to  $1/(p(1-p))$ where p is the minor allele frequency (MAF), the expected $\\chi^2$ statistic of variant j is:\n",
    "\n",
    "$$E[\\chi^2|l_j] = Nh^2l_j/M + Na + 1 \\quad (1)$$\n",
    "\n",
    "where $N$ is the sample size; $M$ is the number of SNPs, such that $h^2/M$ is the average heritability explained per SNP; $a$ measures the contribution of confounding biases, such as cryptic relatedness and population stratification; and $l_j = \\sum_k r^2_{jk}$ is the LD Score of variant $j$, which measures the amount of genetic variation tagged by $j$. A full derivation of this equation is provided in the Supplementary Note of Sullivan et al (2015). An alternative derivation is provided in Supplementary Note of Zhu and Stephens (2017) AoAS.\n",
    "\n",
    "From this we can see that LD Score regression can be used to compute SNP-based heritability for a phenotype or trait, from GWAS summary statistics and does not require genotype information like other methods such as REML do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Stratified LDSC\n",
    "\n",
    "Heritability is the proportion of phenotypic variation (VP) that is due to variation in genetic values (VG) and thus can tell us how much of the difference in observed phenotypes in a sample is due to difference in genetics in the sample. It can also be extended to analyze partitioned heritability for a phenotype/trait split over categories. \n",
    "\n",
    "For Partitioned Heritability or Stratified LD Score Regression (S-LDSC) more power is added to our analysis by leveraging LD Score information as well as using SNPs that haven't reached Genome Wide Significance to partition heritability for a trait over categories which many other methods do not do. \n",
    "\n",
    "\n",
    "S-LDSC relies on the fact that the $\\chi^2$ association statistic for a given SNP includes the effects of all SNPs tagged by this SNP meaning that in a region of high LD in the genome the given SNP from the GWAS represents the effects of a group of SNPs in that region.\n",
    "\n",
    "S-LDSC determines that a category of SNPs is enriched for heritability if SNPs with high LD to that category have more significant $\\chi^2$ statistics than SNPs with low LD to that category.\n",
    "\n",
    "Here, enrichment of a category is defined as the proportion of SNP heritability in the category divided by the proportion of SNPs in that category.\n",
    "\n",
    "More precisely, under a polygenic model, the expected $\\chi^2$ statistic of SNP $j$ is\n",
    "\n",
    "$$E[\\chi^2_j] = N\\sum_CT_Cl(j,C) + Na + 1 \\quad (2)$$\n",
    "\n",
    "where $N$ is sample size, C indexes categories, $ℓ(j, C)$ is the LD score of SNP j with respect to category $l(j,C) = \\sum_{k\\epsilon C} r^2_{jk}$, $a$ is a term that measures the contribution of confounding biases, and if the categories are disjoint, $\\tau_C$ is the per-SNP heritability in category $C$; if the categories overlap, then the per-SNP heritability of SNP j is $\\sum_{C:j\\epsilon C} \\tau_C$.  Equation 2 allows us to estimate $\\tau_C$ via a (computationally simple) multiple regression of $\\chi^2$ against $ℓ(j, C)$, for either a quantitative or case-control study. \n",
    "\n",
    "To see how these methods have been applied to real world data as well as a further discussion on methods and comparisons to other methods please read the three papers listed at the top of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Command Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run LDSC.ipynb [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  make_annot\n",
      "  munge_sumstats_no_sign\n",
      "  munge_sumstats_sign\n",
      "  calc_ld_score\n",
      "  calc_enrichment\n",
      "\n",
      "Sections\n",
      "  make_annot:\n",
      "    Workflow Options:\n",
      "      --bed VAL (as str, required)\n",
      "                        path to bed file\n",
      "      --bim VAL (as str, required)\n",
      "                        path to bim file\n",
      "      --annot VAL (as str, required)\n",
      "                        name of output annotation file\n",
      "  munge_sumstats_no_sign: This option is for when the summary statistic file\n",
      "                        does not contain a signed summary statistic (Z or Beta).\n",
      "                        In this case,the program will calculate Z for you based\n",
      "                        on A1 being the risk allele\n",
      "    Workflow Options:\n",
      "      --sumst VAL (as str, required)\n",
      "                        path to summary statistic file\n",
      "      --alleles 'w_hm3.snplist'\n",
      "                        path to Hapmap3 SNPs file, keep all columns (SNP, A1,\n",
      "                        and A2) for the munge_sumstats program\n",
      "      --output VAL (as str, required)\n",
      "                        path to output file\n",
      "  munge_sumstats_sign:  This option is for when the summary statistic file does\n",
      "                        contain a signed summary statistic (Z or Beta)\n",
      "    Workflow Options:\n",
      "      --sumst VAL (as str, required)\n",
      "                        path to summary statistic file\n",
      "      --alleles 'w_hm3.snplist'\n",
      "                        path to Hapmap3 SNPs file, keep all columns (SNP, A1,\n",
      "                        and A2) for the munge_sumstats program\n",
      "      --output VAL (as str, required)\n",
      "                        path to output file\n",
      "  calc_ld_score:        Calculate LD Scores **Make sure to delete SNP,CHR, and\n",
      "                        BP columns from annotation files if they are present\n",
      "                        otherwise this code will not work. Before deleting, if\n",
      "                        these columns are present, make sure that the annotation\n",
      "                        file is sorted.**\n",
      "    Workflow Options:\n",
      "      --bim VAL (as str, required)\n",
      "                        Path to bim file\n",
      "      --annot-file VAL (as str, required)\n",
      "                        Path to annotation File. Make sure to remove the SNP,\n",
      "                        CHR, and BP columns from the annotation file if present\n",
      "                        before running.\n",
      "      --output VAL (as str, required)\n",
      "                        name of output file\n",
      "      --snplist 'w_hm3.snplist'\n",
      "                        path to Hapmap3 SNPs file, remove the A1 and A2 columns\n",
      "                        for the Calculate LD Scores program\n",
      "  calc_enrichment:\n",
      "    Workflow Options:\n",
      "      --sumstats VAL (as str, required)\n",
      "                        Path to Summary statistics File\n",
      "      --ref-ld VAL (as str, required)\n",
      "                        Path to Reference LD Scores Files (Base Annotation +\n",
      "                        Annotation you want to analyze, format like minimal\n",
      "                        working example)\n",
      "      --w-ld VAL (as str, required)\n",
      "                        Path to LD Weight Files (Format like minimal working\n",
      "                        example)\n",
      "      --frq-file VAL (as str, required)\n",
      "                        path to frequency files (Format like minimal working\n",
      "                        example)\n",
      "      --output VAL (as str, required)\n",
      "                        Output name\n"
     ]
    }
   ],
   "source": [
    "!sos run LDSC_Code.ipynb -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[global]\n",
    "# Path to the work directory of the analysis.\n",
    "parameter: cwd = path('output')\n",
    "# A genotype file in PLINK binary format (bed/bam/fam) format, or a list of genotype per chrom\n",
    "# Prefix for the analysis output\n",
    "parameter: annotation_name = str\n",
    "parameter: annotation_file = path()\n",
    "parameter: reference_anno_file = path()\n",
    "parameter: genome_ref_file = path() # with .bed \n",
    "parameter: ldsc_path = path() #ldsc github\n",
    "parameter: chromosome = []\n",
    "parameter: snp_list = path()\n",
    "parameter: ld_wind_cm = 1.0\n",
    "\n",
    "parameter: all_traits_file = path()\n",
    "parameter: brain_traits_file = path()\n",
    "parameter: blood_traits_file = path()\n",
    "# Directory containing GWAS summary statistics\n",
    "parameter: sumstat_dir = path() #/mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/sumstat\n",
    "parameter: target_anno_dir = path()  # Directory containing target annotation files\n",
    "parameter: baseline_ld_dir = path()  # Directory containing baseline LD score files\n",
    "parameter: frqfile_dir = path()  # Directory containing allele frequency files\n",
    "parameter: weights_dir = path()  # Directory containing LD weights\n",
    "\n",
    "# Number of threads\n",
    "parameter: numThreads = 8\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "parameter: walltime = '12h'\n",
    "parameter: mem = '16G'\n",
    "# Container option for software to run the analysis: docker or singularity\n",
    "parameter: container = ''\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "\n",
    "# Use the header of the covariate file to decide the sample size\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "# Process input files\n",
    "\n",
    "def adapt_file_path(file_path, reference_file):\n",
    "    \"\"\"\n",
    "    Adapt a single file path based on its existence and a reference file's path.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The file path to adapt.\n",
    "    - reference_file (str): File path to use as a reference for adaptation.\n",
    "\n",
    "    Returns:\n",
    "    - str: Adapted file path.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If no valid file path is found.\n",
    "    \"\"\"\n",
    "    reference_path = os.path.dirname(reference_file)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.isfile(file_path):\n",
    "        return file_path\n",
    "\n",
    "    # Check file name without path\n",
    "    file_name = os.path.basename(file_path)\n",
    "    if os.path.isfile(file_name):\n",
    "        return file_name\n",
    "\n",
    "    # Check file name in reference file's directory\n",
    "    file_in_ref_dir = os.path.join(reference_path, file_name)\n",
    "    if os.path.isfile(file_in_ref_dir):\n",
    "        return file_in_ref_dir\n",
    "\n",
    "    # Check original file path prefixed with reference file's directory\n",
    "    file_prefixed = os.path.join(reference_path, file_path)\n",
    "    if os.path.isfile(file_prefixed):\n",
    "        return file_prefixed\n",
    "\n",
    "    # If all checks fail, raise an error\n",
    "    raise FileNotFoundError(f\"No valid path found for file: {file_path}\")\n",
    "\n",
    "def adapt_file_path_all(df, column_name, reference_file):\n",
    "    return df[column_name].apply(lambda x: adapt_file_path(x, reference_file))\n",
    "    \n",
    "# Process input files based on file type\n",
    "if str(annotation_file).endswith(\"rds\") and str(reference_anno_file).endswith(\"annot.gz\"):\n",
    "    # Case 1: Direct file paths\n",
    "    input_files = [[annotation_file, reference_anno_file, genome_ref_file]]\n",
    "    if len(chromosome) > 0:\n",
    "        input_chroms = [int(x) for x in chromosome]\n",
    "    else:\n",
    "        input_chroms = [0]\n",
    "else:\n",
    "    # Case 2: Files with #id and #path columns\n",
    "    target_files = pd.read_csv(annotation_file, sep=\"\\t\")\n",
    "    reference_files = pd.read_csv(reference_anno_file, sep=\"\\t\")\n",
    "    genome_ref_files = pd.read_csv(genome_ref_file, sep=\"\\t\")\n",
    "    \n",
    "    # Standardize #id and adapt file paths\n",
    "    target_files[\"#id\"] = [x.replace(\"chr\", \"\") for x in target_files[\"#id\"].astype(str)]\n",
    "    target_files[\"#path\"] = target_files[\"#path\"].apply(lambda x: adapt_file_path(x, annotation_file))\n",
    "    \n",
    "    reference_files[\"#id\"] = [x.replace(\"chr\", \"\") for x in reference_files[\"#id\"].astype(str)]\n",
    "    reference_files[\"#path\"] = reference_files[\"#path\"].apply(lambda x: adapt_file_path(x, reference_anno_file))\n",
    "    \n",
    "    genome_ref_files[\"#id\"] = [x.replace(\"chr\", \"\") for x in genome_ref_files[\"#id\"].astype(str)]\n",
    "    genome_ref_files[\"#path\"] = genome_ref_files[\"#path\"].apply(lambda x: adapt_file_path(x, genome_ref_file))\n",
    "    \n",
    "    # Merge the files based on #id\n",
    "    input_files = target_files.merge(reference_files, on=\"#id\").merge(genome_ref_files, on=\"#id\")\n",
    "    \n",
    "    # Filter by specified chromosomes, if any\n",
    "    if len(chromosome) > 0:\n",
    "        input_files = input_files[input_files['#id'].isin(chromosome)]\n",
    "    \n",
    "    # Extract relevant columns as a list of file paths\n",
    "    input_files = input_files.values.tolist()\n",
    "    input_chroms = [x[0] for x in input_files]  # Chromosome IDs\n",
    "    input_files = [x[1:] for x in input_files]  # File paths (annotation, reference, genome_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python 3 (ipykernel)"
   },
   "source": [
    "## Make Annotation File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[make_annot]\n",
    "\n",
    "# Make Annotated Bed File\n",
    "\n",
    "# path to bed file\n",
    "parameter: bed = str \n",
    "#path to bim file\n",
    "parameter: bim = str\n",
    "#name of output annotation file\n",
    "parameter: annot = str\n",
    "bash: expand = True\n",
    "    make_annot.py --bed-file {bed} --bimfile {bim} --annot-file {annot}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "[make_annotation_files_ldscore]\n",
    "# consider joint tau\n",
    "parameter: chr_column = \"CHR\"  \n",
    "parameter: score_column = 3    # Fixed score column for all annotations\n",
    "parameter: joint_tau = False   # Whether to perform joint tau analysis\n",
    "parameter: target_files = []   # List of target annotation files (used when joint_tau is True)\n",
    "input: input_files, group_by = len(input_files[0]), group_with = \"input_chroms\"\n",
    "output: dict([\n",
    "    ('annot', f'{cwd:a}/{annotation_name}/{annotation_name}.{input_chroms[_index]}.annot.gz'),\n",
    "    ('ldscore', f'{cwd:a}/{annotation_name}/{annotation_name}.{input_chroms[_index]}.l2.ldscore.gz')\n",
    "])\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "R: expand= \"${ }\", stderr = f'{_output[\"annot\"]}.stderr', stdout = f'{_output[\"annot\"]}.stdout', container = container, entrypoint = entrypoint\n",
    "    library(data.table)\n",
    "    ref_anno <- fread(${_input[1]:ar})\n",
    "    ref_anno <- ref_anno[,-5]  # Remove the last column\n",
    "    \n",
    "    chr_value = unique(ref_anno$CHR)\n",
    "    \n",
    "    if(${joint_tau}) {\n",
    "        # For joint tau analysis\n",
    "        joint_anno <- matrix(0, nrow=nrow(ref_anno), ncol=length(${target_files}))\n",
    "        colnames(joint_anno) <- paste0(\"anno\", 1:length(${target_files}))\n",
    "        \n",
    "        for(i in 1:length(${target_files})) {\n",
    "            target_anno <- readRDS(${target_files}[i])\n",
    "            pos <- which(target_anno$chr_num == chr_value)\n",
    "            pp <- match(target_anno$pos, ref_anno$BP)\n",
    "            pp1 <- as.numeric(na.omit(pp))\n",
    "            joint_anno[pp1,i] <- target_anno[[${score_column}]][!is.na(pp)]\n",
    "        }\n",
    "        \n",
    "        result_anno <- cbind(ref_anno, as.data.frame(joint_anno))\n",
    "    } else {\n",
    "        # Single annotation analysis\n",
    "        target_anno <- readRDS(${_input[0]:ar})\n",
    "        anno_scores <- rep(0, nrow(ref_anno))\n",
    "        pos <- which(target_anno$chr_num == chr_value)\n",
    "        pp <- match(target_anno$pos, ref_anno$BP)\n",
    "        pp1 <- as.numeric(na.omit(pp))\n",
    "        anno_scores[pp1] <- target_anno[[${score_column}]][!is.na(pp)]\n",
    "        result_anno <- ref_anno\n",
    "        result_anno$ANNOT <- anno_scores\n",
    "    }\n",
    "    \n",
    "    fwrite(result_anno, ${_output[\"annot\"]:nr}, \n",
    "           quote=FALSE, col.names=TRUE, row.names=FALSE, sep=\"\\t\")\n",
    "\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output[\"annot\"]:nnn}.stderr', stdout = f'{_output[\"annot\"]:nnn}.stdout', container = container, entrypoint = entrypoint\n",
    "    gzip -f $[_output[\"annot\"]:n]     \n",
    "\n",
    "bash: expand=\"${ }\", stderr = f'{_output[1]}.stderr', stdout = f'{_output[1]}.stdout'\n",
    "    ldsc \\\n",
    "        --print-snps ${snp_list} \\\n",
    "        --ld-wind-cm ${ld_wind_cm} \\\n",
    "        --out ${_output[\"ldscore\"]:nnn} \\\n",
    "        --bfile ${_input[2]:nar} \\\n",
    "        --yes-really \\\n",
    "        --annot ${_output[0]:a} \\\n",
    "        --l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python 3 (ipykernel)"
   },
   "source": [
    "## Munge Summary Statistics (Option 1: No Signed Summary Statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#This option is for when the summary statistic file does not contain a signed summary statistic (Z or Beta). \n",
    "#In this case,the program will calculate Z for you based on A1 being the risk allele\n",
    "[munge_sumstats_no_sign]\n",
    "\n",
    "\n",
    "\n",
    "#path to summary statistic file\n",
    "parameter: sumst = str\n",
    "#path to Hapmap3 SNPs file, keep all columns (SNP, A1, and A2) for the munge_sumstats program\n",
    "parameter: alleles = \"w_hm3.snplist\"\n",
    "#path to output file\n",
    "parameter: output = str\n",
    "\n",
    "bash: expand = True\n",
    "    munge_sumstats.py --sumstats {sumst} --merge-alleles {alleles} --out {output} --a1-inc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Munge Summary Statistics (Option 2: No Signed Summary Statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# This option is for when the summary statistic file does contain a signed summary statistic (Z or Beta)\n",
    "[munge_sumstats_sign]\n",
    "\n",
    "\n",
    "\n",
    "#path to summary statistic file\n",
    "parameter: sumst = str\n",
    "#path to Hapmap3 SNPs file, keep all columns (SNP, A1, and A2) for the munge_sumstats program\n",
    "parameter: alleles = \"w_hm3.snplist\"\n",
    "#path to output file\n",
    "parameter: output = str\n",
    "\n",
    "bash: expand = True\n",
    "    munge_sumstats.py --sumstats {sumst} --merge-alleles {alleles} --out {output}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python 3 (ipykernel)"
   },
   "source": [
    "## Calculate LD Scores\n",
    "\n",
    "**Make sure to delete SNP,CHR, and BP columns from annotation files if they are present otherwise this code will not work. Before deleting, if these columns are present, make sure that the annotation file is sorted.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#Calculate LD Scores\n",
    "#**Make sure to delete SNP,CHR, and BP columns from annotation files if they are present otherwise this code will not work. Before deleting, if these columns are present, make sure that the annotation file is sorted.**\n",
    "[calc_ld_score]\n",
    "\n",
    "#Path to bim file\n",
    "parameter: bim = str\n",
    "#Path to annotation File. Make sure to remove the SNP, CHR, and BP columns from the annotation file if present before running.\n",
    "parameter: annot_file = str\n",
    "#name of output file\n",
    "parameter: output = str\n",
    "#path to Hapmap3 SNPs file, remove the A1 and A2 columns for the Calculate LD Scores program \n",
    "parameter: snplist = \"w_hm3.snplist\"\n",
    "\n",
    "bash: expand = True\n",
    "    ldsc.py --bfile {bim} --l2 --ld-wind-cm 1 --annot {annot_file} --thin-annot --out {output} --print-snps {snplist}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python 3 (ipykernel)"
   },
   "source": [
    "## Calculate Functional Enrichment using Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#Calculate Enrichment Scores for Functional Annotations\n",
    "[calc_enrichment]\n",
    "\n",
    "#Path to Summary statistics File\n",
    "parameter: sumstats = str\n",
    "#Path to Reference LD Scores Files (Base Annotation + Annotation you want to analyze, format like minimal working example)\n",
    "parameter: ref_ld = str\n",
    "#Path to LD Weight Files (Format like minimal working example)\n",
    "parameter: w_ld = str\n",
    "#path to frequency files (Format like minimal working example)\n",
    "parameter: frq_file = str\n",
    "#Output name\n",
    "parameter: output = str\n",
    "\n",
    "bash: expand = True\n",
    "    ldsc.py --h2 {sumstats} --ref-ld-chr {ref_ld} --w-ld-chr {w_ld} --overlap-annot --frqfile-chr {frq_file} --out {output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "[get_heritability]\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "\n",
    "bash: expand = \"${ }\"\n",
    "    while read -r trait; do\n",
    "        ldsc \\\n",
    "            --h2 ${sumstat_dir}/$trait \\\n",
    "            --ref-ld-chr ${target_anno_dir}/${annotation_name}.,${baseline_ld_dir}/baselineLD. \\\n",
    "            --out ${cwd}/$trait \\\n",
    "            --overlap-annot \\\n",
    "            --frqfile-chr ${frqfile_dir}/1000G.EUR.hg38. \\\n",
    "            --w-ld-chr ${weights_dir}/weights.hm3_noMHC. \\\n",
    "            --print-coefficients \\\n",
    "            --print-delete-vals\n",
    "    done < ${all_traits_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "[meta_analysis]\n",
    "parameter: trait_group_paths = []      # List of paths to trait group files\n",
    "parameter: trait_group_names = []      # List of names for each group\n",
    "parameter: annot_cell = str           # Root path for annotation files\n",
    "parameter: results_cell = str         # Root path for results files\n",
    "parameter: annot_name = str           # Annotation name\n",
    "parameter: joint_tau = False          # Whether to use joint tau analysis\n",
    "parameter: annot_index = None         # Index in results (None for auto-detection)\n",
    "parameter: base_index = None          # Number of baseline annotations (None for auto-detection)\n",
    "parameter: base_path = path           # Path to baseline files (needed for joint tau)\n",
    "output: [\n",
    "    f'{cwd}/{step_name}/single_tau_{annot_cell}_{annot_name}.rds' if not joint_tau else f'{cwd}/{step_name}/{annot_cell}_{annot_name}.rds',\n",
    "    f'{cwd}/{step_name}/enrichment_{annot_cell}_{annot_name}.rds' if not joint_tau else None\n",
    "]\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "\n",
    "R: expand = '${ }', stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout', container = container, entrypoint = entrypoint\n",
    "    library(data.table)\n",
    "    library(rmeta)\n",
    "    \n",
    "    # Function for single tau analysis\n",
    "    get_sd_annot = function(cell_path, annot_index = 1, flag = 0) {\n",
    "        if(flag == 0 && file.exists(paste0(cell_path, \"/\", \"sd_annot_\", annot_index, \".rda\"))) {\n",
    "            sd_annot = get(load(paste0(cell_path, \"/\", \"sd_annot_\", annot_index, \".rda\")))\n",
    "            return(sd_annot)\n",
    "        }\n",
    "        \n",
    "        num = 0\n",
    "        den = 0\n",
    "        ll <- list.files(cell_path, pattern = \".annot.gz\")\n",
    "        for(m in 1:length(ll)) {\n",
    "            dat <- data.frame(fread(paste0(cell_path, \"/\", ll[m])))\n",
    "            num = num + (nrow(dat)-1) * var(dat[,4+annot_index])\n",
    "            den = den + (nrow(dat)-1)\n",
    "        }\n",
    "        \n",
    "        estd_sd_annot = sqrt(num/den)\n",
    "        save(estd_sd_annot, file = paste0(cell_path, \"/\", \"sd_annot_\", annot_index, \".rda\"))\n",
    "        return(estd_sd_annot)\n",
    "    }\n",
    "    \n",
    "    # Function for joint tau analysis\n",
    "    get_sd_annot_joint = function(cell_path, annot_index = 1, base_path, flag = 0) {\n",
    "        if(flag == 0) {\n",
    "            sd_annot = rep(0, length(annot_index))\n",
    "            for(i in 1:length(annot_index)) {\n",
    "                if(file.exists(paste0(cell_path, \"/\", \"sd_annot_\", annot_index[i], \".rda\"))) {\n",
    "                    sd_annot[i] = as.numeric(get(load(paste0(cell_path, \"/\", \"sd_annot_\", annot_index[i], \".rda\"))))\n",
    "                } else {\n",
    "                    flag = 1\n",
    "                    break\n",
    "                }\n",
    "            }\n",
    "            if(flag == 0) return(sd_annot)\n",
    "        }\n",
    "        \n",
    "        num = rep(0, length(annot_index))\n",
    "        den = rep(0, length(annot_index))\n",
    "        ll <- list.files(cell_path, pattern = \".annot.gz\")\n",
    "        ordering = c(1, 10:19, 2, 20:22, 3:9)\n",
    "        \n",
    "        for(m in 1:length(ll)) {\n",
    "            dat <- data.frame(fread(paste0(cell_path, \"/\", ll[m])))\n",
    "            base <- data.frame(fread(paste0(base_path, \"/\", \"baselineLD.\", ordering[m], \".annot.gz\")))\n",
    "            pooled_dat <- cbind(dat[,-(1:4)], base[,-(1:4)])\n",
    "            num = num + (nrow(pooled_dat)-1) * apply(pooled_dat[,annot_index], 2, var)\n",
    "            den = den + (nrow(pooled_dat)-1)\n",
    "            rm(pooled_dat)\n",
    "        }\n",
    "        \n",
    "        sd_annot = sqrt(num/den)\n",
    "        for(i in 1:length(annot_index)) {\n",
    "            temp = sd_annot[i]\n",
    "            save(temp, file = paste0(cell_path, \"/\", \"sd_annot_\", annot_index[i], \".rda\"))\n",
    "        }\n",
    "        return(sd_annot)\n",
    "    }\n",
    "    \n",
    "    # Original single tau analysis\n",
    "    run_single_tau_analysis = function(annot_cell, results_cell, annotations, traits,\n",
    "                                     index_in_results=1, base_index = NULL, flag = 1) {\n",
    "        if(is.null(base_index)) base_index = index_in_results\n",
    "        tau_star_table = matrix(0, length(annotations), 3)\n",
    "        \n",
    "        for(annot_id in 1:length(annotations)) {\n",
    "            cell_path = paste0(annot_cell, \"/\", annotations[annot_id])\n",
    "            sd_annot1 = get_sd_annot(cell_path, annot_index=index_in_results, flag = flag)\n",
    "            Mref = 5961159\n",
    "            df = c()\n",
    "            \n",
    "            for(trait_id in 1:length(traits)) {\n",
    "                result.file = paste0(results_cell, \"/\", annotations[annot_id], \"/\", \n",
    "                                   traits[trait_id], \".sumstats.gz.part_delete\")\n",
    "                new_table = read.table(result.file, header=F)\n",
    "                logfile = paste0(results_cell, \"/\", annotations[annot_id], \"/\", \n",
    "                               traits[trait_id], \".sumstats.gz.log\")\n",
    "                log = read.table(logfile, h=F, fill=T)\n",
    "                h2g = as.numeric(as.character(log[which(log$V4==\"h2:\"), 5]))\n",
    "                coef1 = sd_annot1 * Mref/h2g\n",
    "                sc = sapply(1:nrow(new_table), function(i) {\n",
    "                    tau1 = as.numeric(new_table[i, base_index])\n",
    "                    tau1 * coef1\n",
    "                })\n",
    "                mean_sc = mean(sc)\n",
    "                se_sc = sqrt(199^2/200 * var(sc))\n",
    "                df = rbind(df, c(mean_sc, se_sc))\n",
    "            }\n",
    "            \n",
    "            test_tauj = meta.summaries(df[,1], df[,2], method=\"random\")\n",
    "            tau = test_tauj$summary\n",
    "            tau_se = test_tauj$se.summary\n",
    "            z = tau/tau_se\n",
    "            tau_star_table[annot_id, ] = c(tau, tau_se, 2*pnorm(-abs(z)))\n",
    "        }\n",
    "        rownames(tau_star_table) = annotations\n",
    "        return(tau_star_table)\n",
    "    }\n",
    "    \n",
    "    # Original enrichment analysis\n",
    "    run_single_enrichment_analysis = function(annot_cell, results_cell, annotation, traits,\n",
    "                                            index_in_results=1) {\n",
    "        enrich_table = matrix(0, length(index_in_results), 3)\n",
    "        cell_path = paste0(annot_cell, \"/\", annotation)\n",
    "        res = paste0(results_cell, \"/\", annotation, \"/\", traits[1], \".sumstats.gz.results\")\n",
    "        tab2 = read.table(res, header=T)\n",
    "        annot_names = as.character(tab2$Category[index_in_results])\n",
    "        Mref = 5961159\n",
    "        \n",
    "        for(id in 1:length(index_in_results)) {\n",
    "            meta_enr = NULL\n",
    "            meta_enrstat = NULL\n",
    "            \n",
    "            for(trait_id in 1:length(traits)) {\n",
    "                result.file = paste0(results_cell, \"/\", annotation, \"/\", \n",
    "                                   traits[trait_id], \".sumstats.gz.results\")\n",
    "                res = read.table(result.file, header=T)\n",
    "                logfile = paste0(results_cell, \"/\", annotation, \"/\", \n",
    "                               traits[trait_id], \".sumstats.gz.log\")\n",
    "                log = read.table(logfile, h=F, fill=T)\n",
    "                h2g = as.numeric(as.character(log[which(log$V4==\"h2:\"), 5]))\n",
    "                \n",
    "                myenrstat = (h2g/Mref)*((res[index_in_results[id],3]/res[index_in_results[id],2])-\n",
    "                                      (1-res[index_in_results[id],3])/(1-res[index_in_results[id],2]))\n",
    "                myenrstat_z = qnorm(res[index_in_results[id],7]/2)\n",
    "                myenrstat_sd = myenrstat/myenrstat_z\n",
    "                meta_enrstat = rbind(meta_enrstat, c(myenrstat, myenrstat_sd))\n",
    "                meta_enr = rbind(meta_enr, c(res[index_in_results[id],5], \n",
    "                                           res[index_in_results[id],6]))\n",
    "            }\n",
    "            \n",
    "            test_eni1 = meta.summaries(meta_enr[,1], meta_enr[,2], method=\"random\")\n",
    "            test_eni2 = meta.summaries(meta_enrstat[,1], meta_enrstat[,2], method=\"random\")\n",
    "            \n",
    "            enrich_table[id, ] = c(test_eni1$summary, test_eni1$se.summary,\n",
    "                                2*pnorm(-abs(test_eni2$summary/test_eni2$se.summary)))\n",
    "        }\n",
    "        rownames(enrich_table) = annot_names\n",
    "        return(enrich_table)\n",
    "    }\n",
    "    \n",
    "    # Joint tau analysis\n",
    "    run_many_tau_analysis = function(annot_cell, results_cell, base_path, annotation, traits,\n",
    "                                   index_in_results=NULL, base_index = NULL, flag = 1) {\n",
    "        base <- data.frame(fread(paste0(base_path, \"/\", \"baselineLD.\", 22, \".annot.gz\")))\n",
    "        if(is.null(base_index)) base_index = ncol(base) - 4\n",
    "        \n",
    "        cell_path = paste0(annot_cell, \"/\", annotation)\n",
    "        res = paste0(results_cell, \"/\", annotation, \"/\", traits[1], \".sumstats.gz.results\")\n",
    "        tab2 = read.table(res, header=T)\n",
    "        \n",
    "        if(is.null(index_in_results)) index_in_results = 1:(nrow(tab2) - base_index)\n",
    "        tau_star_table = matrix(0, length(index_in_results), 3)\n",
    "        annot_names = as.character(tab2$Category[index_in_results])\n",
    "        \n",
    "        sd_annot = get_sd_annot_joint(cell_path, annot_index=index_in_results, \n",
    "                                     base_path=base_path, flag=flag)\n",
    "        \n",
    "        for(id in 1:length(index_in_results)) {\n",
    "            sd_annot1 = sd_annot[id]\n",
    "            Mref = 5961159\n",
    "            df = c()\n",
    "            \n",
    "            for(trait_id in 1:length(traits)) {\n",
    "                result.file = paste0(results_cell, \"/\", annotation, \"/\", \n",
    "                                   traits[trait_id], \".sumstats.gz.part_delete\")\n",
    "                new_table = read.table(result.file, header=F)\n",
    "                logfile = paste(results_cell, \"/\", annotation, \"/\", \n",
    "                              traits[trait_id], \".sumstats.gz.log\", sep=\"\")\n",
    "                log = read.table(logfile, h=F, fill=T)\n",
    "                h2g = as.numeric(as.character(log[which(log$V4==\"h2:\"), 5]))\n",
    "                \n",
    "                coef1 = sd_annot1 * Mref/h2g\n",
    "                sc = sapply(1:dim(new_table)[1], function(i) {\n",
    "                    tau1 = as.numeric(new_table[i, index_in_results[id]])\n",
    "                    tau1 * coef1\n",
    "                })\n",
    "                \n",
    "                mean_sc = mean(sc)\n",
    "                se_sc = sqrt(199^2/200 * var(sc))\n",
    "                df = rbind(df, c(mean_sc, se_sc))\n",
    "            }\n",
    "            \n",
    "            test_tauj = meta.summaries(df[,1], df[,2], method=\"random\")\n",
    "            tau = test_tauj$summary\n",
    "            tau_se = test_tauj$se.summary\n",
    "            z = tau/tau_se\n",
    "            tau_star_table[id,] = c(tau, tau_se, 2*pnorm(-abs(z)))\n",
    "        }\n",
    "        \n",
    "        rownames(tau_star_table) = annot_names\n",
    "        return(tau_star_table)\n",
    "    }\n",
    "    \n",
    "    # Process groups\n",
    "    group_paths = strsplit(\"${trait_group_paths}\", \" \")[[1]]\n",
    "    group_names = strsplit(\"${trait_group_names}\", \" \")[[1]]\n",
    "    \n",
    "    if(${joint_tau}) {\n",
    "        # Joint tau analysis\n",
    "        results = list()\n",
    "        for(i in 1:length(group_paths)) {\n",
    "            traits = read.table(group_paths[i])[[1]]\n",
    "            traits = sapply(traits, function(x) return(strsplit(x, \".sumstats\")[[1]][1]))\n",
    "            \n",
    "            results[[group_names[i]]] = run_many_tau_analysis(\n",
    "                annot_cell = \"${annot_cell}\",\n",
    "                results_cell = \"${results_cell}\",\n",
    "                base_path = \"${base_path}\",\n",
    "                annotation = \"${annot_name}\",\n",
    "                traits = traits,\n",
    "                index_in_results = ${annot_index},\n",
    "                base_index = ${base_index},\n",
    "                flag = 0\n",
    "            )\n",
    "        }\n",
    "        saveRDS(results, '${_output[0]}', compress='xz')\n",
    "        \n",
    "    } else {\n",
    "        # Single tau and enrichment analysis\n",
    "        tau_results = list()\n",
    "        enrichment_results = list()\n",
    "        \n",
    "        for(i in 1:length(group_paths)) {\n",
    "            traits = read.table(group_paths[i])[[1]]\n",
    "            traits = sapply(traits, function(x) return(strsplit(x, \".sumstats\")[[1]][1]))\n",
    "            \n",
    "            tau_results[[i]] = run_single_tau_analysis(\n",
    "                \"${annot_cell}\", \"${results_cell}\", \n",
    "                \"${annot_name}\", traits, \n",
    "                index_in_results=${annot_index}\n",
    "            )\n",
    "            \n",
    "            enrichment_results[[i]] = run_single_enrichment_analysis(\n",
    "                \"${annot_cell}\", \"${results_cell}\", \n",
    "                \"${annot_name}\", traits,\n",
    "                index_in_results=${annot_index}\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Save single tau results\n",
    "        tau_df = do.call(rbind.data.frame, tau_results)\n",
    "        rownames(tau_df) = group_names\n",
    "        colnames(tau_df) = c(\"Mean\", \"SD\", \"P\")\n",
    "        saveRDS(tau_df, '${_output[0]}', compress='xz')\n",
    "        \n",
    "        # Save enrichment results\n",
    "        enrichment_df = do.call(rbind.data.frame, enrichment_results)\n",
    "        rownames(enrichment_df) = group_names\n",
    "        colnames(enrichment_df) = c(\"Mean\", \"SD\", \"P\")\n",
    "        saveRDS(enrichment_df, '${_output[1]}', compress='xz')\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Markdown",
     "markdown",
     "markdown",
     "",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0
   },
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
