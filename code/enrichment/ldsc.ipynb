{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Stratified LD Score Regression \n",
    "This notebook implements the pipepline of [S-LDSC](https://github.com/bulik/ldsc/wiki) for LD score and functional enrichment analysis. It is written by Anmol Singh (singh.anmol@columbia.edu), with input from Dr. Gao Wang.\n",
    "\n",
    "**FIXME: the initial draft is complete but pending Gao's review and documentation with minimal working example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "The pipeline is developed to integrate GWAS summary statistics data, annotation data, and LD reference panel data to compute functional enrichment for each of the epigenomic annotations that the user provides using the S-LDSC model. We will first start off with an introduction, instructions to set up, and the minimal working examples. Then the workflow code that can be run using SoS on any data will be at the end. \n",
    "\n",
    "## A brief review on Stratified LD score regression\n",
    "\n",
    "Here I briefly review LD Score Regression and what it is used for. For more in depth information on LD Score Regression please read the following three papers:\n",
    "\n",
    "1. \"LD Score regression distinguishes confounding from polygenicity in genome-wide association studies\" by Sullivan et al (2015)\n",
    "\n",
    "2. \"Partitioning heritability by functional annotation using genome-wide association summary statistics\" by Finucane et al (2015)\n",
    "\n",
    "3. \"Linkage disequilibrium–dependent architecture of human complex traits shows action of negative selection\" by Gazal et al (2017)\n",
    "\n",
    "As stated in Sullivan et al 2015, confounding factors and polygenic effects can cause inflated test statistics and other methods cannot distinguish between inflation from confounding bias and a true signal. LD Score Regression (LDSC) is a technique that aims to identify the impact of confounding factors and polygenic effects using information from GWAS summary statistics. \n",
    "\n",
    "This approach involves using regression to mesaure the relationship between Linkage Disequilibrium (LD) scores and test statistics of SNPs from the GWAS summary statistics. Variants in LD with a \"causal\" variant show an elevation in test statistics in association analysis proportional to their LD (measured by $r^2$) with the causal variant within a certain window size (could be 1 cM, 1kB, etc.). In contrast, inflation from confounders such as population stratification that occur purely from genetic drift will not correlate with LD. For a polygenic trait, SNPs with a high LD score will have more significant χ2 statistics on average than SNPs with a low LD score. Thus, if we regress the $\\chi^2$ statistics from GWAS against LD Score, the intercept minus one is an estimator of the mean contribution of confounding bias to the inflation in the test statistics. The regression model is known as LD Score regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### LDSC model\n",
    "\n",
    "Under a polygenic assumption, in which effect sizes for variants are drawn independently from distributions with variance proportional to  $1/(p(1-p))$ where p is the minor allele frequency (MAF), the expected $\\chi^2$ statistic of variant j is:\n",
    "\n",
    "$$E[\\chi^2|l_j] = Nh^2l_j/M + Na + 1 \\quad (1)$$\n",
    "\n",
    "where $N$ is the sample size; $M$ is the number of SNPs, such that $h^2/M$ is the average heritability explained per SNP; $a$ measures the contribution of confounding biases, such as cryptic relatedness and population stratification; and $l_j = \\sum_k r^2_{jk}$ is the LD Score of variant $j$, which measures the amount of genetic variation tagged by $j$. A full derivation of this equation is provided in the Supplementary Note of Sullivan et al (2015). An alternative derivation is provided in Supplementary Note of Zhu and Stephens (2017) AoAS.\n",
    "\n",
    "From this we can see that LD Score regression can be used to compute SNP-based heritability for a phenotype or trait, from GWAS summary statistics and does not require genotype information like other methods such as REML do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Stratified LDSC\n",
    "\n",
    "Heritability is the proportion of phenotypic variation (VP) that is due to variation in genetic values (VG) and thus can tell us how much of the difference in observed phenotypes in a sample is due to difference in genetics in the sample. It can also be extended to analyze partitioned heritability for a phenotype/trait split over categories. \n",
    "\n",
    "For Partitioned Heritability or Stratified LD Score Regression (S-LDSC) more power is added to our analysis by leveraging LD Score information as well as using SNPs that haven't reached Genome Wide Significance to partition heritability for a trait over categories which many other methods do not do. \n",
    "\n",
    "\n",
    "S-LDSC relies on the fact that the $\\chi^2$ association statistic for a given SNP includes the effects of all SNPs tagged by this SNP meaning that in a region of high LD in the genome the given SNP from the GWAS represents the effects of a group of SNPs in that region.\n",
    "\n",
    "S-LDSC determines that a category of SNPs is enriched for heritability if SNPs with high LD to that category have more significant $\\chi^2$ statistics than SNPs with low LD to that category.\n",
    "\n",
    "Here, enrichment of a category is defined as the proportion of SNP heritability in the category divided by the proportion of SNPs in that category.\n",
    "\n",
    "More precisely, under a polygenic model, the expected $\\chi^2$ statistic of SNP $j$ is\n",
    "\n",
    "$$E[\\chi^2_j] = N\\sum_CT_Cl(j,C) + Na + 1 \\quad (2)$$\n",
    "\n",
    "where $N$ is sample size, C indexes categories, $ℓ(j, C)$ is the LD score of SNP j with respect to category $l(j,C) = \\sum_{k\\epsilon C} r^2_{jk}$, $a$ is a term that measures the contribution of confounding biases, and if the categories are disjoint, $\\tau_C$ is the per-SNP heritability in category $C$; if the categories overlap, then the per-SNP heritability of SNP j is $\\sum_{C:j\\epsilon C} \\tau_C$.  Equation 2 allows us to estimate $\\tau_C$ via a (computationally simple) multiple regression of $\\chi^2$ against $ℓ(j, C)$, for either a quantitative or case-control study. \n",
    "\n",
    "To see how these methods have been applied to real world data as well as a further discussion on methods and comparisons to other methods please read the three papers listed at the top of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Command Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run LDSC.ipynb [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  make_annot\n",
      "  munge_sumstats_no_sign\n",
      "  munge_sumstats_sign\n",
      "  calc_ld_score\n",
      "  calc_enrichment\n",
      "\n",
      "Sections\n",
      "  make_annot:\n",
      "    Workflow Options:\n",
      "      --bed VAL (as str, required)\n",
      "                        path to bed file\n",
      "      --bim VAL (as str, required)\n",
      "                        path to bim file\n",
      "      --annot VAL (as str, required)\n",
      "                        name of output annotation file\n",
      "  munge_sumstats_no_sign: This option is for when the summary statistic file\n",
      "                        does not contain a signed summary statistic (Z or Beta).\n",
      "                        In this case,the program will calculate Z for you based\n",
      "                        on A1 being the risk allele\n",
      "    Workflow Options:\n",
      "      --sumst VAL (as str, required)\n",
      "                        path to summary statistic file\n",
      "      --alleles 'w_hm3.snplist'\n",
      "                        path to Hapmap3 SNPs file, keep all columns (SNP, A1,\n",
      "                        and A2) for the munge_sumstats program\n",
      "      --output VAL (as str, required)\n",
      "                        path to output file\n",
      "  munge_sumstats_sign:  This option is for when the summary statistic file does\n",
      "                        contain a signed summary statistic (Z or Beta)\n",
      "    Workflow Options:\n",
      "      --sumst VAL (as str, required)\n",
      "                        path to summary statistic file\n",
      "      --alleles 'w_hm3.snplist'\n",
      "                        path to Hapmap3 SNPs file, keep all columns (SNP, A1,\n",
      "                        and A2) for the munge_sumstats program\n",
      "      --output VAL (as str, required)\n",
      "                        path to output file\n",
      "  calc_ld_score:        Calculate LD Scores **Make sure to delete SNP,CHR, and\n",
      "                        BP columns from annotation files if they are present\n",
      "                        otherwise this code will not work. Before deleting, if\n",
      "                        these columns are present, make sure that the annotation\n",
      "                        file is sorted.**\n",
      "    Workflow Options:\n",
      "      --bim VAL (as str, required)\n",
      "                        Path to bim file\n",
      "      --annot-file VAL (as str, required)\n",
      "                        Path to annotation File. Make sure to remove the SNP,\n",
      "                        CHR, and BP columns from the annotation file if present\n",
      "                        before running.\n",
      "      --output VAL (as str, required)\n",
      "                        name of output file\n",
      "      --snplist 'w_hm3.snplist'\n",
      "                        path to Hapmap3 SNPs file, remove the A1 and A2 columns\n",
      "                        for the Calculate LD Scores program\n",
      "  calc_enrichment:\n",
      "    Workflow Options:\n",
      "      --sumstats VAL (as str, required)\n",
      "                        Path to Summary statistics File\n",
      "      --ref-ld VAL (as str, required)\n",
      "                        Path to Reference LD Scores Files (Base Annotation +\n",
      "                        Annotation you want to analyze, format like minimal\n",
      "                        working example)\n",
      "      --w-ld VAL (as str, required)\n",
      "                        Path to LD Weight Files (Format like minimal working\n",
      "                        example)\n",
      "      --frq-file VAL (as str, required)\n",
      "                        path to frequency files (Format like minimal working\n",
      "                        example)\n",
      "      --output VAL (as str, required)\n",
      "                        Output name\n"
     ]
    }
   ],
   "source": [
    "!sos run LDSC_Code.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MWE: \n",
    "### make_annotation_files_ldscore\n",
    "annotation file can be a txt file with #id, and #path1 #path2 ..., also can be rds files seperate by ',' \n",
    "#### single tau analysis, with one annotation as a input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " sos run /home/al4225/project/quantile_twas/analysis/SLDSC/scripts/sldsc.ipynb make_annotation_files_ldscore \\\n",
    "    --annotation_file /home/al4225/project/quantile_twas/analysis/SLDSC/data/quantile_qtl_annotation/157genes/AC_DeJager_eQTL.shared_heter_qr.txt \\\n",
    "    --reference_anno_file /home/al4225/project/quantile_twas/analysis/SLDSC/data/reference_annotation.txt \\\n",
    "    --genome_ref_file /home/al4225/project/quantile_twas/analysis/SLDSC/data/genome_reference_bfile.txt \\\n",
    "    --snp_list /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/1000G_EUR_Phase3_hg38/list.txt \\\n",
    "    --annotation_name AC_DeJager_eQTL.shared_heter_qr \\\n",
    "    --cwd /home/al4225/project/quantile_twas/analysis/SLDSC/output --chromosome 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  joint tau\n",
    "with more than one annotation as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " sos run /home/al4225/project/quantile_twas/analysis/SLDSC/scripts/sldsc.ipynb make_annotation_files_ldscore \\\n",
    "    --annotation_file /home/al4225/project/quantile_twas/analysis/SLDSC/data/quantile_qtl_annotation/test/AC_DeJager_eQTL.joint_tau_example.txt \\\n",
    "    --reference_anno_file /home/al4225/project/quantile_twas/analysis/SLDSC/data/reference_annotation.txt \\\n",
    "    --genome_ref_file /home/al4225/project/quantile_twas/analysis/SLDSC/data/genome_reference_bfile.txt \\\n",
    "    --snp_list /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/1000G_EUR_Phase3_hg38/list.txt \\\n",
    "    --annotation_name joint_test \\\n",
    "    --cwd /home/al4225/project/quantile_twas/analysis/SLDSC/output --joint_tau --chromosome 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_heritability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sos run /home/al4225/project/quantile_twas/analysis/SLDSC/scripts/sldsc.ipynb get_heritability \\\n",
    "    --target_anno_dir /home/al4225/project/quantile_twas/analysis/SLDSC/output/AC_DeJager_eQTL.unique_qr/ \\\n",
    "    --sumstat_dir /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/sumstat \\\n",
    "    --baseline_ld_dir /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/1000G_EUR_Phase3_hg38/baselineLD_v2.2 \\\n",
    "    --frqfile_dir /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/plink_files \\\n",
    "    --weights_dir /home/al4225/project/quantile_twas/analysis/SLDSC/data/weights \\\n",
    "    --annotation_name AC_DeJager_eQTL.unique_qr \\\n",
    "    --cwd /home/al4225/project/quantile_twas/analysis/SLDSC/output/AC_DeJager_eQTL.unique_qr/heritability \\\n",
    "    --all_traits_file /home/al4225/project/quantile_twas/analysis/SLDSC/data/sumstats_test_all.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processed_stats\n",
    "#### single tau analysis, with one annotation as a input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed stats cwd has to be the same with get_heritability\n",
    "sos run /home/al4225/project/quantile_twas/analysis/SLDSC/scripts/sldsc.ipynb processed_stats \\\n",
    "    --target_anno_dir /home/al4225/project/quantile_twas/analysis/SLDSC/output/AC_DeJager_eQTL.unique_qr/test_anno_joint/AC_DeJager_eQTL.unique_qr \\\n",
    "    --baseline_ld_dir /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/1000G_EUR_Phase3_hg38/baselineLD_v2.2 \\\n",
    "    --annotation_name AC_DeJager_eQTL.unique_qr \\\n",
    "    --cwd /home/al4225/project/quantile_twas/analysis/SLDSC/output/AC_DeJager_eQTL.unique_qr/heritability \\\n",
    "    --trait_group_paths \"/home/al4225/project/quantile_twas/analysis/SLDSC/data/sumstats_test_all.txt /home/al4225/project/quantile_twas/analysis/SLDSC/data/sumstats_test_brain_trait.txt /home/al4225/project/quantile_twas/analysis/SLDSC/data/sumstats_test_blood_trait.txt\" \\\n",
    "    --trait_group_names \"All Brain Blood\" \\\n",
    "    --all_traits_file /home/al4225/project/quantile_twas/analysis/SLDSC/data/sumstats_test_all.txt -s build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  joint tau\n",
    "with more than one annotation as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed stats cwd has to be the same with get_heritability\n",
    "sos run /home/al4225/project/quantile_twas/analysis/SLDSC/scripts/sldsc.ipynb processed_stats \\\n",
    "    --target_anno_dir /home/al4225/project/quantile_twas/analysis/SLDSC/output/joint_test/ \\\n",
    "    --baseline_ld_dir /mnt/vast/hpc/csg/xc2270/colocboost/post/SLDSC/1000G_EUR_Phase3_hg38/baselineLD_v2.2 \\\n",
    "    --annotation_name joint_test \\\n",
    "    --cwd /home/al4225/project/quantile_twas/analysis/SLDSC/output/joint_test/heritability \\\n",
    "    --trait_group_paths \"/home/al4225/project/quantile_twas/analysis/SLDSC/data/sumstats_test_all.txt /home/al4225/project/quantile_twas/analysis/SLDSC/data/sumstats_test_brain_trait.txt /home/al4225/project/quantile_twas/analysis/SLDSC/data/sumstats_test_blood_trait.txt\" \\\n",
    "    --trait_group_names \"All Brain Blood\" \\\n",
    "    --all_traits_file /home/al4225/project/quantile_twas/analysis/SLDSC/data/sumstats_test_all.txt -s build --joint_tau --joint_annotation_names \"AC.eqtl.unique_qr AC.eqtl.shared_heter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[global]\n",
    "# Path to the work directory of the analysis.\n",
    "parameter: cwd = path('output')\n",
    "# Prefix for the analysis output\n",
    "parameter: annotation_name = str\n",
    "parameter: joint_annotation_names = [] #if joint analysis is used, pass annotation names\n",
    "parameter: joint_tau = False\n",
    "parameter: ldsc_path = path() #optional: ldsc github\n",
    "\n",
    "# for make_annotation_files_ldscore workflow:\n",
    "parameter: annotation_file = path()\n",
    "parameter: reference_anno_file = path()\n",
    "parameter: genome_ref_file = path() # with .bed \n",
    "parameter: chromosome = []\n",
    "parameter: snp_list = path()\n",
    "parameter: ld_wind_cm = 1.0\n",
    "\n",
    "# for get_heritability workflow\n",
    "parameter: all_traits_file = path() # txt file, each row contains all GWAS summary statistics name: e.g. CAD_META.filtered.sumstats.gz\n",
    "parameter: sumstat_dir = path() # Directory containing GWAS summary statistics\n",
    "parameter: target_anno_dir = path()  # Directory containing target annotation files: output of ldscore\n",
    "parameter: baseline_ld_dir = path()  # Directory containing baseline LD score files\n",
    "parameter: frqfile_dir = path()  # Directory containing allele frequency files\n",
    "parameter: weights_dir = path()  # Directory containing LD weights\n",
    "\n",
    "# Number of threads\n",
    "parameter: numThreads = 8\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "parameter: walltime = '12h'\n",
    "parameter: mem = '16G'\n",
    "# Container option for software to run the analysis: docker or singularity\n",
    "parameter: container = ''\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python 3 (ipykernel)"
   },
   "source": [
    "## Make Annotation File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "[make_annotation_files_ldscore]\n",
    "parameter: score_column = 3  \n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "# Process input files\n",
    "\n",
    "def adapt_file_path(file_path, reference_file):\n",
    "    \"\"\"\n",
    "    Adapt a single file path based on its existence and a reference file's path.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The file path to adapt.\n",
    "    - reference_file (str): File path to use as a reference for adaptation.\n",
    "\n",
    "    Returns:\n",
    "    - str: Adapted file path.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If no valid file path is found.\n",
    "    \"\"\"\n",
    "    reference_path = os.path.dirname(reference_file)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.isfile(file_path):\n",
    "        return file_path\n",
    "\n",
    "    # Check file name without path\n",
    "    file_name = os.path.basename(file_path)\n",
    "    if os.path.isfile(file_name):\n",
    "        return file_name\n",
    "\n",
    "    # Check file name in reference file's directory\n",
    "    file_in_ref_dir = os.path.join(reference_path, file_name)\n",
    "    if os.path.isfile(file_in_ref_dir):\n",
    "        return file_in_ref_dir\n",
    "\n",
    "    # Check original file path prefixed with reference file's directory\n",
    "    file_prefixed = os.path.join(reference_path, file_path)\n",
    "    if os.path.isfile(file_prefixed):\n",
    "        return file_prefixed\n",
    "\n",
    "    # If all checks fail, raise an error\n",
    "    raise FileNotFoundError(f\"No valid path found for file: {file_path}\")\n",
    "\n",
    "def adapt_file_path_all(df, column_name, reference_file):\n",
    "    return df[column_name].apply(lambda x: adapt_file_path(x, reference_file))\n",
    "    \n",
    "# Process input files based on file type\n",
    "if str(annotation_file).endswith(\"rds\") and str(reference_anno_file).endswith(\"annot.gz\"):\n",
    "    # Case 1: Direct file paths\n",
    "    if joint_tau:\n",
    "        # joint tau: annotation_fills: rds files separete with \",\"\n",
    "        target_files = annotation_file.split(',')\n",
    "        input_files = [[*target_files, reference_anno_file, genome_ref_file]]\n",
    "    else:\n",
    "        # single annotation\n",
    "        input_files = [[annotation_file, reference_anno_file, genome_ref_file]]\n",
    "    \n",
    "    if len(chromosome) > 0:\n",
    "        input_chroms = [int(x) for x in chromosome]\n",
    "    else:\n",
    "        input_chroms = [0]\n",
    "else:\n",
    "    # Case 2: Files with #id and cols starting with #path columns: e.g. #id, #path1, #path2\n",
    "    target_files = pd.read_csv(annotation_file, sep=\"\\t\")\n",
    "    reference_files = pd.read_csv(reference_anno_file, sep=\"\\t\")\n",
    "    genome_ref_files = pd.read_csv(genome_ref_file, sep=\"\\t\")\n",
    "    \n",
    "    # Standardize #id \n",
    "    target_files[\"#id\"] = [x.replace(\"chr\", \"\") for x in target_files[\"#id\"].astype(str)]\n",
    "    reference_files[\"#id\"] = [x.replace(\"chr\", \"\") for x in reference_files[\"#id\"].astype(str)]\n",
    "    genome_ref_files[\"#id\"] = [x.replace(\"chr\", \"\") for x in genome_ref_files[\"#id\"].astype(str)]\n",
    "    \n",
    "    # process our target annotation files with multiple #path columns\n",
    "    path_columns = [col for col in target_files.columns if col.startswith('#path')]\n",
    "    for col in path_columns:\n",
    "        target_files[col] = target_files[col].apply(lambda x: adapt_file_path(x, annotation_file))\n",
    "    \n",
    "    # process reference and genome files\n",
    "    reference_files[\"#path\"] = reference_files[\"#path\"].apply(lambda x: adapt_file_path(x, reference_anno_file))\n",
    "    genome_ref_files[\"#path\"] = genome_ref_files[\"#path\"].apply(lambda x: adapt_file_path(x, genome_ref_file))\n",
    "    \n",
    "    # Merge the files based on #id\n",
    "    input_files = target_files.merge(reference_files, on=\"#id\").merge(genome_ref_files, on=\"#id\")\n",
    "    \n",
    "    # Filter by specified chromosomes, if any\n",
    "    if len(chromosome) > 0:\n",
    "        input_files = input_files[input_files['#id'].isin(chromosome)]\n",
    "    \n",
    "    # Extract relevant columns as a list of file paths\n",
    "    input_files = input_files.values.tolist()\n",
    "    input_chroms = [x[0] for x in input_files]  # Chromosome IDs\n",
    "    \n",
    "    if joint_tau:\n",
    "        # joint tau, keep all path columns\n",
    "        input_files = [[*x[1:len(path_columns)+1], x[-2], x[-1]] for x in input_files]\n",
    "    else:\n",
    "        # single annotation\n",
    "        input_files = [[x[1], x[-2], x[-1]] for x in input_files]\n",
    "#print(input_files)\n",
    "\n",
    "input: input_files, group_by = len(input_files[0]), group_with = \"input_chroms\"\n",
    "output: dict([\n",
    "   ('annot', f'{cwd:a}/{annotation_name}/{annotation_name}.{input_chroms[_index]}.annot.gz'),\n",
    "   ('ldscore', f'{cwd:a}/{annotation_name}/{annotation_name}.{input_chroms[_index]}.l2.ldscore.gz')\n",
    "])\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bnn}'\n",
    "\n",
    "R: expand= \"${ }\", stderr = f'{_output[\"annot\"]}.stderr', stdout = f'{_output[\"annot\"]}.stdout', container = container, entrypoint = entrypoint\n",
    "   library(data.table)\n",
    "   \n",
    "   process_annotation <- function(target_anno, ref_anno, score_column_value) {\n",
    "       target_anno <- as.data.frame(target_anno)\n",
    "       ref_anno <- as.data.frame(ref_anno)\n",
    "       \n",
    "       chr_value = unique(ref_anno$CHR)\n",
    "       pos <- which(target_anno$chr == chr_value)\n",
    "       anno_scores <- rep(0, nrow(ref_anno))\n",
    "       \n",
    "       match_pos <- match(target_anno$pos, ref_anno$BP)\n",
    "       valid_pos <- as.numeric(na.omit(match_pos))\n",
    "       \n",
    "       if (score_column_value <= ncol(target_anno)) {\n",
    "           anno_scores[valid_pos] <- target_anno[[score_column_value]][!is.na(match_pos)]\n",
    "       } else {\n",
    "           anno_scores[valid_pos] <- 1\n",
    "           print(\"Warning: Score column does not exist. Setting scores to 1\")\n",
    "       }\n",
    "       \n",
    "       return(anno_scores)\n",
    "   }\n",
    "\n",
    "   ref_anno <- fread(${_input[-2]:ar})\n",
    "   print(head(ref_anno))\n",
    "   ref_anno <- as.data.frame(ref_anno)\n",
    "   if(\"ANNOT\" %in% colnames(ref_anno)) {\n",
    "       ref_anno <- ref_anno[,-which(colnames(ref_anno) == \"ANNOT\")]\n",
    "   }\n",
    "   result_anno <- ref_anno\n",
    "   \n",
    "   score_column_value <- ${score_column}\n",
    "   print(paste(\"score =\", score_column_value))    \n",
    "   is_joint <- ${\"TRUE\" if joint_tau else \"FALSE\"}\n",
    "   print(paste(\"Is joint analysis:\", is_joint))\n",
    "\n",
    "   if(is_joint) {\n",
    "        target_files = c(${\",\".join(['\"%s\"' % x.absolute() for x in _input[:-2]])})\n",
    "        # Get all inputs except last two (ref and genome)\n",
    "        # Process each annotation file\n",
    "        for(i in seq_along(target_files)) {\n",
    "            target_anno <- readRDS(target_files[i])\n",
    "            print(paste(\"Processing annotation file\", i))\n",
    "            anno_scores <- process_annotation(target_anno, ref_anno, score_column_value)\n",
    "            result_anno[[paste0(\"ANNOT_\", i)]] <- anno_scores\n",
    "       }\n",
    "   } else {\n",
    "       target_anno <- readRDS(${_input[0]:ar})\n",
    "       print(head(target_anno))\n",
    "       anno_scores <- process_annotation(target_anno, ref_anno, score_column_value)\n",
    "       result_anno$ANNOT <- anno_scores\n",
    "   }\n",
    "   \n",
    "   fwrite(result_anno, ${_output[\"annot\"]:nr}, \n",
    "          quote=FALSE, col.names=TRUE, row.names=FALSE, sep=\"\\t\")\n",
    "\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output[\"annot\"]:nnn}.stderr', stdout = f'{_output[\"annot\"]:nnn}.stdout', container = container, entrypoint = entrypoint\n",
    "   gzip -f $[_output[\"annot\"]:n] \n",
    "\n",
    "bash: expand=\"${ }\", stderr = f'{_output[1]}.stderr', stdout = f'{_output[1]}.stdout'   \n",
    "  #/home/al4225/miniconda3/envs/ldsc/bin/python2 ${ldsc_path}/ldsc.py \\\n",
    "  ldsc \\\n",
    "      --print-snps ${snp_list} \\\n",
    "      --ld-wind-cm ${ld_wind_cm} \\\n",
    "      --out ${_output[\"ldscore\"]:nnn} \\\n",
    "      --bfile ${_input[-1]:nar} \\\n",
    "      --yes-really \\\n",
    "      --annot ${_output[0]:a} \\\n",
    "      --l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python 3 (ipykernel)"
   },
   "source": [
    "## Munge Summary Statistics (Option 1: No Signed Summary Statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#This option is for when the summary statistic file does not contain a signed summary statistic (Z or Beta). \n",
    "#In this case,the program will calculate Z for you based on A1 being the risk allele\n",
    "[munge_sumstats_no_sign]\n",
    "\n",
    "\n",
    "\n",
    "#path to summary statistic file\n",
    "parameter: sumst = str\n",
    "#path to Hapmap3 SNPs file, keep all columns (SNP, A1, and A2) for the munge_sumstats program\n",
    "parameter: alleles = \"w_hm3.snplist\"\n",
    "#path to output file\n",
    "parameter: output = str\n",
    "\n",
    "bash: expand = True\n",
    "    munge_sumstats.py --sumstats {sumst} --merge-alleles {alleles} --out {output} --a1-inc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Munge Summary Statistics (Option 2: No Signed Summary Statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# This option is for when the summary statistic file does contain a signed summary statistic (Z or Beta)\n",
    "[munge_sumstats_sign]\n",
    "\n",
    "\n",
    "\n",
    "#path to summary statistic file\n",
    "parameter: sumst = str\n",
    "#path to Hapmap3 SNPs file, keep all columns (SNP, A1, and A2) for the munge_sumstats program\n",
    "parameter: alleles = \"w_hm3.snplist\"\n",
    "#path to output file\n",
    "parameter: output = str\n",
    "\n",
    "bash: expand = True\n",
    "    munge_sumstats.py --sumstats {sumst} --merge-alleles {alleles} --out {output}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python 3 (ipykernel)"
   },
   "source": [
    "## Calculate Functional Enrichment using Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "[get_heritability]\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "\n",
    "bash: expand = \"${ }\"\n",
    "    output_dir=\"${cwd}\"\n",
    "    mkdir -p $output_dir\n",
    "    while read -r trait; do\n",
    "        #/home/al4225/miniconda3/envs/ldsc/bin/python2 ${ldsc_path}/ldsc.py \\\n",
    "        ldsc \\\n",
    "            --h2 ${sumstat_dir}/$trait \\\n",
    "            --ref-ld-chr ${target_anno_dir}/${annotation_name}.,${baseline_ld_dir}/baselineLD. \\\n",
    "            --out ${cwd}/$trait \\\n",
    "            --overlap-annot \\\n",
    "            --frqfile-chr ${frqfile_dir}/1000G.EUR.hg38. \\\n",
    "            --w-ld-chr ${weights_dir}/weights.hm3_noMHC. \\\n",
    "            --print-coefficients \\\n",
    "            --print-delete-vals\n",
    "    done < ${all_traits_file}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "[processed_stats_1]\n",
    "output: f'{cwd:a}/{annotation_name}.joint_tau.initial_processed_stats.rds' if joint_tau else f'{cwd:a}/{annotation_name}.single_tau.initial_processed_stats.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "\n",
    "R: expand= \"${ }\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout', container = container, entrypoint = entrypoint\n",
    "    library(data.table)\n",
    "    print(paste(\"Joint tau analysis:\", ${str(joint_tau).upper()}))\n",
    "    is_joint <- ${\"TRUE\" if joint_tau else \"FALSE\"}\n",
    "    print(paste(\"Is joint analysis:\", is_joint))\n",
    "       \n",
    "    calculate_sd_annot <- function(cell_path, annot_index = 1) {\n",
    "    ll <- list.files(cell_path, pattern = \"\\\\.annot\\\\.gz$\", full.names = TRUE)\n",
    "    num = numeric(length(annot_index))\n",
    "    den = 0\n",
    "    \n",
    "    for(m in ll) {\n",
    "        dat <- fread(m, fill = TRUE)\n",
    "        if(length(annot_index) > 1) {\n",
    "            cols <- paste0(\"ANNOT_\", annot_index) \n",
    "            num <- num + ((nrow(dat)-1) * sapply(dat[, ..cols], var))\n",
    "        } else {\n",
    "            if((4 + annot_index) > ncol(dat)) stop(paste(\"Index out of range:\", m))\n",
    "            num <- num + ((nrow(dat)-1) * var(dat[, 4+annot_index, with=FALSE]))\n",
    "        }\n",
    "        den <- den + (nrow(dat)-1)\n",
    "    }\n",
    "    sqrt(num/den)\n",
    "    }\n",
    "\n",
    "    check_file_exists <- function(file_path) {\n",
    "        if (!file.exists(file_path)) {\n",
    "            warning(paste(\"File does not exist:\", file_path))\n",
    "            return(FALSE)\n",
    "        }\n",
    "        return(TRUE)\n",
    "    }\n",
    "\n",
    "    # Process single trait\n",
    "    process_trait <- function(trait, cwd) {\n",
    "    files <- list(\n",
    "        result = paste0(cwd, \"/\", trait, \".results\"),\n",
    "        log = paste0(cwd, \"/\", trait, \".log\"),\n",
    "        delete = paste0(cwd, \"/\", trait, \".part_delete\")\n",
    "    )\n",
    "    \n",
    "    if(!all(sapply(files, file.exists))) {\n",
    "        warning(paste(\"Missing files for trait:\", trait))\n",
    "        return(NULL)\n",
    "    }\n",
    "\n",
    "    tryCatch({\n",
    "        results <- fread(files$result)\n",
    "        h2g <- as.numeric(gsub(\".*h2: ([0-9.]+).*\", \"\\\\1\", \n",
    "                            grep(\"Total Observed scale h2:\", readLines(files$log), value=TRUE)))\n",
    "        delete_values <- fread(files$delete)\n",
    "        \n",
    "        list(trait = trait, h2g = h2g, results = results, delete_values = delete_values)\n",
    "    }, error = function(e) {\n",
    "        warning(paste(\"Error processing trait:\", trait, \"-\", e$message))\n",
    "        NULL\n",
    "    })\n",
    "    }\n",
    "\n",
    "    # Process tau analysis\n",
    "    process_tau_analysis <- function(trait_data, sd_annots, base_index = NULL, is_joint = FALSE) {\n",
    "    Mref <- 5961159\n",
    "    \n",
    "    if(is_joint) {\n",
    "        indices <- 1:(nrow(trait_data$results) - base_index)\n",
    "        sc_matrices <- matrix(0, nrow=nrow(trait_data$delete_values), ncol=length(indices))\n",
    "        \n",
    "        for(i in seq_along(indices)) {\n",
    "            coef <- sd_annots[i] * Mref / trait_data$h2g\n",
    "            sc_matrices[,i] <- trait_data$delete_values[[indices[i]]] * coef\n",
    "        }\n",
    "        \n",
    "        list(joint_stats = list(\n",
    "            h2g = trait_data$h2g,\n",
    "            sd_annots = sd_annots,\n",
    "            sc_matrices = sc_matrices\n",
    "        ))\n",
    "    } else {\n",
    "        coef <- sd_annots * Mref / trait_data$h2g\n",
    "        sc_matrix <- trait_data$delete_values[[1]] * coef\n",
    "        \n",
    "        list(single_tau = list(\n",
    "            h2g = trait_data$h2g,\n",
    "            sd_annot = sd_annots,\n",
    "            sc_matrix = matrix(as.vector(trait_data$delete_values[[1]] * coef), ncol=1)\n",
    "        ))        \n",
    "    }\n",
    "    }\n",
    "\n",
    "    # Process enrichment\n",
    "    process_enrichment <- function(trait_data) {\n",
    "    Mref <- 5961159\n",
    "    enrichment_p <- as.numeric(trait_data$results[1, \"Enrichment_p\"])\n",
    "    enrich_ratio <- ((trait_data$results[1, \"Prop._h2\"] / trait_data$results[1, \"Prop._SNPs\"]) - \n",
    "                        (1 - trait_data$results[1, \"Prop._h2\"]) / (1 - trait_data$results[1, \"Prop._SNPs\"]))\n",
    "    enrich_term <- trait_data$h2g / Mref * enrich_ratio\n",
    "    \n",
    "    list(\n",
    "        enrichment_summary = data.table(\n",
    "            Enrichment = trait_data$results[1, \"Enrichment\"],\n",
    "            Enrichment_std_error = trait_data$results[1, \"Enrichment_std_error\"],\n",
    "            Prop._h2 = trait_data$results[1, \"Prop._h2\"],\n",
    "            Prop._SNPs = trait_data$results[1, \"Prop._SNPs\"],\n",
    "            Enrichment_p = enrichment_p\n",
    "        ),\n",
    "        meta_enrstat = list(\n",
    "            enrich_stat = as.numeric(enrich_term),\n",
    "            enrich_z = as.numeric(qnorm(enrichment_p / 2)),\n",
    "            enrich_sd = as.numeric(enrich_term / qnorm(enrichment_p / 2))\n",
    "        ),\n",
    "        meta_enr = list(\n",
    "            Enrichment = as.numeric(trait_data$results[1, \"Enrichment\"]),\n",
    "            Enrichment_std_error = as.numeric(trait_data$results[1, \"Enrichment_std_error\"]) \n",
    "        )\n",
    "    )\n",
    "    }\n",
    "\n",
    "    # Main analysis\n",
    "    traits <- scan(\"${all_traits_file}\", what = \"character\")\n",
    "    results <- list()\n",
    "    problematic_traits <- c()\n",
    "\n",
    "    if (is_joint) {\n",
    "    # Get baseline info and indices\n",
    "    base <- fread(paste0(\"${baseline_ld_dir}\", \"/baselineLD.22.annot.gz\"))\n",
    "    base_index <- ncol(base) - 4\n",
    "    \n",
    "    tab2 <- fread(paste0(\"${cwd}\", \"/\", traits[1], \".results\"))\n",
    "    indices <- 1:(nrow(tab2) - base_index)\n",
    "    \n",
    "    sd_annots <- calculate_sd_annot(\"${target_anno_dir}\", indices)\n",
    "    \n",
    "    for (trait in traits) {\n",
    "        if (!is.null(trait_data <- process_trait(trait, \"${cwd}\"))) {\n",
    "            results[[trait]] <- process_tau_analysis(trait_data, sd_annots, base_index, TRUE)\n",
    "        } else {\n",
    "            problematic_traits <- c(problematic_traits, trait)\n",
    "        }\n",
    "    }\n",
    "    } else {\n",
    "    sd_annot <- calculate_sd_annot(\"${target_anno_dir}\")\n",
    "    \n",
    "    for (trait in traits) {\n",
    "        if (!is.null(trait_data <- process_trait(trait, \"${cwd}\"))) {\n",
    "            results[[trait]] <- c(\n",
    "                process_tau_analysis(trait_data, sd_annot, NULL, FALSE),\n",
    "                list(enrichment = process_enrichment(trait_data))\n",
    "            )\n",
    "        } else {\n",
    "            problematic_traits <- c(problematic_traits, trait)\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "\n",
    "    if (length(problematic_traits) > 0) {\n",
    "    warning(\"Problematic traits:\", paste(problematic_traits, collapse=\", \"))\n",
    "    }\n",
    "\n",
    "    saveRDS(results, \"${_output[0]}\", compress = \"xz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "[processed_stats_2]\n",
    "parameter: trait_group_paths = []     \n",
    "parameter: trait_group_names = []\n",
    "input: group_by = \"all\" \n",
    "output: f'{cwd:a}/{step_name}/{\"joint_tau\" if joint_tau else \"single_tau\"}.{annotation_name}.meta_processed_stats.rds'\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "\n",
    "R: expand = '${ }', stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout'\n",
    "  library(data.table)\n",
    "  library(rmeta)\n",
    "\n",
    "  processed_path = ${_input:ar}\n",
    "  results_data <- readRDS(processed_path)\n",
    "  is_joint <- ${str(joint_tau).upper()}\n",
    "  \n",
    "  process_paths <- function(input_str) {\n",
    "      cleaned <- gsub(\"^\\\\[|\\\\]$\", \"\", input_str)\n",
    "      cleaned <- gsub(\"'\", \"\", cleaned)\n",
    "      paths <- strsplit(cleaned, \",\\\\s*|\\\\s+\")[[1]]\n",
    "      paths <- trimws(paths)\n",
    "      return(paths)\n",
    "  }\n",
    "\n",
    "  group_paths <- process_paths(\"${trait_group_paths}\")\n",
    "  group_names <- process_paths(\"${trait_group_names}\")\n",
    "  problematic_traits <- list()\n",
    "  joint_names <- process_paths(\"${joint_annotation_names}\")\n",
    "  if (is_joint) {\n",
    "      joint_tau_results <- list()\n",
    "      \n",
    "      for (i in 1:length(group_paths)) {\n",
    "          traits <- readLines(group_paths[i])\n",
    "          group_stats <- list()\n",
    "          \n",
    "          for (trait in traits) {\n",
    "              if(trait %in% names(results_data) && !is.null(results_data[[trait]]$joint_stats)) {\n",
    "                  group_stats[[trait]] <- results_data[[trait]]$joint_stats\n",
    "              } else {\n",
    "                  warning(paste(\"Invalid data for trait:\", trait))\n",
    "                  problematic_traits[[trait]] <- \"Invalid data\"\n",
    "              }\n",
    "          }\n",
    "          \n",
    "          if(length(group_stats) > 0) {\n",
    "              n_annotations <- ncol(group_stats[[1]]$sc_matrices)\n",
    "              meta_result <- matrix(0, nrow = n_annotations, ncol = 3)\n",
    "              colnames(meta_result) <- c(\"Mean\", \"SD\", \"P\")\n",
    "              if(length(joint_names) > 0) {\n",
    "                  rownames(meta_result) <- joint_names  # Add row names\n",
    "              }              \n",
    "              for(j in 1:n_annotations) {\n",
    "                  df <- matrix(0, nrow = length(group_stats), ncol = 2)\n",
    "                  for(k in 1:length(group_stats)) {\n",
    "                      sc_values <- group_stats[[k]]$sc_matrices[,j]\n",
    "                      df[k,] <- c(mean(sc_values), sqrt(199^2/200 * var(sc_values)))\n",
    "                  }\n",
    "                  \n",
    "                  meta <- meta.summaries(df[,1], df[,2], method = \"random\")\n",
    "                  meta_result[j,] <- c(\n",
    "                      meta$summary, \n",
    "                      meta$se.summary,\n",
    "                      2 * pnorm(-abs(meta$summary/meta$se.summary))\n",
    "                  )\n",
    "              }\n",
    "              joint_tau_results[[group_names[i]]] <- meta_result\n",
    "          }\n",
    "      }\n",
    "      saveRDS(joint_tau_results, '${_output[0]}')\n",
    "      \n",
    "  } else {\n",
    "      tau_results <- list()\n",
    "      enrichment_results <- list()\n",
    "      \n",
    "      for (i in 1:length(group_paths)) {\n",
    "          traits <- readLines(group_paths[i])\n",
    "          \n",
    "          group_stats <- Filter(function(x) {\n",
    "              !is.null(x) && !is.null(x$single_tau$sc_matrix)\n",
    "          }, results_data[traits])\n",
    "          \n",
    "          if (length(group_stats) > 0) {\n",
    "              sc_data <- lapply(group_stats, function(x) {\n",
    "                  mean_sc <- mean(x$single_tau$sc_matrix[, 1])\n",
    "                  se_sc <- sqrt(199^2 / 200 * var(x$single_tau$sc_matrix[, 1]))\n",
    "                  return(c(mean_sc, se_sc))\n",
    "              })\n",
    "              sc_matrix <- do.call(rbind, sc_data)\n",
    "              \n",
    "              meta_tau <- meta.summaries(sc_matrix[, 1], sc_matrix[, 2], method = \"random\")\n",
    "              tau_results[[group_names[i]]] <- c(meta_tau$summary, \n",
    "                                              meta_tau$se.summary,\n",
    "                                              2 * pnorm(-abs(meta_tau$summary / meta_tau$se.summary)))\n",
    "              \n",
    "              enr_data <- lapply(group_stats, function(x) {\n",
    "                  return(c(x$enrichment$meta_enr$Enrichment,\n",
    "                          x$enrichment$meta_enr$Enrichment_std_error))\n",
    "              })\n",
    "              enr_matrix <- do.call(rbind, enr_data)\n",
    "              \n",
    "              meta_enr <- meta.summaries(enr_matrix[, 1], enr_matrix[, 2], method = \"random\")\n",
    "              enrichment_results[[group_names[i]]] <- c(meta_enr$summary,\n",
    "                                                      meta_enr$se.summary,\n",
    "                                                      2 * pnorm(-abs(meta_enr$summary / meta_enr$se.summary)))\n",
    "          } else {\n",
    "              warning(paste(\"No valid data for group:\", group_names[i]))\n",
    "          }\n",
    "      }\n",
    "      \n",
    "      tau_df <- do.call(rbind, tau_results)\n",
    "      enrichment_df <- do.call(rbind, enrichment_results)\n",
    "      \n",
    "      colnames(tau_df) <- c(\"Mean\", \"SD\", \"P\")\n",
    "      colnames(enrichment_df) <- c(\"Mean\", \"SD\", \"P\")\n",
    "      rownames(tau_df) <- group_names\n",
    "      rownames(enrichment_df) <- group_names\n",
    "      \n",
    "      results <- list(tau = tau_df, enrichment = enrichment_df)\n",
    "      saveRDS(results, '${_output[0]}')\n",
    "  }\n",
    "\n",
    "  if (length(problematic_traits) > 0) {\n",
    "      warning(\"The following traits had issues:\")\n",
    "      print(problematic_traits)\n",
    "  }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Markdown",
     "markdown",
     "markdown",
     "",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0
   },
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
