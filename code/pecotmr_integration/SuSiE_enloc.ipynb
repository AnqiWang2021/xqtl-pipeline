{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "forbidden-ocean",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# GWAS integration: enrichment and colocalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-salon",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "This workflow processes fine-mapping results for xQTL, generated by `susie_twas` in the `mnm_regression.ipynb` notebook for cis xQTL, and GWAS fine-mapping results produced by `susie_rss` in the `rss_analysis.ipynb` notebook. It is designed to perform enrichment and colocalization analysis, particularly when fine-mapping results originate from different regions in the case of cis-xQTL and GWAS. The pipeline is capable to integrate and analyze data across these distinct regions. Originally tailored for cis-xQTL and GWAS integration, this pipeline can be applied to other pairwise integrations. An example of such application is in trans analysis, where the fine-mapped regions might be identical between trans-xQTL and GWAS, representing a special case of this broader implementation.\n",
    "\n",
    "## Input\n",
    "\n",
    "Lists of SuSiE fine-mapping output objects, in RDS format, of `class(susie)` in R. \n",
    "- For xQTL the list is meta-data of format: `chr`, `start`, `end`, `original_data`, `conditions_top_loci`, `block_top_loci` where `original_data` is an RDS file, `conditions_top_loci` is showing which contexts have top loci table (potential signals) `block_top_loci` is the blocks have overlapped top loci variant with xQTL data. That file could be output from `fine_mapping_post_processing/overlap_qtl_gwas`.\n",
    "- For GWAS the list is meta-data of format: `chr`, `start`, `end`, `original_data`, `block_top_loci...` where `original_data` is an RDS file. That file could be output from `fine_mapping_post_processing/gwas_results_export`.\n",
    "- Context meta is a metafile that shows the cohorts and the contained contexts. It can be used to identify the corresponding raw data for each context. \n",
    "\n",
    "## Analytical Logic \n",
    "1. Identify contexts within the xQTL meta-data that contain top loci results for each gene (condition_top_loci). Retrieve their corresponding xQTL original files (original_data) by referencing back to the context meta data. **Also, determine the GWAS blocks containing overlapping top loci variants for each gene (block_top_loci).**\n",
    "2. Use the GWAS list to locate the original file names (original_data) for the identified block candidates.\n",
    "3. Execute `xqtl_gwas_enrichment` to combine all QTL and GWAS original susie rds files for each context, enabling enrichment analysis and generating priors for subsequent coloc analysis. **Or**\n",
    "4. Utilize `susie_coloc` to conduct coloc analysis for the selected xQTL and GWAS original files, analyzing each gene under each condition. This procedure automatically initiates enrichment analysis.\n",
    "\n",
    "## Output\n",
    "\n",
    "1. Enrichment analysis results --- this is a global enrichment estimate that combines all input data for each context. \n",
    "2. Colocalization results for regions of interest --- if `ld_meta_file_path` is provided, would also report coloc cs by coloc postprocessing. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b8d7a2-cfde-4221-9431-0ced7eab6957",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Example\n",
    "enrichment\n",
    "```\n",
    "sos run ~/codes/xqtl-pipeline/pipeline/SuSiE_enloc.ipynb xqtl_gwas_enrichment   \\\n",
    "    --gwas_meta_data  /mnt/vast/hpc/csg/rf2872/Work/pecotmr/encoloc_test/gwas.block_results_db.tsv     \\\n",
    "    --xqtl_meta_data  /mnt/vast/hpc/csg/rf2872/Work/Multivariate/gwas/overlap_test/ROSMAP_eQTL.overlapped.gwas.tsv         \\\n",
    "    --xqtl_finemapping_obj preset_variants_result susie_result_trimmed              \\\n",
    "    --xqtl_varname_obj preset_variants_result variant_names             \\\n",
    "    --gwas_finemapping_obj AD_Bellenguez_2022 single_effect_regression susie_result_trimmed          \\\n",
    "    --gwas_varname_obj  AD_Bellenguez_2022 single_effect_regression variant_names         \\\n",
    "    --xqtl_region_obj  region_info   grange   \\\n",
    "    --qtl-path /mnt/vast/hpc/csg/rf2872/Work/Multivariate/susie_2024_new/rds_files         \\\n",
    "    --gwas_path /mnt/vast/hpc/csg/hs3393/RSS_QC/GWAS_finemapping_Feb22_7dataset/SuSiE_RSS \\\n",
    "    --context_meta /mnt/vast/hpc/homes/rf2872/codes/fungen-xqtl-analysis/resource/context_meta.tsv \n",
    "```\n",
    "\n",
    "coloc (would trigger enrichment automatically)\n",
    "```\n",
    " sos run ~/codes/xqtl-pipeline/pipeline/SuSiE_enloc.ipynb susie_coloc    \\\n",
    "    --gwas_meta_data  /mnt/vast/hpc/csg/rf2872/Work/pecotmr/encoloc_test/gwas.block_results_db.tsv         \\\n",
    "    --xqtl_meta_data  /mnt/vast/hpc/csg/rf2872/Work/Multivariate/gwas/overlap_test/ROSMAP_eQTL.overlapped.gwas.tsv      \\\n",
    "    --xqtl_finemapping_obj preset_variants_result susie_result_trimmed              \\\n",
    "    --xqtl_varname_obj preset_variants_result variant_names             \\\n",
    "    --gwas_finemapping_obj AD_Bellenguez_2022 single_effect_regression susie_result_trimmed              \\\n",
    "    --gwas_varname_obj  AD_Bellenguez_2022 single_effect_regression variant_names             \\\n",
    "    --xqtl_region_obj  region_info   grange       \\\n",
    "    --qtl-path /mnt/vast/hpc/csg/rf2872/Work/Multivariate/susie_2024_new/rds_files             \\\n",
    "    --gwas_path /mnt/vast/hpc/csg/hs3393/RSS_QC/GWAS_finemapping_Feb22_7dataset/SuSiE_RSS     \\\n",
    "    --context_meta /mnt/vast/hpc/homes/rf2872/codes/fungen-xqtl-analysis/resource/context_meta.tsv      \\\n",
    "    --ld_meta_file_path /mnt/vast/hpc/csg/data_public/20240120_ADSP_LD_matrix/ld_meta_file.tsv \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4434e2-65d3-4ac0-b249-1fd445b71e77",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "\n",
    "eg: `gwas_meta_data`  (output from `fine_mapping_post_processing/gwas_results_export`)\n",
    "\n",
    "```\n",
    "#chr\tstart\tend\tregion_id\tTSS\toriginal_data\tcombined_data\tcombined_data_sumstats\tconditions\tconditions_top_loci\n",
    "chr1\t101384274\t104443097\t1_101384274-104443097\tNA\t1_101384274-104443097.susie_rss.rds\tgwas.1_101384274-104443097.cis_results_db.export.rds\tgwas.1_101384274-104443097.cis_results_db.export_sumstats.rds\tNA\tNA\n",
    "chr19\t44935906\t46842901\t19_44935906-46842901\tNA\t19_44935906-46842901.susie_rss.rds\tgwas.19_44935906-46842901.cis_results_db.export.rds\tgwas.19_44935906-46842901.cis_results_db.export_sumstats.rds\tAD_Bellenguez_2022,AD_Jansen_2021,AD_Kunkle_Stage1_2019,AD_Wightman_Full_2021,AD_Wightman_Excluding23andMe_2021,AD_Wightman_ExcludingUKBand23andME_2021\tAD_Wightman_ExcludingUKBand23andME_2021.qc_impute,AD_Wightman_ExcludingUKBand23andME_2021.qc_only\tAD_Wightman_ExcludingUKBand23andME_2021.qc_impute\n",
    "```\n",
    "\n",
    "\n",
    "eg: `xqtl_meta_data` (output from `fine_mapping_post_processing/overlap_qtl_gwas`)\n",
    "\n",
    "\n",
    "```\n",
    "#chr\tstart\tend\tregion_id\tTSS\toriginal_data\tcombined_data\tcombined_data_sumstats\tconditions\tconditions_top_loci\tblock_top_loci\tfinal_combined_data\n",
    "chr1\t167600000\t171480000\tENSG00000000457\t169894266\tMSBB_eQTL.ENSG00000000457.univariate_susie_twas_weights.rds, ROSMAP_Kellis_eQTL.ENSG00000000457.univariate_susie_twas_weights.rds\tFungen_xQTL.ENSG00000000457.cis_results_db.export.rds\tFungen_xQTL.ENSG00000000457.cis_results_db.export_sumstats.rds\tBM_10_MSBB_eQTL,BM_22_MSBB_eQTL,BM_36_MSBB_eQTL,BM_44_MSBB_eQTL,Ast_Kellis_eQTL\tBM_22_MSBB_eQTL,BM_44_MSBB_eQTL\t\t\n",
    "chr19\t41840000\t47960000\tENSG00000130203\t44905790\tROSMAP_Kellis_eQTL.ENSG00000130203.univariate_susie_twas_weights.rds, ROSMAP_Bennett_Klein_pQTL.ENSG00000130203.univariate_susie_twas_weights.rds\tFungen_xQTL.ENSG00000130203.cis_results_db.export.rds\tFungen_xQTL.ENSG00000130203.cis_results_db.export_sumstats.rds\tMic_Kellis_eQTL,DLPFC_Bennett_pQTL\tMic_Kellis_eQTL\t19_44935906-46842901\tFungen_xQTL.ENSG00000130203.cis_results_db.export.overlapped.gwas.rds\n",
    "chr20\t49934867\t53560000\tENSG00000000419\t50958554\tMSBB_eQTL.ENSG00000000419.univariate_susie_twas_weights.rds\tFungen_xQTL.ENSG00000000419.cis_results_db.export.rds\tFungen_xQTL.ENSG00000000419.cis_results_db.export_sumstats.rds\tBM_10_MSBB_eQTL,BM_22_MSBB_eQTL\tBM_22_MSBB_eQTL\tFungen_xQTL.ENSG00000000419.cis_results_db.export.overlapped.gwas.rds\n",
    "```\n",
    "eg: `context_meta` (should be updated as more analysis is done)\n",
    "```\n",
    "cohort\tcontext\n",
    "KNIGHT_pQTL\tKnight_pQTL_brain,Knight_eQTL_brain\n",
    "MSBB_eQTL\tBM_10_MSBB_eQTL,BM_22_MSBB_eQTL,BM_36_MSBB_eQTL,BM_44_MSBB_eQTL\n",
    "MIGA_eQTL\tMiGA_GTS_eQTL,MiGA_SVZ_eQTL,MiGA_THA_eQTL\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-implement",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Workdir\n",
    "parameter: cwd = path(\"output\")\n",
    "# A list of file paths for fine-mapped GWAS results. \n",
    "parameter: gwas_meta_data = path\n",
    "# A list of file paths for fine-mapped xQTL results. \n",
    "parameter: xqtl_meta_data = path\n",
    "# Optional: if a region list is provide the enrichment analysis will be focused on provided region. \n",
    "# The LAST column of this list will contain the ID of regions to focus on\n",
    "parameter: region_list = path()\n",
    "# Optional: if a region name is provided \n",
    "# the analysis would be focused on the union of provides region list and region names\n",
    "parameter: region_name = []\n",
    "# It is required to input the name of the analysis\n",
    "parameter: name = f\"{xqtl_meta_data:bnn}.{gwas_meta_data:bnn}\"\n",
    "parameter: container = \"\"\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 200\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5m\"\n",
    "# Memory expected: quite large for enrichment analysis but small for xQTL colocalization\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 1\n",
    "#Optional table name in xQTL RDS files to get fine mapping results (eg 'susie_result_trimmed ').\n",
    "parameter: xqtl_finemapping_obj = []\n",
    "#Optional table name in xQTL RDS files to get variable names (eg ' variant_names ').\n",
    "parameter: xqtl_varname_obj = []\n",
    "#Optional table name in GWAS RDS files to get fine mapping results (eg 'AD_Bellenguez_2022 single_effect_regression susie_result_trimmed ').\n",
    "parameter: gwas_finemapping_obj = []\n",
    "#Optional table name in GWAS RDS files to get variable names(eg 'AD_Bellenguez_2022 single_effect_regression variant_names ').\n",
    "parameter: gwas_varname_obj = []\n",
    "#Optional table name in xQTL RDS files to get region info (eg 'susie_result_trimmed region_info grange ').\n",
    "parameter: xqtl_region_obj = []\n",
    "#Optional table name in GWAS RDS files to get region info (eg 'AD_Bellenguez_2022 single_effect_regression region_info grange ').\n",
    "parameter: gwas_region_obj = []\n",
    "#Directory path for GWAS orignal finemapping results \n",
    "parameter: gwas_path = ''\n",
    "#Directory path for xQTL orignal finemapping results \n",
    "parameter: qtl_path = ''\n",
    "# a meta file showing the context and corresponding cohort\n",
    "parameter: context_meta = path()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def adapt_file_path(file_path, reference_file):\n",
    "    \"\"\"\n",
    "    Adapt a single file path based on its existence and a reference file's path.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The file path to adapt.\n",
    "    - reference_file (str): File path to use as a reference for adaptation.\n",
    "\n",
    "    Returns:\n",
    "    - str: Adapted file path.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If no valid file path is found.\n",
    "    \"\"\"\n",
    "    reference_path = os.path.dirname(reference_file)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.isfile(file_path):\n",
    "        return file_path\n",
    "\n",
    "    # Check file name without path\n",
    "    file_name = os.path.basename(file_path)\n",
    "    if os.path.isfile(file_name):\n",
    "        return file_name\n",
    "\n",
    "    # Check file name in reference file's directory\n",
    "    file_in_ref_dir = os.path.join(reference_path, file_name)\n",
    "    if os.path.isfile(file_in_ref_dir):\n",
    "        return file_in_ref_dir\n",
    "\n",
    "    # Check original file path prefixed with reference file's directory\n",
    "    file_prefixed = os.path.join(reference_path, file_path)\n",
    "    if os.path.isfile(file_prefixed):\n",
    "        return file_prefixed\n",
    "\n",
    "    # If all checks fail, raise an error\n",
    "    raise FileNotFoundError(f\"No valid path found for file: {file_path}\")\n",
    "\n",
    "def adapt_file_path_all(df, column_name, reference_file):\n",
    "    return df[column_name].apply(lambda x: adapt_file_path(x, reference_file))\n",
    "\n",
    "def group_by_region(lst, partition):\n",
    "    # from itertools import accumulate\n",
    "    # partition = [len(x) for x in partition]\n",
    "    # Compute the cumulative sums once\n",
    "    # cumsum_vector = list(accumulate(partition))\n",
    "    # Use slicing based on the cumulative sums\n",
    "    # return [lst[(cumsum_vector[i-1] if i > 0 else 0):cumsum_vector[i]] for i in range(len(partition))]\n",
    "    return partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1885fc43-2777-4e2c-9345-e3fa1f1911d3",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[get_analysis_regions: shared = \"regional_data\"]\n",
    "import pandas as pd\n",
    "\n",
    "def make_unique_data(row):\n",
    "    \"\"\"Make the GWAS data unique by removing duplicates.\"\"\"\n",
    "    unique_data = ','.join(set(row.split(',')))\n",
    "    return unique_data\n",
    "\n",
    "# def check_required_columns(df, required_columns):\n",
    "#     \"\"\"Check if the required columns are present in the dataframe.\"\"\"\n",
    "#     missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "#     if missing_columns:\n",
    "#         raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "def map_blocks_to_data(blocks, mapping_dict):\n",
    "    \"\"\"Map blocks to data using a provided mapping dictionary.\"\"\"\n",
    "    mapped_data = ','.join(mapping_dict.get(block.strip(), 'NA') for block in blocks.split(','))\n",
    "    return mapped_data\n",
    "\n",
    "def process_dataframes(xqtl_meta_data, gwas_meta_data):\n",
    "    \"\"\"Process the XQTL and GWAS dataframes.\"\"\"\n",
    "    xqtl_df = pd.read_csv(xqtl_meta_data, sep=\"\\t\")\n",
    "    # filter xQTL data with the ones have overlapped top loci variants with GWAS data\n",
    "    xqtl_df = xqtl_df[xqtl_df['block_top_loci'].notna()]\n",
    "\n",
    "    gwas_df = pd.read_csv(gwas_meta_data, sep=\"\\t\")\n",
    "    gwas_df = gwas_df[gwas_df['region_id'].isin(xqtl_df['block_top_loci'].str.split(',').explode().unique())]\n",
    "\n",
    "    region_to_combined_data = dict(zip(gwas_df['region_id'], gwas_df['original_data']))\n",
    "    # and only consider the overlapped GWAS blocks\n",
    "    xqtl_df['block_data'] = xqtl_df['block_top_loci'].apply(map_blocks_to_data, args=(region_to_combined_data,))\n",
    "\n",
    "    return xqtl_df, gwas_df\n",
    "\n",
    "def generate_meta_dataframe(meta_data_path):\n",
    "    \"\"\"Generate a new long metadata by converting to context:cohort (1:1).\"\"\"\n",
    "    meta = pd.read_csv(meta_data_path, sep='\\t')\n",
    "    new_meta = pd.DataFrame()\n",
    "    contexts = pd.unique(meta['context'].str.split(',', expand=True).stack().str.strip())\n",
    "\n",
    "    for context in contexts:\n",
    "        mask = meta['context'].str.contains(context)\n",
    "        tmp = pd.DataFrame({\n",
    "            'context': [context],\n",
    "            'cohort': [','.join(meta.loc[mask, 'cohort'])]\n",
    "        })\n",
    "        new_meta = pd.concat([new_meta, tmp], ignore_index=True)\n",
    "\n",
    "    return new_meta\n",
    "\n",
    "def generate_condition_based_dataframe(xqtl_df):\n",
    "    \"\"\"Generate a new long table by converting to condition:orginal_data (1:1).\"\"\"\n",
    "    # only consider the QTL contexts have top loci data\n",
    "    conditions = pd.unique(xqtl_df['conditions_top_loci'].str.split(',', expand=True).stack().str.strip())\n",
    "    new_df = pd.DataFrame()\n",
    "\n",
    "    for condition in conditions:\n",
    "        mask = xqtl_df['conditions_top_loci'].str.contains(condition)\n",
    "        tmp = pd.DataFrame({\n",
    "            'condition': [condition],\n",
    "            'QTL_original_data': [','.join(xqtl_df.loc[mask, 'original_data'])],\n",
    "            'GWAS_original_data': [','.join(xqtl_df.loc[mask, 'block_data'])]\n",
    "        })\n",
    "        new_df = pd.concat([new_df, tmp], ignore_index=True)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "def merge_and_filter_dfs(new_df, new_meta):\n",
    "    \"\"\"Merge and filter the dataframes based on condition/context and cohort to pick the corresponding original files.\"\"\"\n",
    "    new_df = new_df.set_index(['condition', 'GWAS_original_data'])['QTL_original_data'].str.split(',', expand=True).stack().reset_index(name='QTL_original_data').drop('level_2', axis=1)\n",
    "    new_df['cohort_prefix'] = new_df['QTL_original_data'].apply(lambda x: x.split('.')[0])\n",
    "\n",
    "    merged_df = pd.merge(new_df, new_meta, left_on='condition', right_on='context')\n",
    "    filtered_df = merged_df[merged_df['cohort_prefix'].str.strip() == merged_df['cohort'].str.strip()]\n",
    "\n",
    "    filtered_df = filtered_df.drop(columns=['cohort_prefix', 'context']).drop_duplicates()\n",
    "    filtered_df['GWAS_original_data'] = filtered_df['GWAS_original_data'].apply(make_unique_data)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "def prepare_final_paths(filtered_df, qtl_path, gwas_path):\n",
    "    \"\"\"Prepare final paths for QTL and GWAS original data.\"\"\"\n",
    "    filtered_df['QTL_original_data'] = filtered_df['QTL_original_data'].apply(\n",
    "        lambda x: ','.join(f\"{qtl_path}/{file_name.strip()}\" for file_name in x.split(','))\n",
    "    )\n",
    "    filtered_df['GWAS_original_data'] = filtered_df['GWAS_original_data'].apply(\n",
    "        lambda x: ','.join(f\"{gwas_path}/{file_name.strip()}\" for file_name in x.split(','))\n",
    "    )\n",
    "    return filtered_df\n",
    "\n",
    "xqtl_df, gwas_df = process_dataframes(xqtl_meta_data, gwas_meta_data)\n",
    "new_meta = generate_meta_dataframe(context_meta)\n",
    "new_df = generate_condition_based_dataframe(xqtl_df)\n",
    "filtered_df = merge_and_filter_dfs(new_df, new_meta)\n",
    "filtered_df = prepare_final_paths(filtered_df, qtl_path, gwas_path)\n",
    "regional_data = {\n",
    "    'data': [row['QTL_original_data'].split(',') for _, row in filtered_df.iterrows()],\n",
    "    'conditions': [(f\"{row['condition']}\", *row['GWAS_original_data'].split(',')) for _, row in filtered_df.iterrows()]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-carnival",
   "metadata": {
    "kernel": "SoS",
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "[xqtl_gwas_enrichment]\n",
    "depends: sos_variable(\"regional_data\")\n",
    "stop_if(len(regional_data['data']) == 0, f'No files left for analysis')\n",
    "\n",
    "meta = regional_data['conditions']\n",
    "input: regional_data[\"data\"], group_by = lambda x: group_by_region(x, regional_data[\"data\"]), group_with = \"meta\"\n",
    "output: f'{cwd:a}/{name}.{_meta[0]}.enrichment.txt'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "  # RDS files for GWAS data\n",
    "  gwas_finemapped_data = c(${paths([x for x in _meta[1:len(_meta)]]):r,})\n",
    "  # RDS files for xQTL data\n",
    "  xqtl_finemapped_data = c(${paths([x for x in _input]):r,})\n",
    "  ##FIXME: for now we are using orginal data as input, but one question is, since there are several contexts, and several orginal files, we don't kow which ones belongs to which\n",
    "  ## if we use exported data, we need to add (preset) alpha and (preset) V in exported data, which would make it big again, and we still need to figure out above question in coloc step\n",
    "  enrich_result = pecotmr::xqtl_enrichment_wrapper(gwas_files = gwas_finemapped_data, xqtl_files = xqtl_finemapped_data, \n",
    "                                              xqtl_finemapping_obj =  c(\"${_meta[0]}\",${\",\".join(['\"%s\"' % x  for x in xqtl_finemapping_obj]) if len(xqtl_finemapping_obj) != 0 else \"NULL\"}), \n",
    "                                              xqtl_varname_obj =   c(\"${_meta[0]}\",${\",\".join(['\"%s\"' % x  for x in xqtl_varname_obj]) if len(xqtl_varname_obj) != 0 else \"NULL\"}), \n",
    "                                              gwas_finemapping_obj =  c(${\",\".join(['\"%s\"' % x for x in gwas_finemapping_obj]) if len(gwas_finemapping_obj) != 0 else \"NULL\"}), \n",
    "                                              gwas_varname_obj =  c(${\",\".join(['\"%s\"' % x for x in gwas_varname_obj]) if len(gwas_varname_obj) != 0 else \"NULL\"}))\n",
    "  writeLines(paste(names(enrich_result), unlist(enrich_result), sep = \":\"), ${_output:ar})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-benchmark",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[susie_coloc]\n",
    "depends: sos_variable(\"regional_data\"), sos_step(\"xqtl_gwas_enrichment\")\n",
    "stop_if(len(regional_data['data']) == 0, f'No files left for analysis')\n",
    "\n",
    "parameter: ld_meta_file_path = path()\n",
    "meta = regional_data['conditions']\n",
    "input: regional_data[\"data\"], group_by = lambda x: group_by_region(x, regional_data[\"data\"]), group_with = \"meta\"\n",
    "# output: f'{cwd:a}/{step_name[:-2]}/{name}.{_meta[0]}.coloc.rds'\n",
    "output: f'{cwd:a}/{step_name}/{name}.{_meta[0]}.coloc.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(tidyverse)\n",
    "    library(pecotmr)\n",
    "    library(coloc)\n",
    "    pkgs <- list.files(\"/mnt/vast/hpc/homes/rf2872/codes/pecotmr/R\", full.names = TRUE)\n",
    "    for(i in pkgs){\n",
    "        source(i)\n",
    "    }   \n",
    "    # RDS files for xQTL data\n",
    "    xqtl_finemapped_datas = c(${paths([x for x in _input]):r,})\n",
    "    coloc_res <- list()\n",
    "    # are we doing something like this? that means results are by each context, and each one has \n",
    "    for(xqtl_finemapped_data in xqtl_finemapped_datas){\n",
    "      qtl_dat <- readRDS(xqtl_finemapped_data)\n",
    "      gene = names(qtl_dat)\n",
    "      gene_region = pecotmr:::get_nested_element(qtl_dat[[1]],  c(\"${_meta[0]}\",\"region_info\", \"grange\"))\n",
    "  \n",
    "      # Step 1: find relevant GWAS regions that overlap each the xQTL region of interest\n",
    "      gwas_finemapped_datas <- c(${ \",\".join(set('\"%s\"' % path for sublist in [','.join(x[1:]) for x in regional_data['conditions']] for path in sublist.split(',')))}) \n",
    " \n",
    "      gwas_regions <- gwas_finemapped_datas %>% basename %>% gsub('.susie_rss.rds','',.)\n",
    "      overlap_index <- NULL\n",
    "      for (i in 1:length(gwas_regions)) {\n",
    "         print(i)\n",
    "        region <- gwas_regions[i]\n",
    "        split_region <- unlist(strsplit(region, \"_\"))\n",
    "        block_chrom <- as.numeric(split_region[1])\n",
    "        block_start <- as.numeric(split_region[2] %>% strsplit(., \"-\") %>% unlist %>% .[1])\n",
    "        block_end <- as.numeric(split_region[2] %>% strsplit(., \"-\") %>% unlist %>% .[2])\n",
    "        if (gene_region$chrom == block_chrom && (gene_region$start <= block_end |  gene_region$end >= block_start)) {\n",
    "          overlap_index <- c(overlap_index, i)\n",
    "        }\n",
    "      }\n",
    "  \n",
    "     if (!is.null(overlap_index)) {\n",
    "       gwas_finemapped_data <- gwas_finemapped_datas[overlap_index]\n",
    "  \n",
    "       # Step 2: load enrichment analysis results\n",
    "       # Function to extract the numeric value for a given parameter name\n",
    "       get_coloc_prior <- function(param_name, lines) {\n",
    "         line <- grep(paste0(param_name, \":\"), lines, value = TRUE)\n",
    "         numeric_part <- as.numeric(gsub(paste0(\".*\", param_name, \":\"), \"\", line))\n",
    "         return(numeric_part)\n",
    "       }\n",
    "        # Extract values for p1, p2, and p12\n",
    "       enrich_file = paste0('${cwd:a}','/', '${name}', '.' ,'${_meta[0]}','.enrichment.txt')\n",
    "       p1 <- get_coloc_prior(\"p1\", readLines(enrich_file))\n",
    "       p2 <- get_coloc_prior(\"p2\", readLines(enrich_file))\n",
    "       p12 <- get_coloc_prior(\"p12\", readLines(enrich_file))\n",
    "\n",
    "       message(\"Priors are P1:\", p1, \"; p2: \", p2, \"; p12: \", p12)\n",
    "       # Step 3: Apply colocalization analysis between each condition and GWAS\n",
    "\n",
    "       coloc_res[[gene]] <- coloc_wrapper(xqtl_file = xqtl_finemapped_data, gwas_files = gwas_finemapped_data, \n",
    "                                          xqtl_finemapping_obj =  c(\"${_meta[0]}\",${\",\".join(['\"%s\"' % x  for x in xqtl_finemapping_obj]) if len(xqtl_finemapping_obj) != 0 else \"NULL\"}), \n",
    "                                          xqtl_varname_obj =   c(\"${_meta[0]}\",${\",\".join(['\"%s\"' % x  for x in xqtl_varname_obj]) if len(xqtl_varname_obj) != 0 else \"NULL\"}), \n",
    "                                          gwas_finemapping_obj =  c(${\",\".join(['\"%s\"' % x for x in gwas_finemapping_obj]) if len(gwas_finemapping_obj) != 0 else \"NULL\"}), \n",
    "                                          gwas_varname_obj =  c(${\",\".join(['\"%s\"' % x for x in gwas_varname_obj]) if len(gwas_varname_obj) != 0 else \"NULL\"}),\n",
    "                                          xqtl_region_obj =   c(\"${_meta[0]}\",${\",\".join(['\"%s\"' % x  for x in xqtl_region_obj]) if len(xqtl_region_obj) != 0 else \"NULL\"}), \n",
    "                                          gwas_region_obj =  c(${\",\".join(['\"%s\"' % x for x in gwas_region_obj]) if len(gwas_region_obj) != 0 else \"NULL\"}),\n",
    "                                          p1 = p1, p2 = p2, p12 = p12)\n",
    "\n",
    "        if (${\"TRUE\" if ld_meta_file_path.is_file() else \"FALSE\"}) {\n",
    "          coloc_res[[gene]] <- coloc_post_processor(coloc_res[[gene]], LD_meta_file_path = ${ld_meta_file_path:r}, analysis_region = coloc_res[[gene]]$analysis_region)\n",
    "        }\n",
    "      \n",
    "      } else {\n",
    "        print(\"No overlap found\")\n",
    "        coloc_res <-  \"No overlap found\"\n",
    "      }\n",
    "    }\n",
    "    saveRDS(coloc_res, ${_output:r})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "calysto_bash",
     "Bash",
     "#E6EEFF",
     "shell"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.24.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
