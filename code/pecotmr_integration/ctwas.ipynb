{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "996383b6-1060-4c66-8314-f8b33b1b5f6e",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# cTWAS pipeline\n",
    "This module provide an interface for multigroup cTWAS (causal TWAS) analysis with multi-context twas results across all regions.\n",
    "\n",
    "```\n",
    "Zhao S, Crouse W, Qian S, Luo K, Stephens M, He X. Adjusting for genetic confounders in transcriptome-wide association studies improves discovery of risk genes of complex traits. Nat Genet (2024). https://doi.org/10.1038/s41588-023-01648-9\n",
    "```\n",
    "#### Main Steps\n",
    "1. Harmonize weights, gwas against LD reference panel \n",
    "2. Pre-process input data formats for cTWAS input data format. \n",
    "3. Perform cTWAS analysis with LD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3921b69-c069-4419-8aa0-76a7f0920a3c",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input\n",
    "- **LD meta data**: meta data from LD processing with column names of `#chrom`, `start`, `end`, `path`. \n",
    "- **regions data**: dataframe of meta data for LD block information with column names of `chr`, `start`, `stop`. \n",
    "- **xqtl_region_data**: a dataframe with regions data and file path of the corresponding `refined_twas_weights_data`(*.twas.rds) data from twas pipeline output \n",
    "- **gwas_meta_file**: a dataframe with GWAS summary statistics data file paths by chromosome. \n",
    "\n",
    "## Output\n",
    "- A dataframe of cTWAS fine-mapping results for SNP and genes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1145df9c-f481-453d-89b1-71ec6c41ae65",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Example\n",
    "```\n",
    "sos run ~/githubrepo/xqtl-protocol/code/pecotmr_integration/ctwas.ipynb ctwas \\\n",
    "--cwd /mnt/vast/hpc/csg/cl4215/mrmash/workflow/ \\\n",
    "--xqtl_region_data /mnt/vast/hpc/csg/cl4215/mrmash/workflow/susie_twas/regional_xqtl_data.tsv \\\n",
    "--regions /mnt/vast/hpc/csg/cl4215/mrmash/workflow/twas_mr/pipeline/EUR_LD_blocks_test.bed \\\n",
    "--ld_meta_data /mnt/vast/hpc/csg/data_public/20240409_ADSP_LD_matrix/ld_meta_file.tsv \\\n",
    "--gwas_meta_file /mnt/vast/hpc/csg/cl4215/mrmash/workflow/GWAS/gwas_meta.tsv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d978d4d5-c0c9-470b-b733-ab83028bfafa",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path(\"output/\")\n",
    "parameter: ld_meta_data = path()\n",
    "parameter: gwas_meta_file = path()\n",
    "# region info for the input refined_twas_weights_data\n",
    "parameter: xqtl_region_data = path()\n",
    "# ld region for the input data\n",
    "parameter: regions = path()\n",
    "parameter: name = f\"{xqtl_region_data:bn}\"\n",
    "parameter: container = ''\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "parameter: job_size = 100\n",
    "parameter: walltime = \"3h\"\n",
    "parameter: mem = \"20G\"\n",
    "parameter: numThreads = 1\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "def check_required_columns(df, required_columns):\n",
    "    \"\"\"Check if the required columns are present in the dataframe.\"\"\"\n",
    "    missing_columns = [col for col in required_columns if col not in list(df.columns)]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "required_xqtl_region_data_columns = ['chrom','start','end','file_path']\n",
    "required_ld_columns = ['chr', 'start', 'stop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be63c87f-a6d1-45d8-8af7-e35ee0f5597f",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[ctwas_1]\n",
    "# this step we load & format input data for a ld region\n",
    "regions_df = pd.read_csv(regions, sep=\"\\t\",skipinitialspace=True)\n",
    "regions_df.columns = [col.strip() for col in regions_df.columns] \n",
    "regions_df['chr'] = regions_df['chr'].str.strip()\n",
    "xqtl_region_data = pd.read_csv(xqtl_region_data, sep=\"\\t\")\n",
    "#check for required columns\n",
    "check_required_columns(regions_df, required_ld_columns)\n",
    "check_required_columns(xqtl_region_data, required_xqtl_region_data_columns)\n",
    "\n",
    "# Create a dictionary to map each LD region to its corresponding xQTL file paths\n",
    "region_xqtl_dict = OrderedDict()\n",
    "for _, region in regions_df.iterrows():\n",
    "    region_id = f\"{region['chr']}_{region['start']}_{region['stop']}\"\n",
    "    matching_files = xqtl_region_data[\n",
    "        (xqtl_region_data['chrom'] == region['chr']) & (xqtl_region_data['start'] == region['start']) & \n",
    "        (xqtl_region_data['end'] == region['stop'])]['file_path'].tolist()\n",
    "    region_xqtl_dict[region_id] = matching_files\n",
    "# Generate inputs for the next steps\n",
    "region_files = [file for files in region_xqtl_dict.values() for file in files]\n",
    "region_ids = [region_id for region_id, files in region_xqtl_dict.items() for file in files]\n",
    "region_ids_str = ','.join(f'\"{region_id}\"' for region_id in region_ids)\n",
    "\n",
    "if len(region_files) != len(region_ids):\n",
    "    raise ValueError(\"Mismatch between region_files and region_ids lengths\")\n",
    "\n",
    "input: region_files, paired_with=['region_ids_str']\n",
    "output: f'{cwd:a}/{step_name}/{name}_ctwas.rds', f'{cwd:a}/{step_name}/{name}_weights.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output[0]:n}.stdout\", stderr = f\"{_output[0]:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "\n",
    "    library(IRanges)\n",
    "    library(R6)\n",
    "    library(data.table)\n",
    "    devtools::load_all(\"/home/cl4215/githubrepo/pecotmr/\") #library(pecotmr)\n",
    "    devtools::load_all(\"/home/cl4215/githubrepo/ctwas_multigroup/ctwas/\")#library(ctwas)\n",
    "  \n",
    "    # Step1: Harmonize weights and gwas\n",
    "    twas_weights_data <- lapply(c(${_input:r,}), readRDS)\n",
    "    post_qc_data <- lapply(twas_weights_data, function(twas_data){\n",
    "                      harmonize_twas(twas_data, \"${ld_meta_data}\", \"${gwas_meta_file}\", refined_twas_weights_loader, scale_weights=TRUE)\n",
    "                    })\n",
    "    gwas_studies <- unique(names(find_data(post_qc_data, c(3, \"gwas_qced\"))))\n",
    "    saveRDS(post_qc_data, ${_output[1]:r})\n",
    "  \n",
    "    # Step2: Preprocess weights, LD variants data. \n",
    "    weights <- do.call(c, lapply(post_qc_data, function(twas_data){\n",
    "                  get_ctwas_weights(twas_data, \"${ld_meta_data}\")# reshape weights for all gene-context pairs in the region for cTWAS analysis\n",
    "              }))\n",
    "    weights <- weights[!sapply(weights, is.null)]\n",
    "    \n",
    "    # get region_info and snp_info: LD block meta info and variant - bim file data.\n",
    "    region_of_interest <- region_to_df(c(${region_ids_str}))\n",
    "    meta_data <- get_ctwas_meta_data(\"${ld_meta_data}\", \"${regions}\")\n",
    "    region_info <- meta_data$region_info\n",
    "    LD_info <- meta_data$LD_info\n",
    "        \n",
    "    # load LD variants\n",
    "    bim_file_paths <- unique(do.call(c, lapply(1:nrow(region_of_interest), function(region_row){\n",
    "                          get_regional_ld_meta(\"${ld_meta_data}\", region_of_interest[region_row,,drop=FALSE])$intersections$bim_file_paths})))\n",
    "    snp_info <- lapply(bim_file_paths, function(file){\n",
    "                       bimfile <- read.table(file, header = FALSE, sep=\"\\t\")[, c(1,2,4:8)]# original colnames: \"chrom\", \"variants\", \"GD\", \"pos\", \"A1\", \"A2\", \"variance\", \"allele_freq\", \"n_nomiss\"\n",
    "                       bimfile$V2 <- gsub(\"chr\", \"\", gsub(\"_\", \":\", bimfile$V2))\n",
    "                       colnames(bimfile) <- c(\"chrom\", \"id\", \"pos\", \"alt\", \"ref\", \"variance\", \"allele_freq\") # A1:alt, A2: ref\n",
    "                       return(bimfile)})\n",
    "    names(snp_info)<- do.call(c, lapply(bim_file_paths, function(x) { parts <- strsplit(basename(x), \"[_:/.]\")[[1]][1:3]\n",
    "                            gsub(\"chr\", \"\", paste(parts, collapse=\"_\"))}))\n",
    "    \n",
    "    # Step3: Simple cTWAS with LD for all regions\n",
    "    ctwas_res <- list()\n",
    "    for (study in gwas_studies){\n",
    "      gwas_z <- do.call(rbind, lapply(post_qc_data, function(x) find_data(x, c(2, \"gwas_qced\", study), docall=rbind)))\n",
    "      colnames(gwas_z)[which(colnames(gwas_z)==\"variant_id\")] <- \"id\"\n",
    "      z_snp <- gwas_z[, c(\"id\", \"A1\", \"A2\", \"z\")]\n",
    "      z_snp <- z_snp[!duplicated(z_snp$id), ]\n",
    "      # compute gene_z for with each study z_snp \n",
    "      z_gene <- compute_gene_z(z_snp, weights, ncore = ${numThreads}, logfile = file.path(${_output[0]:dr}, paste0(\"${name}\", \".compute_gene_z.log\")))\n",
    "      weights_update <- weights[names(weights)[!names(weights) %in% z_gene$id[is.na(z_gene$z)]]] # remove gene-context pair with NA result\n",
    "      z_gene  <- z_gene[!is.na(z_gene$z),] \n",
    "      ctwas_res[[study]] <- ctwas_res <- ctwas_sumstats(z_snp,\n",
    "                              weights_update,\n",
    "                              region_info,\n",
    "                              snp_info,\n",
    "                              LD_info,\n",
    "                              z_gene = z_gene,\n",
    "                              thin = 1,\n",
    "                              ncore = ${numThreads},\n",
    "                              outputdir = ${_output[0]:dr},\n",
    "                              outname = \"${name}\",\n",
    "                              logfile = file.path(${_output[0]:dr}, paste0(\"${name}\", \".ctwas_sumstats.log\")),\n",
    "                              verbose = FALSE, \n",
    "                              cor_dir = NULL,\n",
    "                              save_cor = TRUE,\n",
    "                              screen_method=\"nonSNP_PIP\",\n",
    "                              LD_format=\"custom\", \n",
    "                              LD_loader_fun = ctwas_ld_loader)\n",
    "    }\n",
    "\n",
    "    # Step4: save results \n",
    "    saveRDS(ctwas_res, ${_output[0]:r})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "version": "0.24.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
