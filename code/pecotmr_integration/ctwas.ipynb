{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "996383b6-1060-4c66-8314-f8b33b1b5f6e",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# cTWAS pipeline\n",
    "## Main Steps\n",
    "1. Harmonize weights, gwas against LD reference panel \n",
    "2. Pre-process input data formats for cTWAS input data format. \n",
    "3. Perform simple cTWAS analysis without loading LD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3921b69-c069-4419-8aa0-76a7f0920a3c",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input\n",
    "- **LD meta data**: meta data from LD processing with column names of `#chrom`, `start`, `end`, `path`. \n",
    "- **regions data**: dataframe of meta data for LD block information with column names of `chr`, `start`, `stop`. \n",
    "- **xqtl_region_data**: a dataframe with regions data and file path of the corresponding `refined_twas_weights_data`(*.twas.rds) data from twas pipeline output \n",
    "- **gwas_meta_file**: a dataframe with GWAS summary statistics data file paths by chromosome. \n",
    "\n",
    "## Output\n",
    "- A dataframe of cTWAS fine-mapping results for SNP and genes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1145df9c-f481-453d-89b1-71ec6c41ae65",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Example\n",
    "```\n",
    "sos run ~/githubrepo/xqtl-protocol/code/pecotmr_integration/ctwas.ipynb ctwas \\\n",
    "--cwd /mnt/vast/hpc/csg/cl4215/mrmash/workflow/test \\\n",
    "--xqtl_region_data /mnt/vast/hpc/csg/cl4215/mrmash/workflow/susie_twas/regional_xqtl_data.tsv \\\n",
    "--regions /mnt/vast/hpc/csg/cl4215/mrmash/workflow/twas_mr/pipeline/EUR_LD_blocks_CLU.bed \\\n",
    "--ld_meta_data /mnt/vast/hpc/csg/data_public/20240409_ADSP_LD_matrix/ld_meta_file.tsv \\\n",
    "--gwas_meta_file /mnt/vast/hpc/csg/cl4215/mrmash/workflow/GWAS/gwas_meta.tsv \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d978d4d5-c0c9-470b-b733-ab83028bfafa",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path(\"output/\")\n",
    "parameter: ld_meta_data = path()\n",
    "parameter: gwas_meta_file = path()\n",
    "# region info for the input refined_twas_weights_data\n",
    "parameter: xqtl_region_data = path()\n",
    "# ld region for the input data\n",
    "parameter: regions = path()\n",
    "parameter: name = f\"{xqtl_region_data:bn}\"\n",
    "parameter: container = ''\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "parameter: job_size = 100\n",
    "parameter: walltime = \"30m\"\n",
    "parameter: mem = \"20G\"\n",
    "parameter: numThreads = 1\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "def check_required_columns(df, required_columns):\n",
    "    \"\"\"Check if the required columns are present in the dataframe.\"\"\"\n",
    "    missing_columns = [col for col in required_columns if col not in list(df.columns)]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "required_xqtl_region_data_columns = ['chrom','start','end','file_path']\n",
    "required_ld_columns = ['chr', 'start', 'stop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be63c87f-a6d1-45d8-8af7-e35ee0f5597f",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[ctwas_1]\n",
    "# this step we load & format input data for a ld region\n",
    "regions_df = pd.read_csv(regions, sep=\"\\t\",skipinitialspace=True)\n",
    "regions_df.columns = [col.strip() for col in regions_df.columns] \n",
    "regions_df['chr'] = regions_df['chr'].str.strip()\n",
    "xqtl_region_data = pd.read_csv(xqtl_region_data, sep=\"\\t\")\n",
    "#check for required columns\n",
    "check_required_columns(regions_df, required_ld_columns)\n",
    "check_required_columns(xqtl_region_data, required_xqtl_region_data_columns)\n",
    "\n",
    "# Create a dictionary to map each LD region to its corresponding xQTL file paths\n",
    "region_xqtl_dict = OrderedDict()\n",
    "for _, region in regions_df.iterrows():\n",
    "    region_id = f\"{region['chr']}_{region['start']}_{region['stop']}\"\n",
    "    matching_files = xqtl_region_data[\n",
    "        (xqtl_region_data['chrom'] == region['chr']) & (xqtl_region_data['start'] == region['start']) & \n",
    "        (xqtl_region_data['end'] == region['stop'])]['file_path'].tolist()\n",
    "    region_xqtl_dict[region_id] = matching_files\n",
    "# Generate inputs for the next steps\n",
    "region_files = [file for files in region_xqtl_dict.values() for file in files]\n",
    "region_ids = [region_id for region_id, files in region_xqtl_dict.items() for file in files]\n",
    "region_ids_str = ','.join(f'\"{region_id}\"' for region_id in region_ids)\n",
    "\n",
    "if len(region_files) != len(region_ids):\n",
    "    raise ValueError(\"Mismatch between region_files and region_ids lengths\")\n",
    "\n",
    "input: region_files, paired_with=['region_ids_str']\n",
    "output: f'{cwd:a}/{step_name}/{name}_ctwas.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "\n",
    "    library(IRanges)\n",
    "    library(R6)\n",
    "    devtools::load_all(\"/home/cl4215/githubrepo/pecotmr/\") #library(pecotmr)\n",
    "    devtools::load_all(\"/home/cl4215/githubrepo/ctwas_multigroup/ctwas/\")#library(ctwas)\n",
    "  \n",
    "    # Step1: Harmonize weights and gwas\n",
    "    twas_weights_data <- lapply(c(${_input:r,}), readRDS)\n",
    "    post_qc_data <- lapply(twas_weights_data, function(twas_data){\n",
    "                      harmonize_twas(twas_data, \"${ld_meta_data}\", \"${gwas_meta_file}\", refined_twas_weights_loader, scale_weights=TRUE)\n",
    "                    })\n",
    "    gwas_studies <- unique(names(find_data(post_qc_data, c(3, \"gwas_qced\"))))\n",
    "    \n",
    "    # Step2: Preprocess weights, LD variants data. \n",
    "    weights <- do.call(c, lapply(post_qc_data, function(twas_data){\n",
    "                  get_ctwas_weights(twas_data, \"${ld_meta_data}\")# reshape weights for all gene-context pairs in the region for cTWAS analysis\n",
    "              }))\n",
    "    weights <- weights[!sapply(weights, is.null)]\n",
    "    # get region_info and snp_info: LD block meta info and variant - bim file data.\n",
    "    region_of_interest <- region_to_df(c(${region_ids_str}))\n",
    "    region_info <- read.table(\"${regions}\", sep=\"\\t\", header=TRUE) # to get exact LD bim file without over-including neighboring LD's info. \n",
    "    colnames(region_info)[1] <- \"chrom\"\n",
    "    region_info$chrom <- as.integer(gsub(\"chr\",\"\",region_info$chrom))  \n",
    "    region_info$region_id <- paste(region_info$chrom, region_info$start, region_info$stop, sep=\"_\")\n",
    "    \n",
    "    # load LD variants\n",
    "    bim_file_paths <- unique(do.call(c, lapply(1:nrow(region_of_interest), function(region_row){\n",
    "                          get_regional_ld_meta(\"${ld_meta_data}\", region_of_interest[region_row,,drop=FALSE])$intersections$bim_file_paths\n",
    "                      })))\n",
    "    snp_info <- lapply(bim_file_paths, function(file){\n",
    "                       bimfile <- read.table(file, header = FALSE, sep=\"\\t\")[, c(1,2,4:8)]# original colnames: \"chrom\", \"variants\", \"GD\", \"pos\", \"A1\", \"A2\", \"variance\", \"allele_freq\", \"n_nomiss\"\n",
    "                       bimfile$V2 <- gsub(\"chr\", \"\", gsub(\"_\", \":\", bimfile$V2))\n",
    "                       colnames(bimfile) <- c(\"chrom\", \"id\", \"pos\", \"alt\", \"ref\", \"variance\", \"allele_freq\") # A1:alt, A2: ref\n",
    "                       return(bimfile)})\n",
    "    names(snp_info)<- do.call(c, lapply(bim_file_paths, function(x) { parts <- strsplit(basename(x), \"[_:/.]\")[[1]][1:3]\n",
    "                              gsub(\"chr\", \"\", paste(parts, collapse=\"_\"))}))\n",
    "    \n",
    "    # Step3: Simple cTWAS with noLD for all regions\n",
    "    ctwas_res <- list()\n",
    "    for (study in gwas_studies){\n",
    "      gwas_z <- do.call(rbind, lapply(post_qc_data, function(x) find_data(x, c(2, \"gwas_qced\", study), docall=rbind)))\n",
    "      colnames(gwas_z)[which(colnames(gwas_z)==\"variant_id\")] <- \"id\"\n",
    "      z_snp <- gwas_z[, c(\"id\", \"A1\", \"A2\", \"z\")]\n",
    "      z_snp <- z_snp[!duplicated(z_snp$id), ]\n",
    "      ctwas_res[[study]] <- ctwas_sumstats_noLD(z_snp,\n",
    "                                 weights,\n",
    "                                 region_info,\n",
    "                                 snp_info,\n",
    "                                 thin = 1,\n",
    "                                 outputdir = ${_output:dr},\n",
    "                                 niter_prefit = 3,\n",
    "                                 niter = 30,\n",
    "                                 group_prior_var_structure = \"shared_type\",\n",
    "                                 maxSNP = 20000,\n",
    "                                 min_nonSNP_PIP = 0.5,\n",
    "                                 ncore = ${numThreads})\n",
    "    }\n",
    "\n",
    "    # Step4: save results \n",
    "    saveRDS(ctwas_res, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fe0975-0d01-482d-a73e-1a4bdd4b7677",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "version": "0.24.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
