{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# GWAS integration: TWAS and MR\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This module provides software implementations for transcriptome-wide association analysis (TWAS), and performs variant selection for providing sparse signals for cTWAS (causal TWAS) analysis as described in Qian et al (2024+) the multi-group cTWAS method. It will additionally perform Mendelian Randomization using fine-mapping instrumental variables (IV) as described in Zhang et al 2020 for \"causal\" effects estimation and model validation, with the unit of analysis being a single gene-trait pair.\n",
    "\n",
    "This procedure is a continuation of the SuSiE-TWAS workflow --- it assumes that xQTL fine-mapping has been performed and moleuclar traits prediction weights pre-computed (to be used for TWAS). Cross validation for TWAS weights is optional but highly recommended.\n",
    "\n",
    "GWAS data required are GWAS summary statistics and LD matrix for the region of interest.\n",
    "\n",
    "### Step 1: TWAS \n",
    "\n",
    "1. Extract GWAS z-score for region of interest and corresponding LD matrix.\n",
    "2. (Optional) perform allele matching QC for the LD matrix with summary stats.\n",
    "3. Process weights: for a number of methods such as LASSO, Elastic Net and mr.ash we have to take the weights as is for QTL variants overlapping with GWAS variants. For SuSiE weights it can be adjusted to exactly match GWAS variants.\n",
    "4. Perofrm TWAS test for multiple sets of weights. \n",
    "5. For each gene, filter TWAS results by keeping the best model selected by CV. Drop the genes that don't show good evidence of TWAS prediction weights.\n",
    "\n",
    "### Step 2: Variant Selection for Imputable Genes via the Best Prediction Methods\n",
    "1. Determine if the gene is imputable at each condition based on the twas_cv performance by adjusted $r^2$ (>=0.01) and p-values (<0.05).\n",
    "2. The imputable gene-condition pair will go through variant selection step. Maximum 10 variants with top pip selected from either `top_loci` table or SuSiE CS set. \n",
    "3. Harmonize weights against LD reference and udpate SuSiE weight. \n",
    "4. Extract weights by best model for the condition then by the variant names were selected from the previous step\n",
    "\n",
    "### Step 3: cTWAS analysis\n",
    "\n",
    "**FIXME: add more documentation here**\n",
    "\n",
    "### Step 4: MR for candidate genes\n",
    "\n",
    "1. Limit MR only to those showing some evidence of cTWAS significance AND have strong instrumental variable (fine-mapping PIP or CS). \n",
    "2. Use fine-mapped xQTL with GWAS data to perform MR. \n",
    "3. For multiple IV, aggregate individual IV estimates using a fixed-effect meta-analysis procedure.\n",
    "4. Identify and exclude results with severe violations of the exclusion restriction (ER) assumption.\n",
    "\n",
    "## Input\n",
    "\n",
    "### GWAS Data Input Interface (Similar to `susie_rss`)\n",
    "\n",
    "I. **GWAS Summary Statistics Files**\n",
    "- **Input**: Vector of files for one or more GWAS studies.\n",
    "- **Format**: \n",
    "  - Tab-delimited files.\n",
    "  - First 4 columns: `chr`, `pos`, `a0`, `a1`\n",
    "  - Additional columns can be loaded using column mapping file see below  \n",
    "- **Column Mapping files (optional)**:\n",
    "  - Optional YAML file for custom column mapping.\n",
    "  - Required columns: `chr`, `pos`, `a0`, `a1`, either `z` or (`betahat` and `sebetahat`).\n",
    "  - Optional columns: `n`, `var_y` (relevant to fine-mapping).\n",
    "\n",
    "II. **GWAS Summary Statistics Meta-File**: this is optional and helpful when there are lots of GWAS data to process via the same command\n",
    "- **Columns**: `study_id`, chromosome number, path to summary statistics file, optional path to column mapping file.\n",
    "- **Note**: Chromosome number `0` indicates a genome-wide file.\n",
    "\n",
    "eg: `gwas_meta.tsv`\n",
    "\n",
    "```\n",
    "study_id    chrom    file_path                 column_mapping_file\n",
    "study1      1        gwas1.tsv.gz         column_mapping.yml\n",
    "study1      2        gwas2.tsv.gz         column_mapping.yml\n",
    "study2      0        gwas3.tsv.gz         column_mapping.yml\n",
    "```\n",
    "\n",
    "If both summary stats file (I) and meta data file (II) are specified we will take the union of the two.\n",
    "\n",
    "\n",
    "III. **LD Reference Metadata File**\n",
    "- **Format**: Single TSV file.\n",
    "- **Contents**:\n",
    "  - Columns: `chr`, `start`, `end`, path to the LD matrix, genomic build.\n",
    "  - LD matrix path format: comma-separated, first entry is the LD matrix, second is the bim file.\n",
    "- **Documentation**: Refer to [our LD reference preparation document](https://cumc.github.io/xqtl-protocol/code/reference_data/ld_reference_generation.html) for detailed information.\n",
    "\n",
    "### Output of Fine-Mapping & TWAS Pipeline\n",
    "\n",
    "**xQTL Weight Database Metadata File**: \n",
    "- **Essential columns**: `chrom`, `start`, `end`, `region_id`, `original_data`, `conditions`\n",
    "- **Structure of the weight database**: \n",
    "  - RDS format.\n",
    "  - Organized hierarchically: region → condition → weight matrix.\n",
    "  - Each column represents a different method.\n",
    "\n",
    "eg: `xqtl_meta.tsv`\n",
    "\n",
    "```\n",
    "#chr start end region_id TSS original_data combined_data combined_data_sumstats conditions conditions_top_loci\n",
    "chr1 0 6480000 ENSG00000008128 1724356 \"KNIGHT_pQTL.ENSG00000008128.univariate_susie_twas_weights.rds, MiGA_eQTL.ENSG00000008128.univariate_susie_twas_weights.rds, MSBB_eQTL.ENSG00000008128.univariate_susie_twas_weights.rds, ROSMAP_Bennett_Klein_pQTL.ENSG00000008128.univariate_susie_twas_weights.rds, ROSMAP_DeJager_eQTL.ENSG00000008128.univariate_susie_twas_weights.rds, ROSMAP_Kellis_eQTL.ENSG00000008128.univariate_susie_twas_weights.rds, ROSMAP_mega_eQTL.ENSG00000008128.univariate_susie_twas_weights.rds, STARNET_eQTL.ENSG00000008128.univariate_susie_twas_weights.rds\" Fungen_xQTL.ENSG00000008128.cis_results_db.export.rds Fungen_xQTL.ENSG00000008128.cis_results_db.export_sumstats.rds Knight_eQTL_brain,MiGA_GFM_eQTL,MiGA_GTS_eQTL,MiGA_SVZ_eQTL,MiGA_THA_eQTL,BM_10_MSBB_eQTL,BM_22_MSBB_eQTL,BM_36_MSBB_eQTL,BM_44_MSBB_eQTL,monocyte_ROSMAP_eQTL,Mic_DeJager_eQTL,Ast_DeJager_eQTL,Oli_DeJager_eQTL,Exc_DeJager_eQTL,Inh_DeJager_eQTL,DLPFC_DeJager_eQTL,PCC_DeJager_eQTL,AC_DeJager_eQTL,Mic_Kellis_eQTL,Ast_Kellis_eQTL,Oli_Kellis_eQTL,OPC_Kellis_eQTL,Exc_Kellis_eQTL,Inh_Kellis_eQTL,Ast_mega_eQTL,Exc_mega_eQTL,Inh_mega_eQTL,Oli_mega_eQTL,STARNET_eQTL_Mac Knight_eQTL_brain,MiGA_GFM_eQTL,MiGA_GTS_eQTL,MiGA_SVZ_eQTL,MiGA_THA_eQTL,BM_10_MSBB_eQTL,BM_22_MSBB_eQTL,BM_36_MSBB_eQTL,BM_44_MSBB_eQTL,monocyte_ROSMAP_eQTL,Mic_DeJager_eQTL,Ast_DeJager_eQTL,Oli_DeJager_eQTL,Exc_DeJager_eQTL,Inh_DeJager_eQTL,DLPFC_DeJager_eQTL,PCC_DeJager_eQTL,AC_DeJager_eQTL,Mic_Kellis_eQTL,Ast_Kellis_eQTL,Oli_Kellis_eQTL,OPC_Kellis_eQTL,Exc_Kellis_eQTL,Inh_Kellis_eQTL,Ast_mega_eQTL,Exc_mega_eQTL,Inh_mega_eQTL,Oli_mega_eQTL,STARNET_eQTL_Mac\n",
    "```\n",
    "\n",
    "This file is automatically generated as part of the FunGen-xQTL protocol, although only the essential columns are relevant to our application here.\n",
    "\n",
    "\n",
    "### TWAS region information\n",
    "\n",
    "This is required for cTWAS analysis, where multiple TWAS and SNP data within each region are combined for joint inference to select the variables, either genes or SNPs, to figure out which variables are likely to be directly associated with the phenotype of interest, rather than being associated through correlations with true causal variables.\n",
    "\n",
    "```\n",
    "chrom    start    end    block_id  \n",
    "1        1000     5000   block1    \n",
    "2        2000     6000   block2\n",
    "3        3000     7000   block3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Output\n",
    "\n",
    "I. A table with the following contents\n",
    "\n",
    "```\n",
    "gwas_study, chrom, block, gene, condition, method, rsq_adj_cv, pval_cv, is_selected_method, twas_z\n",
    "```\n",
    "\n",
    "where\n",
    "\n",
    "- if `twas_z` is `NA` it means the context is not imputable for the method of choice\n",
    "\n",
    "II. a list of  `refined_twas_db` per block, in RDS format, of this structure:\n",
    "\n",
    "```\n",
    "$ region_id\n",
    "    $ context\n",
    "        $ selected_method\n",
    "        $ selected_method_weights\n",
    "        $ selected_top_variants\n",
    "```\n",
    "\n",
    "This will only contain imputatable contexts. It should come with a meta-data file like this:\n",
    "\n",
    "```\n",
    "chrom    start    end    block_id  refined_twas_db\n",
    "1        1000     5000   block1    block1.rds\n",
    "2        2000     6000   block2    block2.rds\n",
    "3        3000     7000   block3    block3.rds\n",
    "```\n",
    "\n",
    "III. cTWAS and MR results\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Example\n",
    "```\n",
    "sos run xqtl-pipeline/code/pecotmr_integration/twas_sparse.ipynb twas \\\n",
    "   --cwd /mnt/vast/hpc/csg/cl4215/mrmash/workflow/twas_mr/pipeline/sparse/ \\\n",
    "   --gwas_meta_data /mnt/vast/hpc/csg/cl4215/mrmash/workflow/twas_ctwas/gwas/gwas_meta.tsv \\\n",
    "   --ld_meta_data /mnt/vast/hpc/csg/data_public/20240409_ADSP_LD_matrix/ld_meta_file.tsv \\\n",
    "   --regions /mnt/vast/hpc/csg/cl4215/mrmash/workflow/twas_mr/pipeline/EUR_blocks.bed \\\n",
    "   --xqtl_meta_data Fungen_xQTL.cis_results_db.exported.tsv \\\n",
    "   --max_var_select 10 --p_value_cutoff 0.05 --rsq_threshold 0.01 --data_type expression -s build\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path(\"output/\")\n",
    "parameter: gwas_name = []\n",
    "parameter: gwas_data = []\n",
    "parameter: gwas_meta_data = path()\n",
    "parameter: column_mapping = []\n",
    "parameter: xqtl_meta_data = path()\n",
    "parameter: ld_meta_data = path()\n",
    "parameter: regions = path()\n",
    "parameter: name = f\"{xqtl_meta_data:bn}.{gwas_meta_data:bn}\"\n",
    "parameter: container = ''\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "parameter: job_size = 100\n",
    "parameter: walltime = \"5m\"\n",
    "parameter: mem = \"8G\"\n",
    "parameter: numThreads = 1\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def adapt_file_path(file_path, reference_file):\n",
    "    \"\"\"\n",
    "    Adapt a single file path based on its existence and a reference file's path.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The file path to adapt.\n",
    "    - reference_file (str): File path to use as a reference for adaptation.\n",
    "\n",
    "    Returns:\n",
    "    - str: Adapted file path.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If no valid file path is found.\n",
    "    \"\"\"\n",
    "    reference_path = os.path.dirname(reference_file)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.isfile(file_path):\n",
    "        return file_path\n",
    "\n",
    "    # Check file name without path\n",
    "    file_name = os.path.basename(file_path)\n",
    "    if os.path.isfile(file_name):\n",
    "        return file_name\n",
    "\n",
    "    # Check file name in reference file's directory\n",
    "    file_in_ref_dir = os.path.join(reference_path, file_name)\n",
    "    if os.path.isfile(file_in_ref_dir):\n",
    "        return file_in_ref_dir\n",
    "\n",
    "    # Check original file path prefixed with reference file's directory\n",
    "    file_prefixed = os.path.join(reference_path, file_path)\n",
    "    if os.path.isfile(file_prefixed):\n",
    "        return file_prefixed\n",
    "\n",
    "    # If all checks fail, raise an error\n",
    "    raise FileNotFoundError(f\"No valid path found for file: {file_path}\")\n",
    "\n",
    "def group_by_region(lst, partition):\n",
    "    # from itertools import accumulate\n",
    "    # partition = [len(x) for x in partition]\n",
    "    # Compute the cumulative sums once\n",
    "    # cumsum_vector = list(accumulate(partition))\n",
    "    # Use slicing based on the cumulative sums\n",
    "    # return [lst[(cumsum_vector[i-1] if i > 0 else 0):cumsum_vector[i]] for i in range(len(partition))]\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "[get_analysis_regions: shared = \"regional_data\"]\n",
    "from collections import OrderedDict\n",
    "\n",
    "def check_required_columns(df, required_columns):\n",
    "    \"\"\"Check if the required columns are present in the dataframe.\"\"\"\n",
    "    missing_columns = [col for col in required_columns if col not in list(df.columns)]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "def extract_regional_data(gwas_meta_data, xqtl_meta_data, gwas_name, gwas_data, column_mapping):\n",
    "    \"\"\"\n",
    "    Extracts data from GWAS and xQTL metadata files and additional GWAS data provided. \n",
    "\n",
    "    Args:\n",
    "    - gwas_meta_data (str): File path to the GWAS metadata file.\n",
    "    - xqtl_meta_data (str): File path to the xQTL weight metadata file.\n",
    "    - gwas_name (list): vector of GWAS study names.\n",
    "    - gwas_data (list): vector of GWAS data.\n",
    "    - column_mapping (list, optional): vector of column mapping files.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of two dictionaries:\n",
    "        - GWAS Dictionary: Maps study IDs to a list containing chromosome number, \n",
    "          GWAS file path, and optional column mapping file path.\n",
    "        - xQTL Dictionary: Nested dictionary with region IDs as keys.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If any specified file path does not exist.\n",
    "    - ValueError: If required columns are missing in the input files or vector lengths mismatch.\n",
    "    \"\"\"\n",
    "    # Check vector lengths\n",
    "    if len(gwas_name) != len(gwas_data):\n",
    "        raise ValueError(\"gwas_name and gwas_data must be of equal length\")\n",
    "    \n",
    "    if len(column_mapping)>0 and len(column_mapping) != len(gwas_name):\n",
    "        raise ValueError(\"If column_mapping is provided, it must be of the same length as gwas_name and gwas_data\")\n",
    "\n",
    "    # Required columns for each file type\n",
    "    required_gwas_columns = ['study_id', 'chrom', 'file_path']\n",
    "    required_xqtl_columns = ['#chr', 'start', 'end', 'gene_id']\n",
    "    \n",
    "    # Reading the GWAS metadata file\n",
    "    gwas_df = pd.read_csv(gwas_meta_data, sep=\"\\t\")\n",
    "    check_required_columns(gwas_df, required_gwas_columns)\n",
    "    gwas_dict = OrderedDict()\n",
    "\n",
    "    # Process additional GWAS data from R vectors\n",
    "    for name, data, mapping in zip(gwas_name, gwas_data, column_mapping or [None]*len(gwas_name)):\n",
    "        gwas_dict[name] = {0: [data, mapping]}\n",
    "\n",
    "    for _, row in gwas_df.iterrows():\n",
    "        file_path = row['file_path']\n",
    "        mapping_file = row.get('column_mapping_file')\n",
    "        \n",
    "        # Adjust paths if necessary\n",
    "        file_path = adapt_file_path(file_path, gwas_meta_data)\n",
    "        if mapping_file:\n",
    "            mapping_file = adapt_file_path(mapping_file,  gwas_meta_data)\n",
    "\n",
    "       # Create or update the entry for the study_id\n",
    "        if row['study_id'] not in gwas_dict:\n",
    "            gwas_dict[row['study_id']] = {}\n",
    "\n",
    "        # Expand chrom 0 to chrom 1-22 or use the specified chrom\n",
    "        chrom_range = range(1, 23) if row['chrom'] == 0 else [row['chrom']]\n",
    "        for chrom in chrom_range:\n",
    "            if chrom in gwas_dict[row['study_id']]:\n",
    "                existing_entry = gwas_dict[row['study_id']][chrom]\n",
    "                raise ValueError(f\"Duplicate chromosome specification for study_id {row['study_id']}, chrom {chrom}. \"\n",
    "                                 f\"Conflicting entries: {existing_entry} and {[file_path, mapping_file]}\")\n",
    "            gwas_dict[row['study_id']][chrom] = [file_path, mapping_file]\n",
    "\n",
    "    # Reading the xQTL weight metadata file\n",
    "    xqtl_df = pd.read_csv(xqtl_meta_data, sep=\"\\t\")\n",
    "    check_required_columns(xqtl_df, required_xqtl_columns)\n",
    "    xqtl_dict = OrderedDict()\n",
    "    for _, row in xqtl_df.iterrows():\n",
    "        #file_paths = [adapt_file_path(fp.strip(), xqtl_meta_data) for fp in row['file_path'].split(',')]  # Splitting and stripping file paths\n",
    "        preliminary_file_paths = [susie_wgt_prefix + gene_id.strip() + susie_wgt_suffix for gene_id in row['gene_id'].split(',')]\n",
    "        xqtl_dict[row['gene_id']] = {\"meta_info\": [int(row['#chr'].lstrip('chr')), row['start'], row['end'], row['gene_id']],\n",
    "                                    \"files\": preliminary_file_paths}\n",
    "    return gwas_dict, xqtl_dict\n",
    "\n",
    "gwas_dict, xqtl_dict = extract_regional_data(gwas_meta_data, xqtl_meta_data, gwas_name, gwas_data, column_mapping)\n",
    "regional_data = dict([(\"GWAS\", gwas_dict), (\"xQTL\", xqtl_dict)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "**FIXME: please add documentation for each paramter in this format, **\n",
    "\n",
    "```\n",
    "# docunmentation for this parameter\n",
    "paramter: p_value_cutoff\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "**FIXME** what we need to do for this pipeline:\n",
    "\n",
    "1. make it work for the input and output formats i design --- follow it 100% but if you feel it awkward reach out to me to discuss rather than making decisions on your own.\n",
    "2. change all \"condition\" to \"context\" (unify the termiology)\n",
    "3. for the new `[twas]` pipeline, we are adding an extra layer of loop to the logic below, which now should look like this:\n",
    "\n",
    "```\n",
    "for each block:\n",
    "-- refined_twas_weights_data = list()\n",
    "-- for each gene:\n",
    "---- imputable_contexts = list() # $context: method1, method2, etc\n",
    "---- for each context:\n",
    "------ update_imputable_contexts()\n",
    "---- if len(imputable_contexts) > 0:\n",
    "------ twas_weights_data = load_twas_weights( ... ) # this is pecotmr function, which is part of my original design in twas_mr pipeline\n",
    "------ update_refined_twas_weights() # see section \"Output\" above for what I expect of the refined database for TWAS, involving extracting information from the best method, and also select top variants for cTWAS\n",
    "------ for each gwas_study:\n",
    "-------- load_and_handle_gwas_data()\n",
    "-------- twas_analysis_for_all_contexts_and_methods(raw_twas_weights_data) # For contexts that are not imputable simply put twas_z to NA\n",
    "```\n",
    "\n",
    "4. for the new `[ctwas]` step, we combine it with MR as `[ctwas_mr]`\n",
    "\n",
    "```\n",
    "---- for each gwas_study:\n",
    "------ ctwas_wrapper(refined_twas_weights_meta_data, ...)\n",
    "------ get causal gene context from cTWAS output\n",
    "------ perform MR on these causal gene in respective contexts\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[twas_1]\n",
    "depends: sos_variable(\"regional_data\")\n",
    "parameter: allele_qc = True\n",
    "parameter: coverage = \"cs_coverage_0.95\"\n",
    "parameter: max_var_select = 10\n",
    "parameter: p_value_cutoff = 0.05\n",
    "parameter: rsq_threshold = 0.01\n",
    "meta_info = [x[\"meta_info\"] for x in regional_data['xQTL'].values()]\n",
    "xqtl_files = [x[\"files\"] for x in regional_data['xQTL'].values()]\n",
    "xqtl_name = os.path.splitext(name)[0].rsplit('_', 1)[0]\n",
    "input: xqtl_files, group_by = lambda x: group_by_region(x, xqtl_files), group_with = \"meta_info\"\n",
    "output: f'{cwd:a}/{xqtl_name}.{_meta_info[3]}.twas_results.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "    # we have potentially multiple weight db RDS files for each region of interest\n",
    "    library(pecotmr)\n",
    "    library(data.table)\n",
    "    library(dplyr)\n",
    "\n",
    "    weight_db = c(${_input:r,})\n",
    "    chrom = ${_meta_info[0]}\n",
    "    start = ${_meta_info[1]} \n",
    "    end = ${_meta_info[2]}\n",
    "    region = \"${_meta_info[3]}\"\n",
    "    ## FIXME: take these information from the xqtl_meta data table\n",
    "    xqtl_conditions <- names(readRDS(weight_db)[[1]]) \n",
    "\n",
    "    LD_meta_file_path = ${ld_meta_data:r}\n",
    "    gwas_studies = c(${paths(regional_data[\"GWAS\"].keys()):r,})\n",
    "    # load gwas data file for this particular chrom\n",
    "    gwas_files = c(${paths([v[_meta_info[0]] for k, v in regional_data[\"GWAS\"].items()]):r,})\n",
    "\n",
    "    # Step 0: Load GWAS data for the region of interest, for each study\n",
    "    # Generate the region of interest\n",
    "    region_of_interest = data.frame(chrom = chrom, start = start, end = end)\n",
    "    #Step 1: load the weight lists for the specified conditions; each element of weight list is a weight matrix for each condition. \n",
    "    ## select variants \n",
    "    twas_weights_results = select_ctwas_weights(weight_db, xqtl_conditions, variable_name_obj=c(\"preset_variants_result\", \"variant_names\"), \n",
    "                                                twas_weights_table = \"twas_weights\", max_var_selection=${max_var_select}, \n",
    "                                                min_rsq_threshold = ${rsq_threshold}, p_val_cutoff = ${p_value_cutoff})\n",
    "    #Step 2: Load GWAS data for the region of interest, for each study\n",
    "    gwas_data = list()\n",
    "    twas_susie_selection= list()\n",
    "    \n",
    "    for (s in seq_along(gwas_studies)) {\n",
    "      gwas_sumstats <- fread(gwas_files[s]) %>%\n",
    "        rename(\"pos\" = \"position\", \"chrom\" = \"chromosome\", \"A1\" = \"ref\", \"A2\" = \"alt\") #%>%\n",
    "         # mutate(z = beta / se) ---- # use original z score, as some variants of sumstats were imputed, have beta and se and NA.  \n",
    "      # Load LD list containing LD matrix and corresponding variants\n",
    "      gwas_LD_list <- load_LD_matrix(LD_meta_file_path, region_of_interest, gwas_sumstats)\n",
    "      # remove duplicate variants\n",
    "      dup_idx <- which(duplicated(gwas_LD_list$combined_LD_variants))\n",
    "      if (length(dup_idx)>=1){\n",
    "          gwas_LD_list$combined_LD_variants <- gwas_LD_list$combined_LD_variants[-dup_idx] \n",
    "          gwas_LD_list$combined_LD_matrix <- gwas_LD_list$combined_LD_matrix[-dup_idx, -dup_idx] \n",
    "          gwas_LD_list$ref_panel <- gwas_LD_list$ref_panel[-dup_idx,]\n",
    "      }\n",
    "      # Allele flip\n",
    "      gwas_allele_flip <- allele_qc(gwas_sumstats[, c(\"chrom\", \"pos\", \"A1\", \"A2\")], gwas_LD_list$combined_LD_variants, gwas_sumstats, c(\"beta\", \"se\", \"z\"))\n",
    "      # Load LD matrix and sumstats\n",
    "      gwas_data[[gwas_studies[s]]][[\"LD\"]] <- gwas_LD_list$combined_LD_matrix\n",
    "      gwas_data[[gwas_studies[s]]][[\"variance\"]] <- gwas_LD_list$ref_panel$variance\n",
    "      gwas_data[[gwas_studies[s]]][[\"sumstats\"]] <- gwas_allele_flip$target_data_qced\n",
    "      twas_susie_selection[[gwas_studies[s]]]$region_info <- twas_weights_results$region_info\n",
    "      twas_susie_selection[[gwas_studies[s]]]$gwas_qced <- gwas_data[[gwas_studies[s]]] \n",
    "\n",
    "      # get weights table for imputable genes x condition \n",
    "      # if the gene is not imputable\n",
    "      if (!isTRUE(twas_weights_results$model_selection$imputable)) {\n",
    "        gene <- unique(twas_weights_results$region_info$region_name)\n",
    "        print(paste0(\"Gene \", gene, \" is not imputable in all conditions, skipping. \"))\n",
    "        for (condition in xqtl_conditions){\n",
    "            twas_susie_selection[[gwas_studies[s]]][[\"weights\"]][[condition]] <- list(variant_selection=c(NULL), selected_weights=c(NULL))\n",
    "            twas_susie_selection[[gwas_studies[s]]][[\"model_selection\"]][[condition]] <- list(\n",
    "              method = c(twas_weights_results$model_selection[[condition]]),\n",
    "              imputable = c(twas_weights_results$model_selection$imputable)\n",
    "            )\n",
    "        }\n",
    "      } else {\n",
    "        # if the gene is imputable \n",
    "        for (condition in xqtl_conditions) {\n",
    "          twas_susie_selection[[gwas_studies[s]]][[\"model_selection\"]][[condition]] <- list(\n",
    "                  method = c(twas_weights_results$model_selection[[condition]]),\n",
    "                  imputable = c(twas_weights_results$model_selection$imputable)\n",
    "              )\n",
    "          # gene is imputable but condition is not imputable although \n",
    "          if (is.null(twas_weights_results$model_selection[[condition]])){\n",
    "              twas_susie_selection[[gwas_studies[s]]][[\"weights\"]][[condition]]<- list(variant_selection = rep(NA, ${max_var_select}), \n",
    "                                                                                      selected_weights = rep(NA, ${max_var_select}))\n",
    "          } else {\n",
    "              # if the condition is imputable\n",
    "              # Step 3: Intersect with gwas summary statistics and adjust susie weights\n",
    "              adjusted_susie_weights <- adjust_susie_weights(twas_weights_results, condition,\n",
    "                keep_variants = get_nested_element(gwas_data, c(gwas_studies[s], \"sumstats\", \"variant_id\")),\n",
    "                allele_qc = ${\"TRUE\" if allele_qc else \"FALSE\"}\n",
    "              )\n",
    "              # Step 4: Overlap weights of other methods with the variants name of adjusted_susie_weights, \n",
    "              # then combine with adjusted susie weights to obtain the subsetted weight matrix\n",
    "              weights_matrix <- get_nested_element(twas_weights_results, c(\"weights\", condition))\n",
    "              weights_matrix_subset <- cbind(\n",
    "                susie_weights = adjusted_susie_weights$adjusted_susie_weights,\n",
    "                weights_matrix[adjusted_susie_weights$remained_variants_ids, !colnames(weights_matrix) %in% \"susie_weights\"]\n",
    "              )\n",
    "              if(${\"TRUE\" if allele_qc else \"FALSE\"}){\n",
    "                weights_matrix_qced <- allele_qc(rownames(weights_matrix_subset), gwas_LD_list$combined_LD_variants,\n",
    "                  weights_matrix_subset, 1:ncol(weights_matrix),\n",
    "                  target_gwas = FALSE\n",
    "                )\n",
    "                weights_matrix_subset <- weights_matrix_qced$target_data_qced[, !colnames(weights_matrix_qced$target_data_qced) %in% \n",
    "                                                                              c(\"chrom\", \"pos\", \"A1\", \"A2\", \"variant_id\")]\n",
    "                rownames(weights_matrix_subset) <- get_nested_element(weights_matrix_qced, c(\"target_data_qced\", \"variant_id\"))\n",
    "              }\n",
    "              # Conduct twas analysis\n",
    "              twas_result <- twas_analysis(\n",
    "                weights_matrix_subset, gwas_data[[gwas_studies[s]]][[\"sumstats\"]], gwas_data[[gwas_studies[s]]][[\"LD\"]],\n",
    "                rownames(weights_matrix_subset)\n",
    "              )\n",
    "              # Step 4: Output the susie and twas weights from best model and selected variants\n",
    "              model <- get_nested_element(twas_weights_results, c(\"model_selection\", condition))\n",
    "              variants_picked <- get_nested_element(twas_weights_results, c(\"susie_results\", condition, \"variant_selection\"))\n",
    "              selec_indx <- match(variants_picked, paste0(\"chr\", rownames(weights_matrix_subset))) # match with rownames of weights_matrix_subset that begin with \"chr\"\n",
    "              ctwas_weights <- weights_matrix_subset[[paste0(model, \"_weights\")]][selec_indx]\n",
    "              names(ctwas_weights) <- variants_picked\n",
    "\n",
    "              # Add selected variants and weights information to the weights \n",
    "              twas_susie_selection[[gwas_studies[s]]][[\"weights\"]][[condition]] <- list(variant_selection = get_nested_element(twas_weights_results, \n",
    "                                                         c(\"susie_results\", condition, \"variant_selection\")), selected_weights = ctwas_weights, \n",
    "                                                         twas_result = twas_result)\n",
    "  \n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    saveRDS(twas_susie_selection, ${_output:ar}, compress='xz')\n",
    "  \n",
    "    # save meta file\n",
    "    study_list <- names(twas_susie_selection)\n",
    "    combined_report <- do.call(rbind, lapply(study_list, function(study){\n",
    "        conditions <- names(twas_susie_selection[[study]][[\"model_selection\"]])\n",
    "        if (isTRUE(twas_susie_selection[[study]][[\"model_selection\"]][[1]][[\"imputable\"]])){\n",
    "            imputable_conditions <- unlist(lapply(conditions, function(condition){\n",
    "                method_selected <- twas_susie_selection[[study]][[\"model_selection\"]][[condition]][[\"method\"]]\n",
    "                ifelse(!is.null(method_selected), return(paste0(condition, \"_\", method_selected)), return(paste0(condition, \"_\", \"non_imput\")))\n",
    "            }))\n",
    "        } else {\n",
    "            imputable_conditions <- NULL\n",
    "        }\n",
    "            report <- data.frame(study=study,\n",
    "                               gene = unique(sub(\"^.*?\\\\_\", \"\", conditions)),\n",
    "                               chrom = as.integer(unique(twas_susie_selection[[study]]$region_info$grange$chrom)),\n",
    "                               type = \"${data_type}\",\n",
    "                               IsImputable = twas_susie_selection[[study]][[\"model_selection\"]][[conditions[1]]][[\"imputable\"]],\n",
    "                               ImputableCondition = ifelse(length(imputable_conditions)>=1, paste(imputable_conditions, collapse = \",\"), NA),\n",
    "                               path = ${_output:ar},\n",
    "                               stringsAsFactors = FALSE)\n",
    "             return(report)\n",
    "\n",
    "    }))\n",
    "    write.table(combined_report, paste0(${_output:annr}, \".meta_table\"), sep=\"\\t\", row.names=FALSE, col.names=TRUE, quote=FALSE)\n",
    "    \n",
    "    # in case of re-running, new will have meta data updated without being ignored.\n",
    "    if (file.exists('${_output:annn}.summary_table.tsv')){\n",
    "      # Remove the file\n",
    "      file.remove('${_output:annn}.summary_table.tsv')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[twas_sparse_2]\n",
    "depends: sos_variable(\"regional_data\")\n",
    "# update records on the same gene_study on the summary table\n",
    "# add new record for new gene_study\n",
    "meta_info = [x[\"meta_info\"] for x in regional_data['xQTL'].values()]\n",
    "xqtl_files = [x[\"files\"] for x in regional_data['xQTL'].values()]\n",
    "xqtl_name = os.path.splitext(name)[0].rsplit('_', 1)[0]\n",
    "step_name = step_name.rsplit('_', 1)[0]\n",
    "input: f\"{cwd:a}/{step_name}/{xqtl_name}.*.meta_table\", group_by='all'\n",
    "output: f\"{cwd:a}/{step_name}/{xqtl_name}.summary_table.tsv\" \n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{cwd:a}/{step_name}/{xqtl_name}.summary_table.stdout\", stderr = f\"{cwd:a}/{step_name}/{xqtl_name}.summary_table.stderr\", container = container, entrypoint = entrypoint\n",
    "\n",
    "    file_paths <- c(${_input:r,})\n",
    "    summary_list <- lapply(file_paths, function(file)read.table(file, sep=\"\\t\", header=TRUE))\n",
    "    summary_table <- data.table::rbindlist(summary_list, use.names=TRUE)\n",
    "    write.table(summary_table, ${_output:r}, sep=\"\\t\", col.names=TRUE, row.names=FALSE, quote=FALSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     "shell"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.24.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
